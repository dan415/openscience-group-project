<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_rhKYJJe">Affective Multimodal Human-Computer Interaction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="2,138.84,110.46,61.69,13.39"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
							<email>mpantic@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of EEMCS</orgName>
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="2,211.22,110.46,54.53,13.39"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Science</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="2,276.49,110.46,80.73,13.39"><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
							<email>jeffcohn@pitt.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Psychology and Psychiatry</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="2,387.70,110.46,81.16,13.39"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
							<email>huang@ifp.uiuc.edu</email>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Beckman Institute</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_kbTUTJr">Affective Multimodal Human-Computer Interaction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E2D5D5EE7AFAEC24381449AF66074288</idno>
					<idno type="DOI">10.1145/1101149.1101299</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-05-11T16:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_WdsmHmj">A.1 [Introductory and Survey] H1.2 [User/Machine Systems]: Human information processing H.5.1 [Multimedia Information Systems]: Audiovisual input I.5.4 [Pattern Recognition Applications]: Models</term>
					<term xml:id="_hcjb8Jc">Learning Algorithms</term>
					<term xml:id="_dwSavQA">Theory</term>
					<term xml:id="_2DXNU8Z">Performance Affective computing</term>
					<term xml:id="_E5NYuR7">Multimodal Human-Computer Interaction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_hP84w9m"><p xml:id="_u4c784t">Social and emotional intelligence are aspects of human intelligence that have been argued to be better predictors than IQ for measuring aspects of success in life, especially in social interactions, learning, and adapting to what is important. When it comes to machines, not all of them will need such skills. Yet to have machines like computers, broadcast systems, and cars, capable of adapting to their users and of anticipating their wishes, endowing them with the ability to recognize user's affective states is necessary. This article discusses the components of human affect, how they might be integrated into computers, and how far are we from realizing affective multimodal human-computer interaction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1." xml:id="_9ZWu4XM">INTRODUCTION</head><p xml:id="_h9YuG4d">We have entered an era of enhanced digital connectivity. Computers and the Internet have become so embedded in the daily fabric of people's lives that we can no longer live without them <ref type="bibr" coords="2,75.26,560.18,14.06,9.96" target="#b19">[20]</ref>. We use this technology to work, communicate, shop, seek out new information, and entertain ourselves. With the everincreasing diffusion of computers into society, human-computer interaction (HCI) is becoming increasingly essential to our daily lives.</p><p xml:id="_VVRSJjV">HCI design was first dominated by direct manipulation and then delegation. The tacit assumption of both approaches to interaction has been that the human will be explicit, unambiguous and fully attentive while controlling information and command flow. Boredom, preoccupation, and stress are unthinkable even though they are "very human" behaviors. The insensitivity of current HCI designs is acceptable for well-codified tasks. It works for making plane reservations, buying and selling stocks and, as a matter of fact, almost everything we do with computers today. But this kind of categorical computing is inappropriate for design, debate, and deliberation. In fact, it is the major impediment to having flexible machines capable of adapting to their users' level of attention, preferences, moods, and intentions.</p><p xml:id="_MXtH9BG">The ability to detect and understand affective states and other social signals of someone with whom we are communicating is the core of social and emotional intelligence. This kind of intelligence is a facet of human intelligence that has been argued to be indispensable and even the most important for a successful social life <ref type="bibr" coords="2,357.74,404.17,14.09,9.96" target="#b17">[18]</ref>. When it comes to computers, however, they are socially ignorant <ref type="bibr" coords="2,382.39,414.73,14.06,9.96" target="#b34">[35]</ref>. Current HCI technology does not account for the fact that human-human communication is always socially situated and that discussions are not just facts but part of a larger social interplay. Not all computers will need social and emotional intelligence and none will need all of the related skills humans have. Yet, human-machine interactive systems capable of sensing stress, inattention, and heedfulness, and capable of adapting and responding to these affective states of users are likely to be perceived as more natural, efficacious, and trustworthy. For example, in education, pupils' affective signals inform the teacher of the need to adjust the instructional message. Successful human teachers acknowledge this and work with it; digital conversational embodied agents must begin to do the same by employing tools that can accurately sense and interpret affective signals and social context of the pupil, learn successful context-dependent social behavior, and use a proper affective presentation language (e.g. <ref type="bibr" coords="2,317.88,583.69,14.69,9.96" target="#b32">[33]</ref>) to drive the animation of the agent. The research area of machine analysis and employment of human affective states to build more natural, flexible HCI goes by the general name of affective computing as introduced by Picard <ref type="bibr" coords="2,479.41,615.37,14.09,9.96" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2." xml:id="_RD4mMdE">THE APPLICATION DOMAIN</head><p xml:id="_kAhna5B">In addition to HCI, various research areas and technologies would benefit from efforts to model human perception of affective feedback computationally. For instance, automatic recognition of human affective states is an important research topic for video surveillance <ref type="bibr" coords="2,364.49,694.34,14.04,9.96" target="#b18">[19]</ref>. Automatic assessment of boredom, inattention, and stress would be highly valuable in situations in which firm attention to a crucial but perhaps tedious task is essential. Examples include air traffic control, nuclear power plant surveillance, and operating a motor vehicle. An automated tool could provide prompts for better performance informed by assessment of the user's affective state.</p><p xml:id="_fMfBJCz">Other domain areas in which machine tools for analysis of human affective feedback could expand and enhance scientific understanding and practical applications include specialized areas in professional and scientific sectors. In the security sector, affective behavioral cues play a crucial role in establishing or detracting from credibility. In the medical sector, affective behavioral cues are a direct means to identify when specific mental processes are occurring. Machine analysis of human affective states could be of considerable value in these situations in which only informal, subjective interpretations are now used. It would also facilitate research in areas such as behavioral science (in studies on emotion and cognition), anthropology (in studies on cross-cultural perception and production of affective states), neurology (in studies on dependence between emotion dysfunction or impairment and brain lesions) and psychiatry (in studies on schizophrenia and mood disorders) in which reliability, sensitivity, and precision of measurement of affective behavior are persisting problems <ref type="bibr" coords="3,127.11,310.69,14.06,9.96" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3." xml:id="_yHfvDuc">THE PROBLEM DOMAIN</head><p xml:id="_SmanxRa">While all agree that machine sensing and interpretation of human affective information would be widely beneficial, addressing these problems is not an easy task. The main problem areas can be defined as follows.</p><p xml:id="_87ZkB6E">What is an affective state? This question is related to psychological issues pertaining to the nature of affective states and the best way to represent them.</p><p xml:id="_3n6peHk">Which human communicative signals convey information about affective state? This issue shapes the choice of different modalities to be integrated into an automatic analyzer of human affective feedback.</p><p xml:id="_qKgxePg">How are various kinds of evidence to be combined to optimize inferences about affective states? This question is related to how best to integrate information across modalities for emotion recognition.</p><p xml:id="_BNkYXDK">What is an affective state? Traditionally, the terms "affect" and "emotion" have been used synonymously. Following Darwin, discrete emotion theorists propose the existence of six or more basic emotions that are universally displayed and recognized <ref type="bibr" coords="3,276.62,550.94,14.07,9.96" target="#b10">[11]</ref>, <ref type="bibr" coords="3,54.00,561.50,14.07,9.96" target="#b22">[23]</ref>. These include happiness, anger, sadness, surprise, disgust, and fear. Data from both Western and traditional societies suggests that non-verbal communicative signals (especially facial and vocal expression) involved in these basic emotions are displayed and recognized cross-culturally. In opposition to this view, Russell <ref type="bibr" coords="3,110.23,614.30,15.21,9.96" target="#b37">[38]</ref> among others argues that emotion is best characterized in terms of a small number of latent dimensions, rather than in terms of a small number of discrete emotion categories. Russell proposes bipolar dimensions of arousal and valence (pleasant versus unpleasant). Watson and Tellegen propose unipolar dimensions of positive and negative affect while Watson and Clark proposed a hierarchical model that integrates discrete emotions and dimensional views <ref type="bibr" coords="3,209.11,688.22,14.07,9.96" target="#b23">[24]</ref>, <ref type="bibr" coords="3,229.97,688.22,14.07,9.96" target="#b42">[43]</ref>, <ref type="bibr" coords="3,250.84,688.22,14.07,9.96" target="#b43">[44]</ref>. Social constructivists argue that emotions are socially constructed ways of interpreting and responding to particular classes of situations. They argue further that emotion is culturally constructed and no universals exist. From their perspective, subjective experience and whether or not emotion is better conceptualized categorically or dimensionally is culture specific. Then there is lack of consensus on how affective displays should be labeled. For example, Fridlund argues that human facial expressions should not be labeled in terms of emotions but in terms of Behavioral Ecology interpretations, which explain the influence a certain expression has in a particular context. Thus, an "angry" face should not be interpreted as anger but as back-off-or-I-will-attack. Yet, people still tend to use anger as the interpretation rather than readinessto-attack interpretation. Another issue is that of culture dependency: the comprehension of a given emotion label and the expression of the related emotion seem to be culture dependent <ref type="bibr" coords="3,317.88,220.21,14.04,9.96" target="#b24">[25]</ref>, <ref type="bibr" coords="3,338.58,220.21,14.04,9.96" target="#b44">[45]</ref>. In summary, previous research literature pertaining to the nature and suitable representation of affective states provides no firm conclusions that could be safely presumed and adopted in studies on machine analysis of human affective states and affective computing. Also, it is not only discrete emotional states like surprise or anger that are of importance for the realization of proactive human-machine interactive systems. Sensing and responding to behavioral cues identifying attitudinal states like interest and boredom, to those underlying moods, and to those disclosing social signaling like empathy and antipathy are essential. Hence, in contrast to traditional approach, we treat affective states as being correlated not only to discrete emotions but to other, aforementioned social signals as well. Furthermore, since it is not certain that each of us will express a particular affective state by modulating the same communicative signals in the same way, nor is it certain that a particular modulation of interactive cues will be interpreted always in the same way independently of the situation and the observer, we advocate that pragmatic choices (e.g., application-and user-profiled choices) must be made regarding the selection of affective states to be recognized by an automatic analyzer of human affective feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_rmBvD9C">Which human communicative signals convey information about affective state?</head><p xml:id="_WfFydJB">Affective arousal modulates all human communicative signals <ref type="bibr" coords="3,403.00,469.33,14.09,9.96" target="#b10">[11]</ref>. However, the visual channel carrying facial expressions and body gestures seems to be most important in the human judgment of behavioral cues <ref type="bibr" coords="3,486.44,490.44,9.79,9.96" target="#b0">[1]</ref>. Human judges seem to be most accurate in their judgment when they are able to observe the face and the body. Ratings that were based on the face and the body were 35% more accurate than the ratings that were based on the face alone. Yet, ratings that were based on the face alone were 30% more accurate than ratings that were based on the body alone and 35% more accurate than ratings that were based on the tone of voice alone <ref type="bibr" coords="3,424.55,564.36,9.79,9.96" target="#b0">[1]</ref>. These findings indicate that to interpret someone's behavioral cues, people rely on shown facial expressions and to a lesser degree on shown body gestures and vocal expressions. However, although basic researchers have been unable to identify a set of voice cues that reliably discriminate among emotions, listeners seem to be accurate in decoding emotions from voice cues <ref type="bibr" coords="3,420.89,627.72,14.07,9.96" target="#b20">[21]</ref>. Thus, automated human affect analyzers should at least include facial expression modality and preferably they should also include (one or both) modalities for perceiving body gestures and tone of the voice. Finally, while too much information from different channels seem to be confusing to human judges, resulting in less accurate judgments of shown behavior when 3 or more observation channels are available (e.g. face, body, and speech) <ref type="bibr" coords="3,407.97,701.64,9.79,9.96" target="#b0">[1]</ref>, combining those multiple modalities (including speech and physiology) may prove appropriate for realization of automatic human affect analysis.</p><p xml:id="_HJPqR87">How are various kinds of evidence to be combined to optimize inferences about affective states? Humans simultaneously employ the tightly coupled modalities of sight, sound and touch. As a result, analysis of the perceived information is highly robust and flexible. Thus, in order to accomplish a multimodal analysis of human interactive signals acquired by multiple sensors, which resembles human processing of such information, input signals should not be considered mutually independent and should not be combined only at the end of the intended analysis as the majority of current studies do. The input data should be processed in a joint feature space and according to a context-dependent model <ref type="bibr" coords="4,54.00,215.90,14.04,9.96" target="#b29">[30]</ref>. The latter refers to the fact that one must know the context in which the observed interactive signals have been displayed (who the expresser is and what his current environment and task are) in order to interpret the perceived multi-sensory information correctly.</p><p xml:id="_MTpkUt8">Hence, an "ideal" automatic analyzer of human affective information should be able to emulate at least some of the capabilities of the human sensory system as summarized below.</p><p xml:id="_vDFmkeY">An "ideal" automatic human-affect analyzer would be:</p><p xml:id="_sHPhVVz">• multimodal (handling the face, body, and tone of voice)</p><p xml:id="_twGeh2f">• robust and accurate (despite occlusions, changes in viewing and lighting conditions, and ambient noise) • generic (independent of physiognomy, sex, age, and ethnicity of the subject) • sensitive to the dynamics of displayed affective expressions (performing temporal analysis of the sensed data, previously processed in a joint feature space) • context-sensitive (realizing environment-and task-dependent data interpretation in terms of user-profiled affect-descriptive labels)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4." xml:id="_VNqRpkV">THE STATE OF THE ART</head><p xml:id="_QvNeWK7">To interpret someone's behavioral cues, including emotional states, people rely mainly on shown facial expressions <ref type="bibr" coords="4,260.18,479.30,9.79,9.96" target="#b0">[1]</ref>, <ref type="bibr" coords="4,276.50,479.30,14.07,9.96" target="#b22">[23]</ref>, and it is not surprising, therefore, that the majority of efforts in affective computing concern automatic analysis of facial displays. For an exhaustive survey of studies on machine analysis of facial affect, readers are referred to <ref type="bibr" coords="4,165.39,521.54,14.09,9.96" target="#b29">[30]</ref>. The survey indicates that the capabilities of currently existing facial affect analyzers are rather limited. Nevertheless, the automated systems achieve an accuracy of 64% to 98% when detecting 3-7 emotions deliberately displayed by 5-40 subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_qYT88fw">Limitations of existing facial-affect analyzers are [30]:</head><p xml:id="_TxgFx9F">• handle only a small set of posed prototypic facial expressions of six basic emotions from portraits or nearlyfrontal views of faces with no facial hair or glasses recorded under constant illumination • do not perform a context-dependent interpretation of shown facial behavior • do not analyze extracted facial information on different time scales (short videos are handled only); consequently, inferences about the expressed mood and attitude (larger time scales) cannot be made by current facial affect analyzers An interesting point, nevertheless, is that we cannot conclude that a system achieving a 92% average recognition rate performs "better" than a system attaining a 74% average recognition rate when detecting six basic emotions from face images unless both systems are tested on the same dataset. The main problem is that no database of images exists that is shared by all diverse facialexpression-research communities. In general, small databases of facial-expression images are made and exploited by each research community. The databases shared currently by several research communities are the Cohn-Kanade Facial Expression Database <ref type="bibr" coords="4,317.88,177.98,14.07,9.96" target="#b21">[22]</ref>, the JAFFE Database [47] and the MMI Facial Expression Database <ref type="bibr" coords="4,353.05,188.54,14.06,9.96" target="#b31">[32]</ref>.</p><p xml:id="_Jczk22F">Although scientists conducted a large number of studies of vocal expression in an attempt to specify what aspects of the voice are predictive of expressed or portrayed emotion, they have been unable to identify a set of voice cues that reliably discriminate among emotions <ref type="bibr" coords="4,385.52,247.34,14.06,9.96" target="#b20">[21]</ref>. In the light of this problem it is not surprising that computer science and related fields produced rather disappointing results on automated vocal affect expression analysis. For a survey of studies on automatic analysis of vocal affect, the readers are referred to <ref type="bibr" coords="4,443.68,289.58,14.09,9.96" target="#b29">[30]</ref>. The survey indicates that the existing automated systems for auditory analysis of human affect are quite limited. Similarly to the case of automatic facial affect analysis, it is still not possible to compare different vocal affect analyzers since isolated, small databases of speech material are made and exploited by each research community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_zsmTsdK">Limitations of existing vocal-affect analyzers are [30]:</head><p xml:id="_TNcDHh9">• perform singular classification of input audio signals into a few emotion categories such as anger, irony, happiness, sadness/grief, fear, disgust, surprise and affection • do not perform a context-sensitive analysis (environment-, user-and task-dependent analysis) of the input audio signal • do not analyze extracted vocal expression information on different time scales (proposed inter-audio-frame analyses are used either for the detection of supra-segmental features, such as the pitch and intensity over the duration of a syllable or word, or for the detection of phonetic features)inferences about moods and attitudes (longer time scales) cannot be made by current vocal-affect analyzers • adopt strong assumptions (e.g., the recordings are noise free, the recorded sentences are short, delimited by pauses, carefully pronounced by non-smoking actors) and use the test data sets that are small (one or more words or one or more short sentences spoken by few subjects) containing exaggerated vocal expressions of affective states Relatively few of the existing works combine different modalities into a single system for human affective state analysis. Although the studies in psychology on the accuracy of predictions from observations of expressive behavior suggest that the combined face and body are the most informative <ref type="bibr" coords="4,466.65,610.70,9.79,9.96" target="#b0">[1]</ref>, except of a tentative attempt of Balomenos et al. <ref type="bibr" coords="4,424.11,621.26,9.79,9.96" target="#b1">[2]</ref>, there is virtually no other effort reported on automatic human affect analysis from combined face and body gestures. Examples of existing works combining different modalities into a single system for human affective state analysis are those of Chen &amp; Huang <ref type="bibr" coords="4,451.57,663.50,9.79,9.96" target="#b1">[2]</ref>, Yoshitomi et al. <ref type="bibr" coords="4,527.53,663.50,14.09,9.96" target="#b45">[46]</ref>, De Silva &amp; Ng <ref type="bibr" coords="4,369.78,674.06,9.79,9.96" target="#b5">[6]</ref>, Go et al. <ref type="bibr" coords="4,429.68,674.06,14.06,9.96" target="#b16">[17]</ref>, and Song et al. <ref type="bibr" coords="4,519.88,674.06,14.06,9.96" target="#b41">[42]</ref>, who investigated the effects of a combined detection of facial and vocal expressions of affective states. In brief, these works achieve an accuracy of 72% to 85% when detecting one or more basic emotions from clean audiovisual input (e.g., noise-free recordings, closely-placed microphone, non-occluded portraits) from an actor speaking a single word and showing exaggerated facial displays of a basic emotion. Although audio and image processing techniques in these systems are relevant to the discussion on the state of the art in affective computing, the systems themselves have all (and some additional) drawbacks of single-modal affect analyzers. In turn, many improvements are needed if those systems are to be used for a multimodal context-sensitive HCI where a clean input from a known actor/announcer cannot be expected and a contextindependent separate processing and interpretation of audio and visual data do not suffice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5." xml:id="_GhnRKQj">CHALLENGES</head><p xml:id="_7UAYnMY">Probably the most remarkable issue about the state of the art in the research on affective multimodal HCI is that only a few efforts toward the implementation of audiovisual human-affect analyzer (combining the facial and the vocal affect analysis) have been reported so far. Although the studies in psychology suggest that the combined face and body are very informative when analyzing human expressive behavior, a single effort toward the realization of such a bi-modal affect analyzer has been reported up to date.</p><p xml:id="_NaJDDa4">No effort toward the integration of more than two modalities into an automated human-affect analyzer has been reported so far.</p><p xml:id="_NXVu3RU">Another issue concerns the interpretation of behavioral cues in terms of affective states. The existing work usually employs singular classification of input data into one of the "basic" emotion categories. Yet pure expressions of "basic" emotions are less frequently elicited; much of the time people show blends of emotional displays. Hence, the classification of human non-verbal affective feedback into a single "basic"-emotion category may not be realistic. Also, not all non-verbal affective cues can be classified as a combination of the "basic" emotion categories.</p><p xml:id="_mymcKDE">Think for instance about the frustration, stress, skepticism or boredom. Furthermore, it has been shown that the comprehension of a given emotion label and the ways of expressing the related affective state may differ from culture to culture and even from person to person. Hence, the definition of interpretation categories in which any facial and/or vocal affective behavior, displayed at any time scale, can be classified is a key challenge in the design of realistic affect-sensitive monitoring tools.</p><p xml:id="_TE2zqBa">Virtually all the existing human-affect analyzers assume that the input data are isolated or pre-segmented expressions showing a single temporal pattern (onset apex offset) of an affective state that begins and ends with a neutral state. In reality, such segmentation is an exception. Human expressive behavior is more complex. Transitions from one affective state to another may include multiple apexes and may be direct, without an intermediate neutral state. For this reason, existing human-affect analyzers have difficulties with handling spontaneously occurring expressions of emotion. In addition, eliciting spontaneous affective behavior, which could be used to train human-affect analyzers, represents a research challenge on its own right. Hence, while answering the question of how to parse the stream of spontaneous affective behavior is essential for the realization of affective multimodal HCI, we also recognize the likelihood that such a goal is still in the relatively distant future.</p><p xml:id="_fDaFvbV">Realization of a human-like interpretation of sensed affective behavior requires context-dependent choices (i.e., environment-, user-and task-profiled choices). Nonetheless, currently existing methods aimed at the automation of human-affect analysis are context insensitive. Although machine-context sensing, that is, answering questions like who is the user, where is (s)he, and what is (s)he doing, has witnessed recently a number of significant advances <ref type="bibr" coords="5,355.30,125.17,14.06,9.96" target="#b33">[34]</ref>, the complexity of this problem makes contextsensitive human-affect analysis a significant research challenge.</p><p xml:id="_68yK34Z">Finally, no readily accessible database of test material that could be used as a basis for benchmarks for efforts in the research area of multimodal human-affect analysis has been established yet. This lack of common testing resource forms a major impediment to comparing, resolving and extending the issues concerned with automatic, multimodal human affect analysis and understanding. It is one of the most critical issues confronting affective multimodal HCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6." xml:id="_cyPTr7d">RECOMMENDATIONS</head><p xml:id="_RqhB5uW">The remarkable aspect of human expressive behavior is its communicative power: even fleeting glimpses ("thin slices") of expressive behavior communicate a great deal of information. This suggestion is confirmed by findings indicating that judgments about the meaning of expressive behavior are quite accurate even when they are based on brief observations <ref type="bibr" coords="5,526.33,315.74,9.79,9.96" target="#b0">[1]</ref>. This is especially true for emotions; judgments about emotions are fairly accurate even from exposures to nonverbal behavior lasting only 375 ms <ref type="bibr" coords="5,366.82,347.42,9.79,9.96" target="#b0">[1]</ref>. Much of this expressive behavior is unintended and unconscious (and yet extremely effective). In fact, these expressive nonverbal cues are so subtle that they are neither encoded nor decoded at an intentional, conscious level of awareness <ref type="bibr" coords="5,356.95,389.66,9.79,9.96" target="#b0">[1]</ref>. This suggests the following:</p><p xml:id="_VST6YVU">• While continuous analysis of human expressive behavior would be ideal, automated human-affect analyzers can be useful even if they are able to analyze only short observations of expressive behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_WrgJUnj">• Biologically inspired classification techniques, like Artificial</head><p xml:id="_DqmjctG">Neural Networks, may prove more suitable for tackling the problem of human affect recognition than methods like Expert Systems, which consider classification problems from a logical rather than biological perspective. The former are motivated by human unconscious problem solving processes while the latter are inspired by human conscious problem solving processes.</p><p xml:id="_pz5ga3m">As noted above and remarked already by Pantic and Rothkrantz <ref type="bibr" coords="5,317.88,534.13,14.06,9.96" target="#b29">[30]</ref>, a typical issue of multimodal data processing proposed so far is that multisensory data are processed separately and only combined at the end. This practice may follow from experimental studies that have shown that a late integration (decision-level data fusion) provides higher recognition scores than an early integration approach <ref type="bibr" coords="5,398.54,586.93,14.09,9.96" target="#b38">[39]</ref>. The differences in time scale of the features from different channels and the lack of a common metric across the modalities add and abet the underlying inference that the features from different channels are not sufficiently correlated to be fused at the feature level. Yet, people display audiovisual expressive cues in a complementary and redundant manner. In order to accomplish a human-like multimodal analysis of multiple input signals acquired by different sensors, the signals cannot be considered mutually independent and cannot be combined in a context-free manner at the end of the intended analysis. The input data should be processed in a joint feature space and according to a context-dependent model. In practice, however, besides the problems of context sensing and developing context-dependent models for combining multisensory data, one should cope with the size of the required joint feature space, which can suffer from large dimensionality, different feature formats, and timing <ref type="bibr" coords="6,267.69,104.06,14.04,9.96" target="#b29">[30]</ref>. A potential way to achieve the target temporal fusion of multisensory data and context is to use learned probabilistic models like Dynamic Bayesian Networks (DBN) <ref type="bibr" coords="6,234.02,135.74,14.06,9.96" target="#b28">[29]</ref>, <ref type="bibr" coords="6,254.15,135.74,14.06,9.96" target="#b14">[15]</ref>. Note, however, that classical DBN learning methods can fail when the data exhibits complex behavior, as is the case with spontaneously occurring expressive behaviors. Iteratively learning sets of DBN models in a supervised manner, in which learning is optimized for classification performance <ref type="bibr" coords="6,151.22,188.54,14.09,9.96" target="#b15">[16]</ref>, may prove successful in that case.</p><p xml:id="_eYCVsKn">If we consider the state of the art in audio and visual signal processing, noisy and partial input data should also be expected.</p><p xml:id="_r3s8dhH">A multimodal system should be able to deal with imperfect data and generate its conclusion so that the certainty associated with it varies in accordance to the input data. A way of achieving this is to consider the time-instance versus time-scale dimension of human nonverbal communicative signals as suggested by Pantic and Rothkrantz <ref type="bibr" coords="6,116.31,279.02,14.07,9.96" target="#b29">[30]</ref>. By considering previously observed data (time scale) with respect to the current data carried by functioning observation channels (time instance), a statistical prediction and its probability might be derived about both the information that have been lost due to malfunctioning/inaccuracy of a particular sensor and the currently displayed action/reaction. Probabilistic graphical models, such as Hidden Markov Models (HMM) and DBN are well suited for accomplishing this. These models can handle noisy features, temporal information, and partial data all by probabilistic inference. Hierarchical HMM-based systems <ref type="bibr" coords="6,283.35,374.05,10.68,9.96" target="#b5">[6]</ref> have proven successful for facial expression recognition. DBN and HMM variants <ref type="bibr" coords="6,131.76,395.17,15.20,9.96" target="#b14">[15]</ref> seem to perform well for user intent recognition, office activity recognition, and event detection from realistic audiovisual stream <ref type="bibr" coords="6,158.02,416.29,14.03,9.96" target="#b13">[14]</ref>. This suggests that probabilistic graphical models are a promising approach to fusing realistic (noisy) audio and video for context-dependent detection of behavioral events such as affective states.</p><p xml:id="_g9E4pkM">An issue that makes the problem of human affect recognition even more difficult to solve in a general case is the dependency of a person's behavior on his/her personality, cultural, social network, and the context in which the observed behavioral cues are encountered. One source of help for these problems is machine learning: rather than using a priori rules to interpret human behavior, we can potentially learn context-dependent rules by watching the user's behavior in the sensed context <ref type="bibr" coords="6,254.46,538.45,14.07,9.96" target="#b33">[34]</ref>, <ref type="bibr" coords="6,276.53,538.45,14.07,9.96" target="#b30">[31]</ref>. Probabilistic graphical models may be a promising approach. Sets of such models can be learned in an iterative manner by building on previously acquired knowledge when learning new models. For instance, a probabilistic graphical model for affect recognition learned from data about a certain user can be used as a starting point for learning such a model for another user or for learning a new model for the same user and a different context. Though context sensing and the time needed to learn appropriate probabilistic graphical models are significant problems in their own right, many benefits could come from adaptive, contextual, multimodal, affect analyzers.</p><p xml:id="_9eBsAAy">To develop and evaluate the envisioned contextual multimodal human-affect analyzers, large collections of training and test data are needed. Nonetheless, there is no comprehensive, readily accessible reference set of audiovisual data that could be used as a basis for benchmarks for efforts in the field. Benchmark databases should contain still and motion images of faces and upper bodies (to facilitate the research on human affect analysis from the face and the body), vocalizations and speech (to facilitate the research on vocal affect analysis), and metadata concerning both the context in which the recorded affective expressions were displayed and interpretations of these in terms of shown face and body actions, emotional and attitudinal states. Also, databases should contain existing research findings in order to facilitate the integration of efforts of researchers, highlighting contradictions and consistencies, and suggesting fruitful paths for new research.</p><p xml:id="_jhRB7Kn">The lack of such easily accessible, suitable, common testing resources forms a major impediment to comparing and extending the issues concerned with automatic human affect analysis.</p><p xml:id="_fguwbYs">Two main issues that make this problem difficult to tackle are those of obtaining the ground truth for the observation data and getting data that genuinely correspond to a particular affective state. Even though there are cases when the data can be easily labeled (e.g., a singular strong emotion is captured, such as an episode of rage), in most cases the ground truth (which affective state was present) is difficult to establish. Furthermore, as any photographer can attest, getting a real smile can be challenging. Asking someone to smile often does not create the same picture as an authentic smile. The fundamental reason of course is that the subject often does not feel happy so his/her smile is artificial and in many subtle ways quite different than a genuine smile <ref type="bibr" coords="6,523.55,342.36,14.05,9.96" target="#b11">[12]</ref>, <ref type="bibr" coords="6,543.43,342.36,9.79,9.96" target="#b8">[9]</ref>.</p><p xml:id="_caJSCGf">Picard et al. <ref type="bibr" coords="6,365.28,358.92,15.19,9.96" target="#b36">[37]</ref> outlined five factors that influence the affective data collection:</p><p xml:id="_rMHhHdy">• Spontaneous versus posed: Is the emotion elicited by a situation or stimulus that is outside the subject's control or the subject is asked to elicit the emotion? • Lab setting versus real-world: Is the data recording taking place in a lab or in the usual environment of the subject? • Expression versus feeling: Is the emphasis on external expression or on internal feeling? • Open recording versus hidden recording: Is the subject aware that (s)he is being recorded? • Emotion-purpose versus other-purpose: Does the subject know that (s)he is a part of an experiment and the experiment is about emotion?</p><p xml:id="_hKMs5EK">Note that these factors are not necessarily independent. The most natural setup would imply that the subject feels the emotion internally (feeling), the emotion occurs spontaneously, while the subject is in his usual environment (real-world). Also, the subject should not know that (s)he is being recorded (hidden recording) and that (s)he is a part of an experiment (other-purpose). Such data are usual impossible to obtain because of privacy and ethics concerns. As a consequence, most researchers who tackled the problem of establishing a comprehensive human-affect expression database used a setup that is rather far from the natural setup. Except of these problems concerned with acquiring valuable data and the related ground truth, another important issue is how does one construct and administer such a large audiovisual benchmark database. The related questions are the following. How does one facilitate efficient, fast, and secure retrieval and inclusion of objects constituting this database? How could fast and reliable object distribution over networks be achieved? How could the performance of a tested automated system be included in the database? How should the relationship between the performance and the database objects used in the evaluation be defined? Pantic et al. <ref type="bibr" coords="7,77.42,226.22,14.09,9.96" target="#b29">[30]</ref>, <ref type="bibr" coords="7,99.02,226.22,15.24,9.96" target="#b31">[32]</ref> emphasized a number of specific research and development efforts needed to address the aforementioned problems. Nonetheless, note that their list of suggestions and recommendations is not exhaustive of worthwhile contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7." xml:id="_mqkVTbV">CONCLUSSIONS</head><p xml:id="_SnxKJB7">As remarked by Pentland <ref type="bibr" coords="7,161.29,294.62,15.19,9.96" target="#b33">[34]</ref> and Oviatt <ref type="bibr" coords="7,229.45,294.62,14.05,9.96" target="#b25">[26]</ref>, multimodal context-sensitive (user-, task-, and application-profiled and affectsensitive) HCI is likely to become the single most widespread research topic of AI research community. Breakthroughs in such HCI designs could bring about the most radical change in the computing world; they could change not only how professionals practice computing, but also how mass consumers conceive and interact with the technology. However, many aspects of this "new generation" HCI technology, in particular ones concerned with the interpretation of human behavior at a deeper level and the provision of the appropriate response, are not yet mature and need many improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_FbDZENr">Main challenges in the field of affective multimodal HCI:</head><p xml:id="_s2sqNRc">• How many and which behavioral channels like the face, the body, and the tone of the voice, should be combined for realization of robust and accurate human affect analysis? Too much information from different channels seems to be confusing for human judges. Does this pertain in HCI? • At which abstraction level are these modalities to be fused?</p><p xml:id="_RZz3jyw">Humans simultaneously employ tightly coupled modalities of sight and sound. Does this tight coupling persists when the modalities are used for affective multimodal HCI, as suggested by, e.g., Chen and Rao <ref type="bibr" coords="7,190.34,534.37,9.79,9.96" target="#b5">[6]</ref>, or not, as suggested by Cohn and Katz <ref type="bibr" coords="7,123.95,544.93,10.68,9.96" target="#b7">[8]</ref> and Scanlon and Reilly <ref type="bibr" coords="7,223.37,544.93,15.46,9.96" target="#b38">[39]</ref>? • How can the grammar of human expressive behavior be learned? Should this be done in a human-centered manner or in an activity-centered manner as suggested by Norman <ref type="bibr" coords="7,67.44,587.77,15.47,9.96" target="#b25">[26]</ref>? How can this information be properly represented and then used to handle malfunctioning / inaccuracy of a particular observation channel and the resulting ambiguities in the observation data? • How can the interpretation of the observed expressive behavior in terms of any emotion-/ attitude-/ mood be achieved? How can the system learn to distinguish between these interpretation classes when for the face channel only there are more than 7000 different facial expressions that humans can display <ref type="bibr" coords="7,142.34,683.41,15.49,9.96" target="#b39">[40]</ref>? We believe that researchers in the field should not focus on solving challenges in psychology of emotion and should not adhere to only one of emotion theories (e.g. discrete vs. dimensional emotion theory, emotion vs. behavioral ecology theory, etc.). Rather, they should focus on finding pragmatic context-sensitive solutions by learning appropriate models of expressive behavior and the related interpretations from available data and intended users. • How to include information about the context (environment, user, user's task) in which the observed expressive behavior has been displayed so that a context-sensitive analysis of human behavior can be achieved? • What properties should automated analyzers of human expressive behavior have in order to be able to analyze human spontaneous behavior? How can such analyzers be realized? How should one elicit spontaneous human expressive behavior including genuine emotional responses? • United efforts of different research groups working in the field should be made to develop a comprehensive, readily accessible database of annotated, multi-sensory observations of human expressive behavior that could be used as a basis for benchmarks for efforts in the field. The related research questions include the following. How does one facilitate efficient, fast, and secure retrieval and inclusion of objects constituting this database? How could the performance of a tested automated system be included into the database? How should the relationship between the performance and the database objects used in the evaluation be defined?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,317.87,621.35,240.18,94.44"><head></head><label></label><figDesc xml:id="_DJUJ6Nq">This setup had the main advantage that it naturally attracted people to watch and could potentially elicit emotions through different genres of video footage -i.e. horror films for shock, comedy for joy, etc.</figDesc><table coords="6,317.88,621.35,240.14,41.64"><row><cell>Cohn and Kanade [22], Pantic and Valstar [32], and Lyons [47]</cell></row><row><cell>collected facial affect data, Banse and Scherer [3] and Nwe et al.</cell></row><row><cell>[26] collected vocal affect data, while De Silva and Ng [8], and</cell></row><row><cell>Chen [5] collected audiovisual human affect data using a posed,</cell></row></table><note xml:id="_VuNCwbS">lab-based, expression-oriented, open-recording, and emotionpurpose methodology. So far only Sebe et al.<ref type="bibr" coords="6,495.78,674.15,15.22,9.96" target="#b39">[40]</ref> reported on efforts in collecting spontaneous audiovisual human affect data. They created a video kiosk (lab setting) with a hidden camera (hidden-recording) which displayed segments from recent movie trailers.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8." xml:id="_BEN6g7j">ACKNOWLEDGMENTS</head><p xml:id="_EK5bRCn">The work of Maja Pantic is supported by the Netherlands Organization for Scientific Research Grant EW-639.021.202. Jeffrey F Cohn is supported in part by NIH grant MHR01 MH051435 and Naval Research Laboratory grant N000140010915.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="7,335.88,464.90,201.90,9.96;7,335.88,475.46,201.96,9.96;7,335.88,486.02,210.05,9.96;7,335.88,496.58,32.59,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_cbN2DFA">Thin slices of expressive behavior as predictors of interpersonal consequences: A meta-analysis</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ambady</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rosenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_pZU8qfT">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="256" to="274" />
			<date type="published" when="1992-02">Feb. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.88,511.10,221.53,9.96;7,335.88,521.66,204.00,9.96;7,335.88,532.22,189.40,9.96;7,335.88,542.78,220.05,9.96;7,335.88,553.34,196.64,9.96;7,335.88,563.90,122.58,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_MNQVKgQ">Emotion Analysis in Man-Machine Interaction Systems</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Balomenos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Raouzaiou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioannou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Drosopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karpouzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kollias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_aZWURNr">Machine Learning for Multimodal Interaction</title>
		<title level="s" xml:id="_ZYAX9mb">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, D</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3361</biblScope>
			<biblScope unit="page" from="318" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.88,578.42,194.98,9.96;7,335.88,588.98,197.20,9.96;7,335.88,599.54,115.23,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_aMSbunS">Acoustic profiles in vocal emotion expression</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Banse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_anV2rPP">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="614" to="636" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.88,614.06,193.28,9.96;7,335.88,624.62,219.79,9.96;7,335.88,635.18,138.39,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_Qh9brFw">Emotional expressions in audiovisual human computer interaction</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_9d77kSX">Proc. Int&apos;l Conf. Multimedia and Expo</title>
				<meeting>Int&apos;l Conf. Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="423" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.88,649.70,217.72,9.96;7,335.88,660.26,220.66,9.96;7,335.88,670.82,206.22,9.96;7,335.88,681.38,66.41,9.96" xml:id="b4">
	<monogr>
		<title level="m" type="main" xml:id="_8CUpKUU">Joint processing of audio-visual information for the recognition of emotional expressions in human-computer interaction</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>University of Illinois at Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="8,72.00,72.38,186.93,9.96;8,72.00,82.94,216.55,9.96;8,72.00,93.50,79.96,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_U9e2RnT">Audio-visual integration in multimodal communication</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kZWCVpQ">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="837" to="852" />
			<date type="published" when="1998-05">May 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,108.02,214.90,9.96;8,72.00,118.58,217.55,9.96;8,72.00,129.14,221.00,9.96;8,72.00,139.70,119.81,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_SfmtChN">Facial expression recognition from video sequences: Temporal and static modeling</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Rkf8HPt">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="160" to="187" />
			<date type="published" when="2003-02">Jan/Feb 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,154.22,221.56,9.96;8,72.00,164.78,212.19,9.96;8,72.00,175.34,215.85,9.96;8,72.00,185.90,11.35,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_GS2Pcjm">Bimodal expression of emotion by face and voice</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_gvC8ueW">Proc. ACM and ATR Workshop on Face and Gesture Recognition and Their Applications</title>
				<meeting>ACM and ATR Workshop on Face and Gesture Recognition and Their Applications</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="41" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,200.42,214.28,9.96;8,72.00,210.98,215.78,9.96;8,72.00,221.54,157.01,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_xuppGQs">The timing of facial motion in posed and spontaneous smiles</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_CRjX8RK">Wavelets, Multiresolution and Information Processing</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,71.96,236.06,221.14,9.96;8,72.00,246.62,216.93,9.96;8,72.00,257.18,15.91,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_np3dQaA">Bimodal emotion recognition</title>
		<author>
			<persName coords=""><forename type="first">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">C</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_72rhKD4">Proc. Int&apos;l Conf. Face and Gesture Recognition</title>
				<meeting>Int&apos;l Conf. Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="332" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,71.97,271.70,205.64,9.96;8,72.00,282.26,182.61,9.96;8,72.00,292.82,95.56,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_s56AB2c">The repertoire of nonverbal behavioral categories -origins, usage, and coding</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">F</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qxgeA6z">Semiotica</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="98" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,71.96,307.34,220.83,9.96;8,72.00,317.89,205.75,9.96;8,72.00,328.45,220.36,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_WFekMZ8">Behavioral markers and recognizability of the smile of enjoyment</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mfnPCFf">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="83" to="93" />
			<date type="published" when="1993-01">Jan. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,71.98,342.97,218.95,9.96;8,72.00,353.53,194.96,9.96;8,72.00,364.09,204.48,9.96;8,72.00,374.65,141.13,9.96" xml:id="b12">
	<monogr>
		<title level="m" type="main" xml:id="_veEwbEv">The new ethology of human facial expression. The psychology of facial expression</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Fridlund</surname></persName>
		</author>
		<editor>Russell, J.A. and Fernandez-Dols, J.M.</editor>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page" from="103" to="129" />
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,71.97,389.17,212.28,9.96;8,72.00,399.73,217.20,9.96;8,72.00,410.29,217.16,9.96;8,72.00,420.85,200.54,9.96;8,72.00,431.41,20.47,9.96" xml:id="b13">
	<monogr>
		<title level="m" type="main" xml:id="_wvbwkjs">Modeling video using input/output Markov models with application to multimodal event detection. Handbook of Video Databases: Design and Applications</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<editor>B. Furth, O. Marques, and B. Furth</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,71.95,445.93,193.35,9.96;8,72.00,456.49,210.25,9.96;8,72.00,467.05,192.87,9.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_dEKYEwA">Audio-visual speaker detection using dynamic Bayesian networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_8uTxjDe">Proc. Int&apos;l Conf. Face and Gesture Recognition</title>
				<meeting>Int&apos;l Conf. Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="384" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,71.97,481.57,220.50,9.96;8,72.00,492.13,191.66,9.96;8,72.00,502.69,203.32,9.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_BJRRtnk">Boosted learning in dynamic Bayesian networks for multimodal speaker detection</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ykmKUD7">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1355" to="1369" />
			<date type="published" when="2003-09">Sep. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,71.96,517.21,208.51,9.96;8,72.00,527.77,206.88,9.96;8,72.00,538.33,209.25,9.96;8,72.00,548.89,64.49,9.96" xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_36EcSr8">Emotion recognition from facial image and speech signal</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">C</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_huE9My4">Proc. Conf. of the Society of Instrument and Control Engineers</title>
				<meeting>Conf. of the Society of Instrument and Control Engineers</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="2890" to="2895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,71.96,563.41,210.28,9.96;8,72.00,573.97,83.42,9.96" xml:id="b17">
	<monogr>
		<title level="m" type="main" xml:id="_bwSNe93">Emotional Intelligence</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Goleman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Bantam Books</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,71.95,588.49,215.28,9.96;8,72.00,599.05,221.14,9.96;8,72.00,609.61,213.53,9.96;8,72.00,620.17,135.77,9.96" xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_cbeAs4Z">A survey on visual surveillance of object motion and behaviors</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9NpGa3R">IEEE Trans. On Systems, Man, and Cybernetics -Part C: Applications and Reviews</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="334" to="352" />
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,71.96,634.69,202.01,9.96;8,72.00,645.25,199.96,9.96;8,72.00,655.81,113.46,9.96" xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_GayVRtQ">Has the Internet become indispensable?</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">L</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">P</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_m7PjZun">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="37" to="42" />
			<date type="published" when="2004-07">July 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,71.98,670.33,217.46,9.96;8,72.00,680.89,199.76,9.96;8,72.00,691.45,212.65,9.96;8,72.00,702.00,160.35,9.96" xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_5aYJhjS">Vocal expression of affect</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Juslin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_CNNJAcK">The New Handbook of Methods in Nonverbal Behavior Research</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Harrigan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Rosenthal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.85,72.36,220.98,9.96;8,335.88,82.92,214.85,9.96;8,335.88,93.48,125.32,9.96" xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_2Xw9sAk">Comprehensive database for facial expression analysis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_pFFtN9e">Proc. Int&apos;l Conf. Face and Gesture Recognition</title>
				<meeting>Int&apos;l Conf. Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="46" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.85,108.00,204.09,9.96;8,335.88,118.56,221.34,9.96;8,335.88,129.12,212.38,9.96" xml:id="b22">
	<monogr>
		<title level="m" type="main" xml:id="_yFnSEuQ">Facial expression of emotion. Handbook of Emotions</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keltner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<editor>Lewis, M. and Haviland-Jones, J.M.</editor>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Guilford Press</publisher>
			<biblScope unit="page" from="236" to="249" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.85,143.64,215.46,9.96;8,335.88,154.20,211.42,9.96;8,335.88,164.76,212.07,9.96;8,335.88,175.32,174.76,9.96" xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_BevAhSb">Promises and problems with the circumplex model of emotion</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Diener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_hp6DbJ9">Review of Personality and Social Psychology</title>
				<editor>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Clark</surname></persName>
		</editor>
		<meeting><address><addrLine>Newbury Park, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Sage Publications</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="25" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.82,189.84,195.20,9.96;8,335.88,200.40,212.55,9.96" xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_xKxnDy3">Cultural similarities and differences in display rules</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_DG6uksJ">Motivation and Emotion</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="195" to="214" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.85,214.92,215.89,9.96;8,335.88,225.48,179.57,9.96" xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_84gCYEd">Human-centered design considered harmful</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ksvF2DK">ACM Interactions</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="14" to="19" />
			<date type="published" when="2005-08">July/Aug. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.84,240.00,219.60,9.96;8,335.88,250.56,195.86,9.96;8,335.88,261.12,211.41,9.96;8,335.88,271.68,15.91,9.96" xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_yrregpR">Speaker Dependent Emotional Speech Recognition Using Hidden Markov Models</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Nwe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">C</forename><surname>De Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_y6j6e8J">Speech Communications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="603" to="623" />
			<date type="published" when="2003-11">Nov. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.85,286.20,188.38,9.96;8,335.88,296.76,216.58,9.96;8,335.88,307.32,67.49,9.96" xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_JmZ79ZE">User-centered modeling and evaluation of multimodal interfaces</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Oviatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_pszpEbS">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1457" to="1468" />
			<date type="published" when="2003-09">Sep. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.82,321.84,221.20,9.96;8,335.88,332.40,218.26,9.96;8,335.88,342.96,210.35,9.96;8,335.88,353.52,38.69,9.96" xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_kxE8vdP">Exploiting the dependencies in information fusion</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">J</forename><surname>Anastasio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_vWKu7M3">Proc. IEEE Conf. on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="407" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.83,368.04,192.40,9.96;8,335.88,378.60,189.22,9.96;8,335.88,389.16,203.32,9.96" xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_fXhG7HZ">Toward an Affect-Sensitive Multimodal Human-Computer Interaction</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Uk2CqcB">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1370" to="1390" />
			<date type="published" when="2003-09">Sep. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.84,403.68,216.85,9.96;8,335.88,414.24,211.18,9.96;8,335.88,424.80,200.55,9.96" xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_YvZrtnm">Case-based reasoning for user-profiled recognition of emotions from face images</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Be8Aj3A">Proc. Int&apos;l Conf. Multimedia and Expo</title>
				<meeting>Int&apos;l Conf. Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="391" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.83,439.32,219.09,9.96;8,335.88,449.88,212.53,9.96;8,335.88,460.43,213.13,9.96" xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_XvVEGaD">Webbased database for facial expression analysis</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rademaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Maat</surname></persName>
		</author>
		<ptr target="www.mmifacedb.com" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_zBCqVyf">Proc. Int&apos;l Conf. Multimedia and Expo</title>
				<meeting>Int&apos;l Conf. Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.82,474.95,208.47,9.96;8,335.88,485.51,206.23,9.96;8,335.88,496.07,205.21,9.96;8,335.88,506.63,135.29,9.96" xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_46RBfAe">Embodied Contextual Agent in Information Delivering Application</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pelachaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Carofiglio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>De Carolis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>De Rosis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Poggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_E87f9zG">Proc. Int&apos;l Conf. Autonomous Agents &amp; Multi-Agent Systems</title>
				<meeting>Int&apos;l Conf. Autonomous Agents &amp; Multi-Agent Systems</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.85,521.15,215.01,9.96;8,335.88,531.71,220.47,9.96;8,335.88,542.27,195.40,9.96" xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_GTHZ7uh">Looking at people: Sensing for ubiquitous and wearable computing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Ayrq4K4">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="107" to="119" />
			<date type="published" when="2000-01">Jan. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.84,556.79,162.95,9.96;8,335.88,567.35,215.33,9.96" xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_994RWfY">Socially aware computation and communication</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XdUV657">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="33" to="40" />
			<date type="published" when="2005-03">Mar. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.86,581.87,183.83,9.96;8,335.88,592.43,106.24,9.96" xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Computing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.88,606.95,191.10,9.96;8,335.88,617.51,210.18,9.96;8,335.88,628.07,213.66,9.96;8,335.88,638.63,160.22,9.96" xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_zKwAMF7">Toward machine emotional intelligence: Analysis of affective physiological state</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vyzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Healey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_haMHEpn">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1175" to="1191" />
			<date type="published" when="2001-10">Oct. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.85,653.15,214.56,9.96;8,335.88,663.71,221.69,9.96;8,335.88,674.27,32.59,9.96" xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_q8wtbh2">Is there universal recognition of emotion from facial expression?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rCqY6up">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="102" to="141" />
			<date type="published" when="1994-01">Jan. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.94,72.38,212.65,9.96;9,72.00,82.94,214.17,9.96;9,72.00,93.50,99.99,9.96" xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_VpYC2st">Feature analysis for automatic speech reading</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Scanlon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Reilly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wTjvpn2">Proc. Int&apos;l Workshop Multimedia Signal Processing</title>
				<meeting>Int&apos;l Workshop Multimedia Signal essing</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="625" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.96,108.02,217.88,9.96;9,72.00,118.58,216.05,9.96;9,72.00,129.14,87.28,9.96" xml:id="b39">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<title level="m" xml:id="_nmnXF8U">Handbook of methods in non-verbal behavior research</title>
				<meeting><address><addrLine>Cambridge, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.95,143.66,211.69,9.96;9,72.00,154.22,201.13,9.96;9,72.00,164.78,192.87,9.96" xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_DbmMMb7">Authentic facial expression analysis</title>
		<author>
			<persName coords=""><forename type="middle">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wAbjV2f">Proc. Int&apos;l Conf. Face and Gesture Recognition</title>
				<meeting>Int&apos;l Conf. Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="517" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.97,179.30,206.37,9.96;9,72.00,189.86,215.23,9.96;9,72.00,200.42,222.04,9.96" xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_grCECTR">Audio-visual based emotion recognition -A new approach</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Aj92KrE">Proc. Int&apos;l Conf. Computer Vision and Pattern Recognition</title>
				<meeting>Int&apos;l Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1020" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.95,214.94,216.26,9.96;9,72.00,225.50,200.00,9.96;9,72.00,236.06,210.11,9.96;9,335.88,72.38,216.79,9.96;9,335.88,82.94,169.48,9.96" xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_x5Mrzfj">Testing a tripartite model: II. Exploring the symptom structure of anxiety and depression in student, adult, and patient samples</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Smith-Assenheimer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Mccormick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_BwuqHWz">Journal of Abnormal Psychology</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="15" to="25" />
			<date type="published" when="1995-01">Jan 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,335.83,97.46,199.10,9.96;9,335.88,108.02,202.28,9.96;9,335.88,118.58,221.88,9.96;9,335.88,129.14,193.39,9.96;9,335.88,139.69,164.92,9.96" xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_2D5ANCS">Testing a tripartite model: I. Evaluating the convergent and discriminant validity of anxiety and depression symptom scales</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Assenheimer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Mccormick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_CAGsvv6">Journal of Abnormal Psychology</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="1995-01">Jan 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,335.88,154.21,196.14,9.96;9,335.88,164.77,121.13,9.96" xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_XrgnnRZ">Reading human faces</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wierzbicka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_U6SdyPt">Pragmatics and Cognition</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="1993-01">Jan. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,335.84,179.29,222.04,9.96;9,335.88,189.85,220.10,9.96;9,335.88,200.41,186.61,9.96;9,335.88,210.97,161.55,9.96" xml:id="b45">
	<analytic>
		<title level="a" type="main" xml:id="_6PGbnat">Effect of sensor fusion for recognition of emotional states using voice, face image and thermal image of face</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yoshitomi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kawano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kitazoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_DPvYZ5T">In Proc. Int&apos;l Workshop on Robot-Human</title>
		<imprint>
			<biblScope unit="page" from="178" to="183" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
