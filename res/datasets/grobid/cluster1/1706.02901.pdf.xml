<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_sGujAb6">Characterizing Types of Convolution in Deep Convolutional Recurrent Neural Networks for Robust Speech Emotion Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-01-13">13 Jan 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,103.20,150.85,193.29,10.55"><roleName>Student Member, IEEE</roleName><forename type="first">Che-Wei</forename><surname>Huang</surname></persName>
							<email>cheweihu@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Signal and Image Processing Institute</orgName>
								<orgName type="laboratory">Signal Analysis and Interpretation Laboratory (SAIL)</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.64,150.85,181.10,10.55"><roleName>Fellow, IEEE</roleName><forename type="first">Shrikanth</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Signal and Image Processing Institute</orgName>
								<orgName type="laboratory">Signal Analysis and Interpretation Laboratory (SAIL)</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_uXyQUqZ">Characterizing Types of Convolution in Deep Convolutional Recurrent Neural Networks for Robust Speech Emotion Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-01-13">13 Jan 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">1C00C02AFEAA32FD863DB167CCFAA4C2</idno>
					<idno type="arXiv">arXiv:1706.02901v2[cs.LG]</idno>
					<note type="submission">SUBMITTED TO THE IEEE TRANSACTIONS 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-05-07T13:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_5UvJEnM">Deep Convolutional Recurrent Neural Networks</term>
					<term xml:id="_dUykDqj">Affective Computing</term>
					<term xml:id="_wYYUq7q">Speech Emotion Recognition</term>
					<term xml:id="_6NHyvYZ">Spectral Convolution</term>
					<term xml:id="_mGpfJW2">Temporal Convolution</term>
					<term xml:id="_Kez78PE">Spectral-Temporal Convolution</term>
					<term xml:id="_NSGmjSm">Full-Spectrum Temporal Convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_yRT8MyJ"><p xml:id="_YRERg63">Deep convolutional neural networks are being actively investigated in a wide range of speech and audio processing applications including speech recognition, audio event detection and computational paralinguistics, owing to their ability to reduce factors of variations, for learning from speech. However, studies have suggested to favor a certain type of convolutional operations when building a deep convolutional neural network for speech applications although there has been promising results using different types of convolutional operations. In this work, we study four types of convolutional operations on different input features for speech emotion recognition under noisy and clean conditions in order to derive a comprehensive understanding. Since affective behavioral information has been shown to reflect temporally varying of mental state and convolutional operation are applied locally in time, all deep neural networks share a deep recurrent sub-network architecture for further temporal modeling. We present detailed quantitative module-wise performance analysis to gain insights into information flows within the proposed architectures. In particular, we demonstrate the interplay of affective information and the other irrelevant information during the progression from one module to another. Finally we show that all of our deep neural networks provide state-of-the-art performance on the eNTERFACE'05 corpus.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_6twhbS2">INTRODUCTION</head><p xml:id="_vJxKEgJ">E Motion plays a fundamental role in our daily lives for effective communication, which underlies the abilities of humans to interact, collaborate, empathize and even compete with others. Researchers have been working on understanding human emotion, or in general human behaviors <ref type="bibr" coords="1,48.00,437.38,10.06,9.42" target="#b0">[1]</ref>, for years from both psychological and computational perspectives for several reasons including because it serves as a lens to observe the dynamics of one's internal mental state. Moreover, with the advent of artificially intelligent agents, it is hardly an overstatement to stress the importance of emotion recognition in supporting natural and engaging human-machine interaction.</p><p xml:id="_6cjp7GH">Human behavioral cues often mix and manifest multiple sources of information together. To robustly recover affective information from multiplexed behavioral cues renders emotion recognition a challenging task. For example, speech contains not only linguistic content of what is said but also attributes of the speaker such as identity, gender, age, speaking style, and language background as well as information about the environment and context. All of these factors are entangled and transmitted through a single channel during speech articulation. Speech emotion recognition, therefore, involves the inverse process of disentangling these signals and identifying affective information.</p><p xml:id="_uMPNqKA">A multitude of studies on the subject of emotion recog-nition have discovered a number of emotion-related parameters based on prior knowledge of psychology, speech science, vision science and through signal processing and machine learning approaches. The commonly used features include pitch, log-Mel filterbank energies (log-Mels) <ref type="bibr" coords="1,312.00,437.38,10.06,9.42" target="#b1">[2]</ref>, Mel-frequency cepstral coefficients (MFCCs) <ref type="bibr" coords="1,531.48,437.38,11.08,9.42" target="#b2">[3]</ref> and perceptual linear prediction in the acoustic modality, and Haar, local binary pattern, histogram of oriented gradients and scale-invariant feature transform in visual modality.</p><p xml:id="_X53yQcW">A variety of classifiers based on these features have been reported to perform well. In particular, an extensive feature set consisting of thousands of hand-engineered parameters has been recommended in the past few INTERSPEECH challenges <ref type="bibr" coords="1,359.28,529.66,10.06,9.42" target="#b3">[4]</ref>, and in a recent meta research review article <ref type="bibr" coords="1,312.00,541.30,10.06,9.42" target="#b4">[5]</ref>.</p><p xml:id="_fcKSbhB">In addition to hand-crafted feature engineering, deep learning <ref type="bibr" coords="1,351.84,564.34,10.06,9.42" target="#b5">[6]</ref>, <ref type="bibr" coords="1,370.20,564.34,10.06,9.42" target="#b6">[7]</ref>, <ref type="bibr" coords="1,388.56,564.34,10.96,9.42" target="#b7">[8]</ref> provides an alternative approach to formulate appropriate features for the task at hand. In the last few years, convolutional neural networks (CNNs) <ref type="bibr" coords="1,550.56,587.38,10.06,9.42" target="#b8">[9]</ref>, <ref type="bibr" coords="1,312.00,598.90,14.57,9.42" target="#b9">[10]</ref>, <ref type="bibr" coords="1,333.48,598.90,14.57,9.42" target="#b10">[11]</ref>, <ref type="bibr" coords="1,354.96,598.90,14.48,9.42" target="#b11">[12]</ref>, <ref type="bibr" coords="1,376.44,598.90,15.76,9.42" target="#b12">[13]</ref> have demonstrated outstanding performances in various applications including image recognition, object detection, and recently speech acoustic modeling. Compared to hand-crafted features, a CNN that learns from a large number of training samples via a deep architecture can capture a higher-level representation for the taskspecific knowledge distilled from annotated data. In the area of speech emotion recognition, several researchers have investigated the effectiveness of CNNs into automatically learning affective information from signal data <ref type="bibr" coords="1,523.08,702.82,14.57,9.42" target="#b13">[14]</ref>, <ref type="bibr" coords="1,545.76,702.82,14.57,9.42" target="#b14">[15]</ref>, <ref type="bibr" coords="1,312.00,714.34,14.57,9.42" target="#b15">[16]</ref>, <ref type="bibr" coords="1,332.88,714.34,14.48,9.42" target="#b16">[17]</ref>.</p><p xml:id="_J6JYZxC">Information encoded in speech signals is inherently sequential. Moreover, psychological studies have shown that affective information involves a slow temporal evolution of mental states <ref type="bibr" coords="2,121.32,56.62,14.57,9.42" target="#b17">[18]</ref>. Based on this observation, previous studies have also investigated the use of architectures that explicitly model the temporal dynamics, such as hidden Markov models (HMM) <ref type="bibr" coords="2,155.40,91.18,15.76,9.42" target="#b18">[19]</ref> or recurrent neural networks (RNN) <ref type="bibr" coords="2,81.12,102.70,14.48,9.42" target="#b19">[20]</ref>, <ref type="bibr" coords="2,103.92,102.70,14.48,9.42" target="#b20">[21]</ref>, <ref type="bibr" coords="2,126.72,102.70,15.76,9.42" target="#b21">[22]</ref> for recognizing human emotion in speech.</p><p xml:id="_NseZBAp">Furthermore, there is a growing trend in combining CNN and RNN into one architecture and to train the entire model in an end-to-end fashion. The motivation behind a holistic training is derived from the need to avoid greedily enforcing the distribution of intermediate layers to approximate that of labels, which is believed to maximally exploit the advantage of deep learning over traditional learning methods and would lead to an improved performance. For example, Sainath et al. proposed an architecture, called the Convolutional Long Short-Term Memory Deep Neural Networks (CLDNN) model, made up of a few convolutional layers, long short-term memory gated (LSTM) RNN layers and fully connected (FC) layers in the respective order. They trained CLDNNs on the log-Mel filterbank energies <ref type="bibr" coords="2,284.16,275.86,15.88,9.42" target="#b22">[23]</ref> and on the raw waveform speech signal <ref type="bibr" coords="2,234.12,287.38,15.76,9.42" target="#b23">[24]</ref> for speech recognition, and showed that both CLDNN models outperform CNN and LSTM alone or combined. Likewise, Huang et al. <ref type="bibr" coords="2,101.76,321.94,15.88,9.42" target="#b24">[25]</ref> and Lim et al. <ref type="bibr" coords="2,180.00,321.94,15.76,9.42" target="#b25">[26]</ref> reported CLDNN-based speech emotion recognition experiments, on log-Mels and spectrograms respectively, using similar benchmark settings to highlight the superior performance resulting from an endto-end training.</p><p xml:id="_dkFAeVh">In a recent work, Sainath et al. <ref type="bibr" coords="2,217.08,379.66,15.88,9.42" target="#b26">[27]</ref> observed that under a moderately noisy condition, the spectrally only convolutional operation degrades the performance. They hypothesized noise has made it difficult for local filters to learn translation invariance and thus the local decisions are prone to error. Our work is built upon this observation in order to quantitatively investigate whether different types of convolutional operations could show robustness to noise for speech emotion recognition.</p><p xml:id="_bwgEzbv">In this work, we extend our previous work in <ref type="bibr" coords="2,271.68,483.58,15.88,9.42" target="#b24">[25]</ref> to characterize four types of convolutional operations in a CLDNN for speech emotion recognition. We use log-Mels and MFCCs as input to the proposed models depending on their spectral-temporal correlation. In particular, we compare spectral decorrelation power between one type of the convolutional operations and the discrete cosine transformation (DCT), under both clean and noisy conditions. In addition, we quantitatively and visually analyze modules in the proposed CLDNN-based models in order to gain insights into the information flows within the models.</p><p xml:id="_hZtWeRP">Our contributions are multi-fold. First of all, we consider all commonly used convolutional operations for offering a comprehensive understanding, including two types covered in <ref type="bibr" coords="2,81.48,645.10,14.57,9.42" target="#b14">[15]</ref>. Second, unlike previous studies <ref type="bibr" coords="2,242.76,645.10,14.57,9.42" target="#b14">[15]</ref>, <ref type="bibr" coords="2,264.36,645.10,15.76,9.42" target="#b27">[28]</ref> that increased training corpus size internally, we perform data augmentation with a noise corpus. As a result, we evaluate the proposed models under both clean and noisy conditions to quantitatively measure the influence of noise on different types of convolutional operations. To the best of our knowledge, this is the first work to study noise influence on types of convolutional operations. Furthermore, we carry out module-wise evaluation and visualization to analyze the information flows of different factors encoded in speech and their interplay along the depth of an architecture.</p><p xml:id="_ScbgNVD">The outline of this paper is as follows. Section 2 reviews previous related work. Section 3 presents the architecture of the proposed models and Section 4 describes three competitive baseline models. Section 5 introduces the corpus and data augmentation procedure. Section 6 details the experimental settings and the results are interpreted in Section 7. Section 8 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" xml:id="_rEdJUKu">RELATED WORK</head><p xml:id="_cgdyCdb">Before the present era of deep learning, speech emotion recognition systems prevalently relied on a two-stage training approach, where feature engineering and classifier training were performed separately. Commonly used handcrafted features include pitch, MFCC, log-Mels and the recommended feature sets from the INTERSPEECH challenges. Support vector machine (SVM) and extreme learning machine (ELM) were two of the most competitive classifiers. For the ease to compare models, Eyben et al. <ref type="bibr" coords="2,497.88,275.50,11.08,9.42" target="#b4">[5]</ref> summarized the performances by a SVM trained on the INTERSPEECH challenge feature sets over several public corpora. Yan et al. <ref type="bibr" coords="2,327.36,310.06,15.76,9.42" target="#b28">[29]</ref> recently proposed a sparse kernel reduced-rank regression (SKRRR) for bimodal emotion recognition from facial expressions and speech, which has achieved one of the state-of-the-art performances on the eNTERFACE'05 <ref type="bibr" coords="2,548.16,344.74,15.88,9.42" target="#b29">[30]</ref> corpus.</p><p xml:id="_sbd7VY7">Han et al. <ref type="bibr" coords="2,370.56,367.90,15.76,9.42" target="#b30">[31]</ref> employed a multilayer perceptron (MLP) to learn from spliced data frames and took statistics of aggregated frame posteriors as utterance-level features. An MLP-ELM supervised by these utterance features and the corresponding labels has been shown to outperform the MLP-SVM model.</p><p xml:id="_ejGYPVv">It has been known that emotion involves temporal variations of mental state. To exploit this fact, Wöllmer et al. <ref type="bibr" coords="2,312.00,460.42,15.88,9.42" target="#b19">[20]</ref> and Metallinou et al. <ref type="bibr" coords="2,420.72,460.42,15.76,9.42" target="#b20">[21]</ref> conducted experiments at the conversation-level to show that human emotion depends on the context of a long-term temporal relationship using HMM and Bi-directional LSTM (BLSTM). Lee et al. <ref type="bibr" coords="2,548.16,494.98,15.88,9.42" target="#b21">[22]</ref> posed speech emotion recognition at the utterance level as a sequence learning problem and trained an LSTM with a connectionist temporal classification objective to align voiced frames with emotion activation.</p><p xml:id="_FJ2JSma">Deep CNN models were initially applied to computer vision related tasks and have achieved many ground-breaking results <ref type="bibr" coords="2,343.92,575.86,10.06,9.42" target="#b8">[9]</ref>, <ref type="bibr" coords="2,361.44,575.86,14.57,9.42" target="#b9">[10]</ref>, <ref type="bibr" coords="2,383.76,575.86,14.57,9.42" target="#b10">[11]</ref>, <ref type="bibr" coords="2,406.08,575.86,14.57,9.42" target="#b11">[12]</ref>, <ref type="bibr" coords="2,428.40,575.86,14.57,9.42" target="#b12">[13]</ref>. Recently, researchers have started to consider their use in the acoustic domain, including speech recognition <ref type="bibr" coords="2,409.80,598.90,14.48,9.42" target="#b22">[23]</ref>, <ref type="bibr" coords="2,430.68,598.90,14.48,9.42" target="#b23">[24]</ref>, <ref type="bibr" coords="2,451.44,598.90,14.57,9.42" target="#b31">[32]</ref>, <ref type="bibr" coords="2,472.32,598.90,14.57,9.42" target="#b32">[33]</ref>, <ref type="bibr" coords="2,493.20,598.90,14.48,9.42" target="#b33">[34]</ref>, audio event detection <ref type="bibr" coords="2,355.68,610.54,14.48,9.42" target="#b34">[35]</ref>, <ref type="bibr" coords="2,378.72,610.54,15.76,9.42" target="#b35">[36]</ref> and speech emotion recognition <ref type="bibr" coords="2,545.76,610.54,14.57,9.42" target="#b13">[14]</ref>, <ref type="bibr" coords="2,312.00,622.06,14.57,9.42" target="#b14">[15]</ref>, <ref type="bibr" coords="2,334.68,622.06,14.48,9.42" target="#b36">[37]</ref>. Abdel-Hamid et al. <ref type="bibr" coords="2,446.64,622.06,15.76,9.42" target="#b31">[32]</ref> concluded that one of the advantages in using CNNs to learn from less processed features such as raw waveforms, spectrograms and log-Mels is their ability to reduce spectral variation, including speaker and environmental variabilities; this capability is attributed to structures such as local connectivity, weight sharing, and pooling. When training a CNN model for speech emotion recognition, Mao et al. <ref type="bibr" coords="2,490.56,702.82,15.76,9.42" target="#b13">[14]</ref> proposed to learn the filters in a CNN on spectrally whitened spectrograms. The learning, however, is carried out by a sparse auto-encoder in an unsupervised fashion. Anand et al. <ref type="bibr" coords="3,48.00,44.98,15.88,9.42" target="#b14">[15]</ref> benchmarked two types of convolutional operations in their CNN-based speech emotion recognition systems: the spectral-temporally convolutional operation and the fullspectrum temporally convolutional operation (see Fig. <ref type="figure" coords="3,277.32,79.80,4.21,9.13">?</ref>? for details). Their results showed the full-spectrum temporal convolution is more favorable for speech emotion recognition. They also reported the performance of an LSTM trained on the raw spectrograms.</p><p xml:id="_xX2JcT3">Recently, Sainath et al. proposed the CLDNN architecture for speech recognition based on the log-Mels <ref type="bibr" coords="3,264.84,149.02,15.76,9.42" target="#b22">[23]</ref> and the raw waveform signal <ref type="bibr" coords="3,159.96,160.54,14.48,9.42" target="#b23">[24]</ref>, in which both models have been shown to more competitive than a LSTM and a CNN model alone or combined. They also demonstrated that with a sufficient amount of training data (roughly 2, 000 hours), a CLDNN trained on the raw waveform signal can match the one trained on the log-Mels. Moreover, they found the raw waveform and the log-Mels in fact provide complementary information. Based on the CLDNN architecture, Trigeorgis et al. <ref type="bibr" coords="3,70.32,252.82,15.88,9.42" target="#b37">[38]</ref> published a model using the raw waveform signal for continuous emotion tracking. Huang et al. <ref type="bibr" coords="3,243.96,264.46,15.76,9.42" target="#b24">[25]</ref> trained a CLDNN model on the log-Mels for speech emotion recognition and quantitatively analyzed the difference in spectrally decorrelating power between the discrete cosine transformation and the convolutional operation. Lim et al. <ref type="bibr" coords="3,284.16,310.54,15.88,9.42" target="#b25">[26]</ref> repeated the comparison between CNN, LSTM and CLDNN for speech emotion recognition using spectrograms. Ma et al. <ref type="bibr" coords="3,74.28,345.22,15.88,9.42" target="#b38">[39]</ref> applied the CLDNN architecture to classifying depression based on the log-Mels and spectrograms. They employed the full-spectrum temporally convolutional operation on the log-Mels but the temporally-only convolutional operation on the spectrograms.</p><p xml:id="_QNVrrcT">On the multi-modal side, Zhang et al. <ref type="bibr" coords="3,236.16,403.06,15.76,9.42" target="#b15">[16]</ref> fine-tuned the AlexNet on spectrograms and images, separately, for audio-visual emotion recognition but only applied timeaveraging for temporal pooling. Tzirakis et al. <ref type="bibr" coords="3,243.00,437.62,15.88,9.42" target="#b39">[40]</ref> extended the uni-modal work in <ref type="bibr" coords="3,155.04,449.14,15.76,9.42" target="#b37">[38]</ref> to make use of visual cues. They fine-tuned the pre-trained ResNet model <ref type="bibr" coords="3,244.92,460.66,15.76,9.42" target="#b12">[13]</ref> for facial expression recognition and then re-trained the concatenated bimodal network with the LSTM layers re-initialized again.</p><p xml:id="_rTVA4hD">Our work is similar to Anand et al. <ref type="bibr" coords="3,229.32,495.46,15.88,9.42" target="#b14">[15]</ref> because we both report the benchmarking of convolutional types. However, in addition to the novelty aforementioned, we train our models in an end-to-end fashion on log-Mels and MFCCs depending on their locally spectral-temporal correlation. Moreover, we keep the testing partition speakerindependent of the training parition. Ma et al. <ref type="bibr" coords="3,262.56,564.70,15.88,9.42" target="#b38">[39]</ref> also experimented with two types of convolutional operations but they applied them to different features. As a result, it is difficult to draw a fair conclusion from the comparison. This work is also similar to Trigeorgis et al. <ref type="bibr" coords="3,248.40,610.90,14.57,9.42" target="#b37">[38]</ref>, Lim et al. <ref type="bibr" coords="3,61.08,622.42,15.88,9.42" target="#b25">[26]</ref> and Huang et al. <ref type="bibr" coords="3,155.88,622.42,14.48,9.42" target="#b24">[25]</ref>, where all adopt the CLDNN architecture for speech emotion recognition/tracking but the underlying features and the intended goals are different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" xml:id="_v4pB26f">DEEP CONVOLUTIONAL RECURRENT MODELS</head><p xml:id="_9Nuw3cZ">In this section we describe the proposed deep convolutional recurrent networks and details of structurally different convolutional operations on the log-Mels and the MFCCs. Fig. <ref type="figure" coords="3,295.20,714.34,4.75,9.42" target="#fig_0">1</ref> illustrates the overview of the models we design for speech emotion recognition. In Fig. <ref type="figure" coords="3,172.08,737.38,4.75,9.42" target="#fig_0">1</ref> (a), we define four types of convolutional operations depending on the shape of their feature maps. By dividing the convolutional operations into four types, we expect to understand their differences for a finer analysis after they have been optimized to learn from the spectral-temporal signals. In Fig. <ref type="figure" coords="3,468.36,91.18,4.75,9.42" target="#fig_0">1</ref> (b), we depict a deep recurrent neural network, called the LDNN model, as the common sub-network architecture for every model. Two convolutional layers together with the LDNN sub-network make up a CLDNN architecture. As a convolutional layer is applied locally in time, the LDNN model is supposed to model the long-term temporal relationship within an utterance. We only consider spectral-temporal features as input to a CLDNN model. Specifically, an emotional utterance u is represented by a sequence of spectral features {x u t }. These spectral features can either be the log-Mels or the MFCCs depending on the application scenario. All models are presented for a comprehensive study to understand the role a convolutional layer plays in learning the affective information in speech. Overall, we present eight models based on the combinations of the factors including the input features (the log-Mels or the MFCCs) and the type of convolutional operations (spectral only, temporal only, spectral-temporal or full-spectrum temporal). In the following subsections, we give a brief review of the convolutional and recurrent neural layers and introduce corresponding notations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" xml:id="_GaDvmJt">Types of Convolutional Operations</head><p xml:id="_3VGY2MT">A convolutional neural layer Conv that receives an input tensor X ∈ R C×H 0 ×W 0 consists of a convolutional function F κ : R C×H 0 ×W 0 → R K×H 1 ×W 1 , an activation function σ and an optional pooling function</p><formula xml:id="formula_0" coords="3,312.00,392.08,252.00,25.09">F π : R K×H 1 ×W 1 → R K×H 2 ×W 2 .</formula><p xml:id="_enhfnaU">The convolutional function F κ is defined by</p><formula xml:id="formula_1" coords="3,312.00,418.28,252.11,102.68">K feature maps (h k , b k ) ∈ R C×h×w × R H 1 ×W 1 of shape h × w, where the kijth component of F κ (X) is given as F κ (X) kij h k * X hw ij + b kij = C−1 c=0 h−1 µ=0 w−1 ν=0 X ij [c, µ, ν]h k [c, µ, ν] + b kij ,(1) in which X ij [c, µ, ν] = X[c, i • s κ + µ, j • t κ + ν]</formula><p xml:id="_2pgY6AZ">and s κ , t κ are the strides, i.e. the amount of shift, of the filters in the convolutional operation in their respective directions. Likewise, it is straight-forward to formulate the pooling function F π acting on an input Y ∈ R K×H 1 ×W 1 through a filter of shape m × n by the component-wise definition:</p><formula xml:id="formula_2" coords="3,382.80,584.65,181.24,13.00">F π (Y) kij π Y mn kij ,<label>(2)</label></formula><p xml:id="_6NVFUbN">where Y mn kij ∈ R m×n is a sub-tensor of Y lying on the kth slice of Y with its first entry aligned to</p><formula xml:id="formula_3" coords="3,488.64,615.08,75.34,10.41">Y[k, i • s π , j • t π ],</formula><p xml:id="_9fG2sPV">and π is the pooling operation, usually the max or the mean functions. Similarly, s π and t π are the strides of the filters in the pooling operations in their respective directions.</p><p xml:id="_pGDuWzs">Typical choices of the activation functions include the sigmoid function σ(x) = 1 1+exp(−x) , the hyperbolic tangent function σ(x) = tanh(x) and the rectified linear unit (ReLU) σ(x) = max(0, x).</p><p xml:id="_6SqNsN7">Concisely, a convolutional neural layer can be summarized as a function composition In this work, we concentrate entirely on the convolution function F κ and adjust the pooling function F π accordingly. In particular, we are interested in the relationship between the acoustic emotional pattern learnt by the model and the shape of the filter h k in feature maps. To this end, we divide the shapes of the filters h k into four categories to highlight their structural differences: the full-spectrum temporally (FST-Conv), the spectral-temporally (ST-Conv), the temporally only (T-Conv) and the spectrally only (S-Conv) convolutional operations. In what follows, we mathematically define each category.</p><formula xml:id="formula_4" coords="3,388.56,736.65,175.48,10.40">Conv F π • (σ ⊙ F κ ),<label>(3)</label></formula><p xml:id="_yupYte4">FST-Conv: First of all, we consider filters of shape M × w for w ≥ 2, where M denotes the number of spectral bands and w specifies the width on the temporal axis. Since this type of filters covers the entire spectrum, they convolve with the input tensor only in the temporal direction and as a result the pooling function can only perform temporal pooling. This type convolves with global spectral information and models across neighboring frames.</p><p xml:id="_WGWUsEH">ST-Conv: A ST-Conv layer contains filters of shape h × w, where 2 ≤ h ≤ M − 1 and w ≥ 2. This type of filters observes local spectral-temporal information at a time and is free to convolve with the input tensor in both directions. Accordingly, the pooling function also operates on the convolved tensor through a two-dimensional filter.</p><p xml:id="_vGg4Ah8">T-Conv: A T-Conv layer is similar to a FST-Conv layer except that the filters in a T-Conv layer has a shape of 1 × w for w ≥ 2. These filters convolve with the input tensor along the temporal direction from one frequency band to another and ignore spectrally contextual information. The pooling function acts on the convolved tensor along the temporal direction correspondingly.</p><p xml:id="_jc5kQsj">S-Conv: A spectrally only convolutional neural layer consists of filters of shape h × 1, where h ≥ 1 and the pooling function down-samples the convolved tensor along the spectral direction. Note that the S-Conv type is closely related to the traditional signal processing techniques; for example, DCT transformation from log-Mels to MFCCs belongs to this category when h = M except that the filters in DCT are mathematically pre-defined; see Sec. 4.3 for more details.</p><p xml:id="_jrZFMJe">For each type of the convolutional operations, we employ a stride of 1. Since our focus is on the convolutional operations, we employ a fixed pooling size of 3 and a fixed stride of 2 in their respective direction(s) of convolution. Table <ref type="table" coords="5,72.96,218.02,4.75,9.42">1</ref> summarizes the parameters for all Conv layers. TABLE 1: A summary of the parameters for each model architecture. M denotes the spectral dimensionality and var stands for variable parameters for tuning. The dash symbol indicates the situation where the parameter tuning is not applicable.</p><formula xml:id="formula_5" coords="5,72.72,305.48,199.58,46.95">h w m n sκ tκ sπ tπ S-Conv var 1 3 1 1 1 2 1 T-Conv 1 var 1 3 1 1 1 2 ST-Conv var var 3 3 1 1 2 2 FST-Conv M var 1 3 - 1 -<label>2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" xml:id="_pGF7B3J">Deep Recurrent Neural Network</head><p xml:id="_TsEyVNA">Suppose the input is a sequence of vectors {x t }. The Elman type simple recurrent neural network RNN <ref type="bibr" coords="5,238.56,405.82,15.76,9.42" target="#b40">[41]</ref> is defined through the following equations:</p><formula xml:id="formula_6" coords="5,93.24,435.30,206.80,11.91">h t = σ h U hx x t + U hh h t−1 + u h<label>(4)</label></formula><formula xml:id="formula_7" coords="5,93.60,455.22,206.44,11.91">y t = σ y U yh h t + u y ,<label>(5)</label></formula><p xml:id="_VVtnk6f">where h t as an non-linear recurrent transformation of all past history {x s } t s=1 represents the system memory at time t, (U ba , u b ) is an affine mapping from a space of type a to one of type b, and σ c is the activation function for type c. Here x, h and y denote the input, hidden and output vectors, respectively. However, training a simple RNN with the back-propagation algorithm may cause the issues of gradient vanishing or explosion. Although heuristic techniques such as gradient clipping can alleviate the issue of gradient explosion, the gradient vanishing problem is mitigated by an enhanced architecture: the LSTM architecture <ref type="bibr" coords="5,255.24,591.82,9.97,9.42" target="#b7">[8]</ref>.</p><p xml:id="_DMfzMVK">An LSTM is able to decide when to read from the input, to forget the memory or to write an output by controlling a gating mechanism. By definition an LSTM learns the following internal controlling functions:</p><formula xml:id="formula_8" coords="5,92.76,655.48,203.59,11.89">i t = σ i U ix x t + U is s t−1 + u i (<label>6</label></formula><formula xml:id="formula_9" coords="5,296.35,657.70,3.69,9.42">)</formula><formula xml:id="formula_10" coords="5,92.52,675.52,207.52,11.89">f t = σ f U fx x t + U fs s t−1 + u f<label>(7)</label></formula><formula xml:id="formula_11" coords="5,90.24,691.96,209.80,11.89">o t = σ o (U ox x t + U os s t−1 + u o )<label>(8)</label></formula><formula xml:id="formula_12" coords="5,90.24,706.36,209.80,11.89">g t = tanh (U gx x t + U gs s t−1 + u g )<label>(9)</label></formula><formula xml:id="formula_13" coords="5,90.84,722.25,205.23,10.40">c t = c t−1 ⊙ f t + g t ⊙ i t (<label>10</label></formula><formula xml:id="formula_14" coords="5,296.07,722.98,3.97,9.42">)</formula><formula xml:id="formula_15" coords="5,91.44,736.65,204.63,10.40">s t = tanh (c t ) ⊙ o t (<label>11</label></formula><formula xml:id="formula_16" coords="5,296.07,737.38,3.97,9.42">)</formula><p xml:id="_VrnbfXH">where i, f , o, g, c and s represent the input, forget, output, gate, cell and output vectors, respectively. In particular, the change from non-linear multiplicative recurrence in Eq. ( <ref type="formula" coords="5,315.69,79.66,3.69,9.42" target="#formula_6">4</ref>) to linear additive recurrence in Eq. ( <ref type="formula" coords="5,495.01,79.66,7.94,9.42" target="#formula_13">10</ref>) theoretically prevents gradients from vanishing during back-propagating the error through time. Moreover, studies have found that a BLSTM layer can further improve upon a unidirectional LSTM in applications such as speech recognition <ref type="bibr" coords="5,519.00,125.86,14.57,9.42" target="#b41">[42]</ref>, translation <ref type="bibr" coords="5,340.92,137.38,15.88,9.42" target="#b42">[43]</ref> and emotion recognition <ref type="bibr" coords="5,474.48,137.38,14.57,9.42" target="#b19">[20]</ref>, <ref type="bibr" coords="5,497.52,137.38,15.88,9.42" target="#b20">[21]</ref> as it fuses information from the past and the future. Suppose an LSTM : R D1×T → R D2×T takes in a sequence {x t } T t=1 and returns {y f t } T t=1 , and another LSTM :</p><formula xml:id="formula_17" coords="5,312.00,182.65,252.00,33.92">R D1×T → R D2×T takes in a reversed sequence {x T +1−t } T t=1 and returns {y b t } T t=1 . A BLSTM : R D1×T × R D1×T → R (2 * D2</formula><p xml:id="_gpKcMTw">)×T , which is made of two LSTMs, runs on two sequences {x t } T t=1 and {x T +1−t } T t=1 and gives another sequence {z t } T t=1 , where</p><formula xml:id="formula_18" coords="5,411.36,228.37,54.24,13.72">z t = [y f t ; y b t ]</formula><p xml:id="_25Tegj2">is the concatenation of y f t and y b t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3" xml:id="_yDbrDdW">CLDNN-based Models</head><p xml:id="_5FErZmp">Before defining a variety of CLDNN-based models, we introduce a shared sub-network architecture among them. The sub-network contains one BLSTM layer followed by four fully connected feed-forward layers. Each direction of the BLSTM layer has 128 cells so the BLSTM outputs a sequence of vectors in R 256 . We take a mean pooling over the output of the BLSTM layer to obtain the utterance representation c rather than using the output vector at the last time step. A dropout mechanism <ref type="bibr" coords="5,472.08,379.42,15.88,9.42" target="#b43">[44]</ref> of probability 0.2 is fixed and applied to the representation c to regularize the learning process. These four FC layers have their own size of 128, 32, 32, N , respectively, where N denotes the number of emotion classes, in which the first three FC layers are activated by the ReLU and the last one by the softmax function for classification. This architecture based on (B)LSTM and FC layers is conveniently called an LDNN model <ref type="bibr" coords="5,341.64,471.70,14.48,9.42" target="#b22">[23]</ref>. Note that we employ a BLSTM layer instead of an LSTM layer as in <ref type="bibr" coords="5,406.20,483.34,15.88,9.42" target="#b22">[23]</ref> because it has been shown that the ability of a BLSTM to integrate future information into representation learning is beneficial to emotion recognition.</p><p xml:id="_h9NCEaK">In the bottom of the LDNN sub-network, there are two Conv layers. Each Conv layer has 32 feature maps and each of them is activated by the ReLU. Formally, we define X-CLDNN model to be an LDNN sub-network architecture specified above on top of two X-Conv layers, where X ∈ {S, T, ST, FST}.</p><p xml:id="_tDkPXTw">A Conv layer is often said to be local because its feature maps when being computed at a local region on the input tensor depend only on the entries that the feature maps currently overlap with. As a result, we expect the input tensor to preserve locality in both spectral and temporal directions in general. However, due to the aforementioned structural differences, it is reasonable to relax this expectation a little bit accordingly. For example, a ST-Conv certainly requires its input tensor to maintain spectral-temporal correlation locally while a (FS)T-Conv and a S-Conv only need such locality preservation in the temporal or spectral direction, respectively. Taking this issue into consideration, in this work, we apply all four types of the Conv layer to the log-Mels and denote the corresponding CLDNN-based models as X-CLDNN (log-Mels) for X ∈ {S, T, ST, FST}. On the other hand, because the discrete cosine transformation decorrelates the spectral energies, the MFCCs may not maintain locality in the spectral domain. Therefore, we apply only temporal convolutional operations to the MFCCs and denote these CLDNN-based models as X-CLDNN (MFCCs) for X ∈ {T, FST}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" xml:id="_vE4UbuD">BASELINE MODELS</head><p xml:id="_6AJAq2w">We evaluate our CLDNN-based models for understanding the convolutional operations by comparing with three baseline models on a speech emotion recognition task. First of the baseline models uses the low-level descriptors and their statistical functionals within an utterance to train a support vector machine. The other two of the baseline models are based on the BLSTM recurrent neural networks and take the log-Mels and the MFCCs features as its input, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" xml:id="_MAb5xcn">Support vector machine with the Low-Level Descriptors and Their Statistical Functionals</head><p xml:id="_WutxH2X">Many speech scientific studies have empirically found emotion correlating parameters, also known as the low-level descriptors (LLDs), along different aspects of phonation and articulation in speech, such as speech rate in the time domain, fundamental frequency or formant frequency in the frequency domain, intensity or energy in the amplitude domain, or relative energy in different frequency bands in the spectral energy domain. Furthermore, statistical functionals of an entire emotional utterance are derived from the LLDs to obtain global information, complementary to local information captured by frame-level LLDs. Popular selections of these parameters for developing machine learning algorithms in practical applications often amount to several thousands of features. For example, in the INTER-SPEECH 2013 computational paralinguistics challenge, the recommended feature set contains 6, 373 parameters of the LLDs and statistical functionals altogether <ref type="bibr" coords="6,232.80,476.50,10.06,9.42" target="#b3">[4]</ref>. Fortunately, researchers have identified the support vector machine as one of the most effective machine learners for using these hand-crafted high-dimensional features <ref type="bibr" coords="6,217.92,511.18,10.06,9.42" target="#b4">[5]</ref>.</p><p xml:id="_rxnZbbq">To make our work comparable to the published results, we set up the first baseline model similar to the evaluation experiments conducted in <ref type="bibr" coords="6,158.40,545.74,10.06,9.42" target="#b4">[5]</ref>. We use the openSMILE toolkit <ref type="bibr" coords="6,48.00,557.26,15.88,9.42" target="#b44">[45]</ref> to extract the acoustic feature sets for INTERSPEECH Challenges from 2009 to 2013 , including Emotion Challenge (EC, 384 parameters), Paralinguistic Challenge (PC, 1582 parameters), Speaker State Challenge (SSC, 4368 parameters), Speaker Trait Challenge (STC, 5757 parameters) and Computational Paralinguistic ChallengE (ComParE, 6373 parameters). On each of these feature sets, we train a SVM for speech emotion recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" xml:id="_uxfZz23">LDNN with the log-Mels</head><p xml:id="_X4xc2jS">As suggested by previous studies <ref type="bibr" coords="6,198.84,679.78,14.57,9.42" target="#b18">[19]</ref>, <ref type="bibr" coords="6,221.04,679.78,14.48,9.42" target="#b19">[20]</ref>, <ref type="bibr" coords="6,243.12,679.78,14.57,9.42" target="#b20">[21]</ref>, <ref type="bibr" coords="6,265.20,679.78,14.57,9.42" target="#b21">[22]</ref>, explicit temporal modeling is beneficial for speech emotion recognition, in which a recurrent neural network is a better choice than a hidden Markov model for its outstanding ability to model longer-term temporal relationship. Meanwhile, in order to build a competitive as well as compatible baseline model with respect to the CLDNN-based model, we take the LDNN architecture defined in Sec. 3.3 as our second baseline model. In particular, we use the log-Mels as the input to the LDNN model as the "raw" feature set without temporal or spectral convolutional operations. We denote this model as the LDNN (log-Mels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3" xml:id="_4BGdJHP">LDNN with the MFCCs</head><p xml:id="_2mbDZTU">MFCCs are related to log-Mels via a mathematical construct: the discrete cosine transformation (DCT). Specifically, the relationship is defined as the following:</p><formula xml:id="formula_19" coords="6,324.00,183.01,240.04,29.56">MFCC[k] = M−1 m=0 log-Mel[m] cos kπ M m + 1 2 ,<label>(12)</label></formula><p xml:id="_P8vsTVE">where MFCC[k] and log-Mel[m] are the kth and the mth coefficients of MFCCs and log-Mels, respectively, and M is the number of the Mel-scaled filter banks. We can easily convert Eq. ( <ref type="formula" coords="6,455.65,253.18,7.94,9.42" target="#formula_19">12</ref>) into a convolutional operation along the spectral direction, in which scenario all feature maps are thus tensors of shape M × 1. For the kth feature map h k , its mth component</p><formula xml:id="formula_20" coords="6,375.12,303.21,188.92,23.64">h k [m] = cos kπ M m + 1 2<label>(13)</label></formula><p xml:id="_GcyK5ab">is pre-defined mathematically based on the prior knowledge of signal processing, rather than task-specifically learnt from training samples. With this development, Eq. ( <ref type="formula" coords="6,519.13,356.02,7.94,9.42" target="#formula_19">12</ref>) can be succinctly summarized as</p><formula xml:id="formula_21" coords="6,371.76,384.69,192.28,10.16">MFCC DCT-Conv (log-Mel) ,<label>(14)</label></formula><p xml:id="_XmdGhrd">where DCT-Conv represents the mathematically pre-defined spectrally only convolutional layer transforming log-Mels into MFCCs. Note that the properties of a conventional convolutional layer, such as the pooling function and the non-linear activation function are missing in this special configuration of a convolutional layer. In fact, there is no convolutional operation per se. Nevertheless, the purpose for this identification of DCT as a convolutional operation is to encapsulate this spectral modeling into the language of convolutional operations, to help us focus on the difference among various convolutional operations and mostly to contrast DCT with the S-Conv layer.</p><p xml:id="_rhPEUS9">Our third baseline model is an LDNN model which takes the MFCCs as its input. Similarly, we denote this model as the LDNN (MFCCs). By comparing the the performances of the LDNN (MFCCs) and the S-CLDNN (log-Mels), we are able to quantitatively demonstrate the advantages of the S-Conv layer over the DCT-CNN layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" xml:id="_rBHdtkE">DATABASES DESCRIPTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1" xml:id="_gc7MRt3">The Clean Set</head><p xml:id="_vZTKqfc">We use the eNTERFACE'05 emotion database <ref type="bibr" coords="6,515.76,656.62,14.57,9.42" target="#b29">[30]</ref>, which is a publicly-available multi-modal corpus of elicited emotional utterances, to evaluate the performance of our proposed models. Although the entire database contains speech, facial expression and text, in this work we only conduct experiments on the audio modality. This database includes 42 subjects from 14 various countries, in which 34 of them were male and 8 were female. Each subject was asked to listen carefully to 6 short stories, and each of them was designed to elicit a particular emotion from among the 6 archetypal emotions defined by Ekman et al. <ref type="bibr" coords="7,48.00,79.66,14.57,9.42" target="#b45">[46]</ref>. The subjects then reacted to each of the scenarios to express their emotion according to a proposed script in English. Each subject was asked to speak five utterances per emotion class for 6 emotion classes (anger, disgust, fear, happiness, sadness, and surprise). For each recorded emotional utterance, there is one corresponding global label describing the affective information conveyed by the whole utterance. The resulting corpus, however, is slightly unbalanced in the emotion class distribution because the subject 23 has only two utterances portraying happiness, so the total number of emotional utterances in this corpus is 1, 257. We call the set of these 1, 257 utterances the clean set. The average length of utterances is around 2.78 seconds, and the total duration of the clean set amounts to roughly 0.97 hour. We believe it is the moderate number of speakers and a variety of their cultural backgrounds that render it one of the most popular corpora for benchmarking speech emotion recognition models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2" xml:id="_MUhbU4K">The Noisy Set</head><p xml:id="_P2MPwG7">Deep neural networks have a well-known reputation of being data-hungry. Despite the aforementioned diversity, it is not data-efficient enough to train a deep neural network as big as a CLDNN on the clean set alone for it would potentially incur a high risk of over-fitting. Various techniques have been proposed to implicitly or explicitly regularize the training process of deep neural networks in order to prevent over-fitting as well as to improve the generalization performance, such as dropout <ref type="bibr" coords="7,176.28,413.98,14.57,9.42" target="#b43">[44]</ref>, early-stopping <ref type="bibr" coords="7,261.00,413.98,14.48,9.42" target="#b46">[47]</ref>, data augmentation <ref type="bibr" coords="7,111.96,425.50,14.48,9.42" target="#b27">[28]</ref>, transfer learning <ref type="bibr" coords="7,213.48,425.50,15.88,9.42" target="#b16">[17]</ref> and the recent group convolution approach <ref type="bibr" coords="7,176.28,437.02,14.57,9.42" target="#b47">[48]</ref>, <ref type="bibr" coords="7,198.84,437.02,14.48,9.42" target="#b48">[49]</ref>. In addition to the dropout mechanism and the early-stopping strategy, we also adopt the data augmentation approach to artificially increase the number of our data samples for the purpose of implicit regularization. To be precise, we aggressively mix samples from the clean set with samples from another publicly-available database, called the MUSAN corpus <ref type="bibr" coords="7,281.76,506.26,14.57,9.42" target="#b49">[50]</ref>, for a few randomly chosen levels of signal-to-noise ratio (SNR).</p><p xml:id="_W9wUWK3">The MUSAN corpus consists of three portions: music, speech and noise. As speech and music may inherently convey affective information, mixing samples from these two portions with clean emotional utterances would unnecessarily complicate the learning process and would possibly result in a suboptimal system due to a mixture of inconsistent emotion types. Therefore, to avoid adding confounding factors to clean emotional utterances, we only use the noise portion in the MUSAN corpus for data augmentation. The noise portion contains 929 samples of assorted noise types, including technical noises, such as Dual-tone multifrequency (DTMF) tones, dialtones, fax machine noises, and ambient sounds, such as car idling, thunder, wind, footsteps, paper rustling, rain, animal noises, and so on so forth. The total duration of the noise portion is about 6 hours. We generate artificially corrupted data based on the clean set using the following recipe. For each clean utterance, 20 noise samples are uniformly selected from the noise portion and 3 levels of the SNR are uniformly chosen from the interval [ <ref type="bibr" coords="7,353.28,55.89,31.81,9.96">−10, 15]</ref>. Mixing the clean utterance with the 60 combinations of the 20 noise samples and 3 SNR levels augments the clean set by a factor of 61. Note that randomly selecting samples from the noise portion gives an advantage over simply using a fixed subset of the noise portion. Due to the stochasticity, the probability of choosing the same set of 20 noise samples is on the order of one out of 7 × 10 40 ∼ C 929 20 , which is almost impossible. By carefully eliminating potential artificial patterns, we hope the deep neural networks could concisely capture the true underlying acoustic emotion prototypes.</p><p xml:id="_QxRGTew">We call this set of the resulting 75, 420 noisy utterances the noisy set, as opposed to the clean set defined above. The total duration of the noisy set is about 58.25 hours. In the following sections, when referring to the clean condition, we mean the experiments are conducted on the clean set; on the other hand, when referring to the noisy condition, we mean they are conducted on the union of the noisy and the clean sets. Moreover, we further randomly divide the set of subjects into training, validation and testing (TVT) partitions under the percentage constraint of 70:10:20, respectively, for experimental convenience. Partitioning the subject set, instead of the utterance set, allows us to maintain speaker independence across all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" xml:id="_fQ8fJFg">SPEECH EMOTION RECOGNITION EXPERIMENTS</head><p xml:id="_kXwvjBM">In this section, we evaluate the proposed models with the following experiments: The purposes of these experiments are multi-fold. The comparison between the baseline models and the CLDNN-based models aims to demonstrate the effectiveness of the convolutional operations in learning the affective information. Within the category of CLDNN-based models, the goal is to quantify the difference between types of convolutional operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1" xml:id="_DASmrpT">SVM with openSMILE features</head><p xml:id="_fS3v3jt">For the first set of baseline experiments, we employ two evaluation strategies. In the first one, we perform a leaveone-subject-out (LOSO) cross validation. Since we train our deep neural network models using the TVT partitions, the second strategy evaluates the performances of SVM classifiers on the TVT partitions for a fair comparison. In addition, we also take the regular pre-processing procedures, including speaker standardization for removing speaker characteristics and class weighting for slight class imbalance. We conduct the baseline experiments using SVM classifiers trained on the acoustic feature sets in the past INTERSPEECH challenges. The SVM classifiers are trained on these hand-crafted high-dimensional features using the Scikit-Learn machine learning toolkit <ref type="bibr" coords="8,208.32,102.70,15.76,9.42" target="#b50">[51]</ref> with linear, polynomial and radial basis function (RBF) kernels. All of SVM experiments are conducted under the clean condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2" xml:id="_TMkZZpt">CLDNN-based Models with the MFCCs and the log-Mels</head><p xml:id="_WeARJtS">To begin with, we extract the log-Mels and the MFCCs using the KALDI toolkit <ref type="bibr" coords="8,155.52,194.50,15.76,9.42" target="#b51">[52]</ref> with a window size of 25 ms and a window shift of 10 ms. In both cases, the number of Mel-frequency filterbanks is chosen to be 40. It has been shown <ref type="bibr" coords="8,103.68,229.18,11.08,9.42" target="#b4">[5]</ref> that due to the strong energy compaction property of the discrete cosine transformation, the lower order MFCCs are more important for affective and paralinguistic analysis, while the higher order MFCCs are more related to the phonetic content understanding. In fact, the INTERSPEECH challenges feature sets contain the first 12-14 orders of MFCCs; however, the Geneva Minimalistic Acoustic Parameter Set (GeMAPS) <ref type="bibr" coords="8,198.00,309.94,11.08,9.42" target="#b4">[5]</ref> recommends the use of only the first 4 orders of the MFCCs. In this work, we keep the conventional first 13 coefficients when computing the MFCCs. After feature extraction, we splice the raw log-Mels and raw MFCCs with a context of 10 frames in the left and 5 frames in the right. At this point, each spliced log-Mel or spliced MFCC x t lives in R 40×16 or R 13×16 , respectively. An emotional utterance is now represented as a sequence of spliced spectral vectors {x t }. We train the LDNN (log-Mels) and the LDNN (MFCCs) as depicted in Fig. <ref type="figure" coords="8,272.04,413.74,4.75,9.42" target="#fig_0">1</ref> with their corresponding inputs. TABLE 2: A summary of the ranges for parameter tuning on each type of the convolutional layers, where M denotes the spectral dimensionality and the subscripts of h and w correspond to the first and the second convolutional layers, respectively. In order to accommodate the inputs to various CLDNN models in Fig. <ref type="figure" coords="8,118.44,587.38,3.53,9.42" target="#fig_0">1</ref>, we further reshape each x t to a matrix X t with the shape of 40 × 16 or 13 × 16. We train the X-CLDNN (log-Mels) and X-CLDNN (MFCCs) on the emotional utterances {X u t } for each training utterance u and for X ∈ {S, T, ST, FST}. The ranges of the tunable parameters for the convolutional layers are summarized in Table <ref type="table" coords="8,87.12,656.62,3.59,9.42">2</ref>, where as shown we focus mostly on the first Conv layer. We exhaust all of the parameter combinations for the S-Conv, T-Conv and FST-Conv types when tuning the architectural parameters. Note that, however, the search space of the optimal parameter set for the ST-Conv is rather huge. Therefore, instead of exploring all of the combinations aimlessly, we limit our attention to the combinations of top k parameters from the S-Conv and T-Conv.</p><p xml:id="_v2GWKsJ">We use the Keras library <ref type="bibr" coords="8,444.84,44.98,15.88,9.42" target="#b52">[53]</ref> on top of the Theano <ref type="bibr" coords="8,312.00,56.62,15.88,9.42" target="#b53">[54]</ref> backend to specify the network architectures and execute the learning processes on an NVIDIA K40 Kepler GPU. The weights of all deep neural network models are learnt by minimizing the cross-entropy objective through the Adam method <ref type="bibr" coords="8,400.08,102.70,15.88,9.42" target="#b54">[55]</ref> to adjust the parameters in the stochastic optimization with an initial learning rate being 0.001. The size of mini-batch is fixed to 10 due to the capacity of the GPU memory as well as the pursuit for a better generalizing power <ref type="bibr" coords="8,426.60,148.90,14.48,9.42" target="#b55">[56]</ref>. An early-stopping strategy <ref type="bibr" coords="8,312.00,160.42,15.88,9.42" target="#b46">[47]</ref> with the patience of 3 epochs is employed to avoid over-training. We train all deep neural network models with the emotional utterances in the training partition under the noisy condition; we perform parameter tuning on the validation partition, and the most competitive model on the validation partition under the noisy (clean) condition is tested under the noisy (clean) condition, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7" xml:id="_Ega9q38">EXPERIMENTAL RESULTS</head><p xml:id="_TkPyqAq">We present our experimental results for speech emotion recognition in this section. Even though the class imbalance in the corpus is insignificant, throughout the entire section, we use the un-weighted accuracy (UA) as the performance metric to avoid being biased to the larger classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1" xml:id="_WdUXYUJ">SVM with openSMILE features</head><p xml:id="_5TWMgt5">Table <ref type="table" coords="8,339.24,364.18,4.75,9.42" target="#tab_1">3</ref> summarizes the results of using SVM classifiers to identify the emotion class of an emotional utterance with one of the 6 archetypal emotions. Based on the LOSO evaluation strategy, a SVM with the STC feature set gives the best baseline performance, while under the TVT evaluation strategy, a SVM with the ComParE feature set stands out among other feature sets. It is clear from these results that a SVM learns better from higher-dimensional feature sets such as the ComParE and the STC sets, which is also a consistent phenomenon observed in <ref type="bibr" coords="8,453.24,467.98,10.06,9.42" target="#b4">[5]</ref>. Yan et al. <ref type="bibr" coords="8,511.68,467.98,15.88,9.42" target="#b28">[29]</ref> recently published a baseline result on the eNTERFACE'05 corpus using the PC feature set. They trained a SVM classifier on the PC feature set with a speaker-dependent five-fold cross validation evaluation strategy as one of their baseline models. Their baseline work is comparable to ours, and is included in the Table <ref type="table" coords="8,403.56,537.22,4.75,9.42" target="#tab_1">3</ref> as well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2" xml:id="_GfdPw8M">LDNN with the MFCCs and the log-Mels</head><p xml:id="_GEZuATe">We present the results of the LDNN-based models in Table <ref type="table" coords="8,312.00,737.38,3.59,9.42" target="#tab_2">4</ref>. Under the noisy condition, the LDNN (MFCCs) and the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3" xml:id="_dCxVVBj">CLDNN with the MFCCs and the log-Mels</head><p xml:id="_HpmnPGb">Finally, Table <ref type="table" coords="9,114.36,449.14,4.75,9.42" target="#tab_2">4</ref> also presents the effectiveness of the CLDNN-based models for classifying emotional utterances into one of the 6 archetypal emotions. First of all, notice that with the CNN layers all CLDNN-based models improve upon their LDNN-based counterparts under both noisy and clean conditions, except that the T-CLDNN (MFCCs) results in a slightly inferior performance under the clean condition. Since MFCCs are rather sensitive to noise, it is likely that the T-Conv layers are mainly optimized to reduce prominent variations due to the artificial noise while neglecting other subtle factors of variation such as speaker or gender. Yet, the result from the FST-CLDNN (MFCCs) also suggests that the MFCCs still contain a reasonable amount of affective information which is learnable by a suitable architecture. Among the X-CLDNN (log-Mels) models, the order of performances from high to low is the FST-CLDNN (log-Mels), the ST-CLDNN (log-Mels), the T-CLDNN (log-Mels) and the S-CLDNN (log-Mels). The fact that the FST-Conv outperforms the ST-Conv is consistent with the conclusion from <ref type="bibr" coords="9,71.88,668.14,15.88,9.42" target="#b14">[15]</ref> under the clean condition. However, the margin is not as significant when there is an LDNN sub-network to help with temporal modeling. It has been reported that the S-Conv layer in a S-CLDNN (log-Mels) would degrade the performance for speech recognition under a moderately noisy condition <ref type="bibr" coords="9,120.24,725.86,14.48,9.42" target="#b26">[27]</ref>. The authors attributed this deterioration to the noise-enhanced difficulty for local filters of small sizes to make decision when learning to capture translational invariance. This attribution seems valid when we contrast the FST-Conv with the other three types. Actually, if we take a closer look, we can easily discover that there is a varying degree of enhanced difficulty to the type of convolutional operations, in which the S-Conv suffers from noise the most, followed by the T-Conv and the ST-Conv to a roughly equivalent degree and finally the FST-Conv the least. Even though we validate on the clean validating partition for selecting the model to be tested on the clean testing partition, the performances under the clean condition demonstrate a similar trend influenced by noise since we carried out the training process under the noisy condition.</p><p xml:id="_xdAjmkj">One of our goals is to benchmark the strength of the S-Conv and the discrete cosine transformation for spectral modeling. Specifically, the fair comparison should be between the LDNN (MFCCs) and the S-CLDNN (log-Mels) where the DCT-CNN and the S-Conv layers, respectively, act on the spliced log-Mels along the spectral direction, and both of them have an LDNN sub-network for further temporal modeling. Despite the negative impact on the S-Conv layer by noise, it is interesting to observe a stark performance gap between them under the noisy condition. Even under the clean condition, the S-CLDNN (log-Mels) still has a leading margin by more than 3%. Due to its task independence, DCT is not particularly designed to decorrelate the affective information from the other factors. Moreover, since the DCT-CNN layer is shallow and structurally simple, the S-Conv layer has an advantage over DCT as it is deeper and thus better at disentangling the underlying factors of variations <ref type="bibr" coords="9,447.00,403.06,14.57,9.42" target="#b56">[57]</ref>, <ref type="bibr" coords="9,467.64,403.06,14.48,9.42" target="#b57">[58]</ref>, <ref type="bibr" coords="9,488.16,403.06,14.57,9.42" target="#b58">[59]</ref>. This strength is manifested the most especially when it comes to the noise-related factors. Given that the MFCCs still carry a reasonable amount of affective information, these significant differences in performance between the S-Conv and DCT can be best explained by the inability of DCT to adequately disentangle the affective information from other irrelevant factors of variations.</p><p xml:id="_JzzyBhZ">Last but not the least, we notice that temporally convolutional operations and temporally recurrent operations are learning complimentary information. For instance, the LDNN (log-Mels) models the evolution of affective information through temporal recurrence alone, while the FST-CLDNN (log-Mels) does so by fitting itself to the dynamics via temporal convolution and then temporal recurrence, which improves upon the LDNN (log-Mels) and results in a more competitive system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4" xml:id="_wtFREgw">Finer hyper-parameter search on the spectral axis</head><p xml:id="_JhC4nBS">We have seen the negative effect of noise on the S-Conv for speech emotion recognition in Table <ref type="table" coords="9,475.44,645.10,4.75,9.42" target="#tab_2">4</ref> and speech recognition in <ref type="bibr" coords="9,355.92,656.62,14.57,9.42" target="#b26">[27]</ref>. The authors hypothesized that noise has increased the difficulty for these local filters to correctly capture translational invariance. On the other hand, the performance shown by the FST-CLDNN (log-Mels) model suggests that global information over the entire spectrum helps to learn a better representation. To gain more insight into how convolving with more spectral information contributes to affective learning, we further conduct an extensive search on the spectral axis for the optimal kernel size in the first convolutional layer of the S-CLDNN (log-Mels) model, i.e. h 1 in Table <ref type="table" coords="10,97.20,269.02,3.59,9.42">2</ref>. For the search, we fix h 2 = 3 and the pooling hyper-parameters the same as in Sec 3.1. We iterate the filter height h 1 through all possible sizes from 4 to 30 (to allow pooling and convolution in the second layer).</p><p xml:id="_FwmgZWu">Fig. <ref type="figure" coords="10,81.36,315.10,4.75,9.42" target="#fig_2">2</ref> depicts the validation UA under clean and noisy conditions with respect to different kernel size h 1 . Although highly fluctuating possibly due to the influence of noise, the accuracy is indeed improving along with a larger kernel size until it peaks at h 1 = 22 for both conditions, and increasing the kernel size larger than 22 does not result in any further improvement. Second, from the median filtered curves, the S-Conv is able to benefit more under the noisy condition from having a larger kernel size, specifically h 1 &gt; 18 in Fig. <ref type="figure" coords="10,66.24,419.02,3.53,9.42" target="#fig_2">2</ref>, which suggests a phase transition from small to large filters; however, such a pattern is not equally significant when under the clean condition as the curve is relatively flat. Third, when h 1 = 22, the respective test UA are 85.87% and 95.42% under the noisy and clean conditions. Despite the outstanding performance under the clean condition, when compared with the FST-CLDNN (log-Mels) model, these results further highlight the influence of noise on the S-Conv operation as well as the robustness of two-dimensional filters to noise <ref type="bibr" coords="10,112.80,522.82,15.76,9.42" target="#b33">[34]</ref> even though it has convolved with the optimal amount of spectral information.</p><p xml:id="_SAR643z">Overall, this extended set of experiments demonstrate one of the advantages of convolving with more spectral information, emphasizing on the ability to counter the negative effect due to noise in learning. Since S-Conv shows two characterizations with small and large filter sizes and to convolve with more spectral information is one of the characteristics of the FST-Conv in addition to being twodimensional, therefore, we will continue to refer to S-Conv as defined in Table <ref type="table" coords="10,129.96,638.26,4.75,9.42">2</ref> for consistency in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5" xml:id="_auhFBwx">Module-wise evaluations</head><p xml:id="_KkyhaAx">We have so far analyzed the proposed models from an endto-end perspective and observed interesting phenomena. Although this kind of external analysis has distilled certain working knowledge, what we are equally interested in is the internal mechanism within these models. Along these lines, a key step is to track the flow of relevant information using techniques such as information regularizer <ref type="bibr" coords="10,534.36,44.98,15.88,9.42" target="#b59">[60]</ref> or layer-wise evaluation <ref type="bibr" coords="10,407.76,56.62,14.57,9.42" target="#b24">[25]</ref>, <ref type="bibr" coords="10,429.96,56.62,14.57,9.42" target="#b60">[61]</ref>. In this work, we take the second approach due to its simplicity. To make it clear, we only evaluate the intermediate representations at the module level, where by module we mean the CNN module (two Conv layers), the BLSTM module (a BLSTM layer) and the multi-layer perceptron (MLP) module (four FC layers) that make up a CLDNN model.</p><p xml:id="_4thYQdK">To begin with, we take the trained CLDNN-based model as the feature extractors and the activated responses of each layer as the discriminative features. For each CLDNN model, we only keep the extraction from the output layer of each module. In addition, the raw spectral-temporal features are presented to serve as the lower bound. A mean pooling over the temporal direction is applied to the raw features, the output of the CNN module and the output of the BLSTM module to form an utterance representation for each of them. In order to quantify the improvement of the representations for speech emotion recognition achieved by each module, we train a SVM classifier on the utterance representation from the output of each module as well as the raw features. The experiment setting is similar to the SVM baseline, where only the clean set is used and the evaluation is based on the TVT strategy. Table <ref type="table" coords="10,351.24,483.94,4.75,9.42" target="#tab_3">5</ref> summarizes the results of the module-wise evaluation. As shown in the second column, even though the training and the testing are carried out under the clean condition, the discrete cosine transformation degrades the performance once again. Nevertheless, most of the CNN modules have helped to lift the discriminative power to around 55% regardless of the raw features except for a particularly under-performing model, the S-CLDNN (log-Mels), which based on the previous analysis is known to suffer from noise drastically. One can easily observe that each type of the Conv layers is learning a different representation and hence results in different levels of discriminative power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_TWZkKEb">Quantitative Analysis</head><p xml:id="_6Zq9Dsc">It is interesting to note that the SVMs trained on the activations of the CNN module in the {T,ST}-CLDNN (log-Mels) give a better accuracy than that based on the FST-CLDNN (log-Mels), but from a holistic perspective the FST-Conv based system is the most robust one. This may reflect one of the biggest advantages of the end-to-end training approach over the traditional layer-wise approach, which works on feature engineering and classifier training separately; i.e. a greedy layer-wise training that forces the distribution of an intermediate layer to prematurely approximate (a) Fig. <ref type="figure" coords="11,66.84,375.46,3.59,9.42">3</ref>: The visualization for the modules in the T-CLDNN (log-Mels). The first, second and third rows correspond to the affective, speaker and gender information, while the first, second and third columns denote the output of the CNN, the BLSTM and the MLP modules, respectively. In each subplot, every dot indicates an utterance, where utterances within the same class are painted with the same color and their centers of classes are marked with according labels such as hap, s07 and female. The title of each subplot is the ρ value, i.e. the quality measure of a clustering, for the distributions in the subplot.</p><p xml:id="_AbmKJ4e">the distribution of the label is likely to result in a suboptimal system.</p><p xml:id="_VazKK4y">Going deeper into the networks, we can see most of the BLSTM modules have further improved the discriminative power to the level of 88-89% except for the T-CLDNN (log-Mels). In fact, as we take a closer look at the T-CLDNN (MFCCs) and the T-CLDNN (log-Mels), we find that they both attain one of their optimal forms of affective representation at the output of the BLSTM module. Instead of implying their MLP modules have done nothing based on the constant performance, it may suggest that their MLP modules are integrating out irrelevant information while maintaining the optimal representation. Finally, in the other CLDNN models, the MLP modules further refine the representation to make the prediction an easier task. To sum up, in terms of the UA, the contributions from the CNN module, the BLSTM module and the MLP module are 27.43 ± 5.18%, 35.63 ± 3.61% and 2.85 ± 2.32%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_UcPBDZp">Visualization</head><p xml:id="_TCfHTku">In addition to the quantitative analysis of each module, we also present the visualization of the representations to gain intuition toward the internal working mechanism. In order to demonstrate the interplay between the modules and the other irrelevant information, we take into consideration two other types of information which along with the affective information are embedded in the original utterances at the same time; that is, the gender and speaker information. For every representation extracted from each module, we assign three labels to it, including the gender of the speaker (female, male), the serial number of the speaker (sN, where 1 ≤ N ≤ 42) and the emotional class (ang for anger, dis for disgust, fea for fear, hap for happiness, sad for sadness, and sur for surprise). On the clean training partition, a linear discriminant analysis (LDA) is applied to the representations and projects them onto the space spanned by the first min(N − 1, 2) components, where N stands for the number of classes. Each LDA is carried out with respect to these three labels separately and a class prior is employed to match the number of samples in each class. Moreover, we also compute the intra-cluster and inter-cluster inertia <ref type="bibr" coords="11,545.76,650.63,14.57,9.42" target="#b61">[62]</ref>, <ref type="bibr" coords="11,312.00,662.15,15.88,9.42" target="#b62">[63]</ref> on the extracted representations for each label using the following definitions</p><formula xml:id="formula_22" coords="11,347.64,693.57,216.40,27.09">ω i = 1 |C| c∈C 1 |c| s∈c p c − p s 2 ,<label>(15)</label></formula><formula xml:id="formula_23" coords="11,346.44,722.49,213.63,27.21">ω o = 1 |C| 2 − |C| c∈C c ′ ∈C−c p c ′ − p c 2 , (<label>16</label></formula><formula xml:id="formula_24" coords="11,560.07,729.94,3.97,9.42">)</formula><p xml:id="_G6NzGWa">where C is the training set, c is the subset of C containing only a specific class, p c is the arithmetic center of c, p s is the member of c and • is the Euclidean distance. Note that the vectors p are the original representations rather than the LDA projections. One may expect to have a small intra-cluster inertia and a large inter-cluster inertia when assessing the quality of a good clustering; in other words, the following ratio measures the quality of a clustering</p><formula xml:id="formula_25" coords="12,150.84,150.09,149.20,10.40">ρ = ω i /ω o ,<label>(17)</label></formula><p xml:id="_4KDQB32">where the smaller the value of ρ the better a clustering. Fig. <ref type="figure" coords="12,82.80,182.50,4.75,9.42">3</ref> shows an example of the visualization for the modules in the T-CLDNN (log-Mels). The first, second and third rows correspond to the affective, speaker and gender information, while the first, second and third columns denote the output of the CNN, the BLSTM and the MLP modules, respectively. In each subplot, every dot indicates an utterance, where utterances within the same class are painted with the same color and their centers of classes are marked with according labels such as hap, s07 and female. The title of each subplot is the ρ value for the distributions in the subplot.</p><p xml:id="_Uwt9HNR">Based on the visualization or the ρ values in the first row, it is clear that the CLDNN model is gradually learning to discriminate different affective patterns. Out of the six emotion classes, anger consistently seems to be the most prominent class across different architectures, and sadness is ranked the second. The progressively improving separability in the first row confirms our quantitative analysis results as well.</p><p xml:id="_Q8TCYAy">On the other hand, the speaker and the gender information are rather salient at the beginning of the architecture. As the forward propagation proceeds, these two types of information are getting filtered out incrementally. Note that the LDA projections on the second and the third rows are computed on the raw extracted representations with their respective labels, i.e. speaker and gender labels, and yet the deteriorating separability is apparently evident from the scatter plots and the increasing trend of ρ.</p><p xml:id="_ZrYe8r8">Based on the results of the quantitative analysis, we thought it is the BLSTM module that discards most amount of irrelevant information compared to the other modules. However, contrary to our initial expectation, it is the MLP module that excessively degrades the separability among speaker or gender classes. For instance, even at the output of the BLSTM module, the model still keeps a fair amount of gender information (Fig. <ref type="figure" coords="12,164.28,587.38,8.99,9.42">3h</ref>) but at the output of the MLP module the centers of the male and the female utterances are practically overlapping each other (Fig. <ref type="figure" coords="12,215.28,610.54,6.53,9.42">3i</ref>). Previous studies have shown that the higher-level representation of a deep neural networks could better disentangle the underlying factors of variations embedded in the input signals <ref type="bibr" coords="12,281.76,645.10,14.57,9.42" target="#b56">[57]</ref>, <ref type="bibr" coords="12,48.00,656.62,14.57,9.42" target="#b57">[58]</ref>, <ref type="bibr" coords="12,69.12,656.62,14.48,9.42" target="#b58">[59]</ref>. This visualization suggests that the CNN and the BLSTM modules are mostly playing a role to lift the input tensor into a high-dimensional manifold, a role similar to the kernel method, for disentangling the affective factor from the others, and consequently the MLP module is mainly responsible for integrating out the other factors of variations in order to optimize the corresponding objective function. In addition, this observation also vividly explains the working mechanism of multi-tasking learning that learns multiple related tasks jointly by sharing a common sub-network in the front, and of transfer learning approach that freezes the underlying layers in a pre-trained model and re-learns or fine-tunes the top few, often fully-connected, layers.</p><p xml:id="_xJHVBmF">The progression from the second column to the third column corroborates our working hypothesis in the quantitative analysis about the T-CLDNN models as well. Instead of doing nothing, the MLP module in the T-CLDNN model is refining the representations while keeping the affective information.</p><p xml:id="_bwxVryK">For the visualization of all CLDNN-based models, please refer to Supplemental Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8" xml:id="_Bnevr2f">CONCLUSION</head><p xml:id="_kvJwnKV">We report the benchmarking of four types of convolutional operations in deep convolutional recurrent neural networks for speech emotion recognition, including the spectrally only, the temporally only, the spectral-temporally, and the full-spectrum temporal convolutional operations. We found these types suffer from noise to a varying degree, in which noise negatively influences the S-Conv the most, followed by the T-Conv and the ST-Conv, and the FST-Conv the least. Under both conditions, the FST-Conv outperforms all of the other three types, and one of the state-of-the-art models under the clean condition. A set of extended experiments further shows that insufficient amount of spectral information is the major reason that leads to the negative influence of noise on the S-Conv. However, without temporal convolution, the S-Conv with larger filters is still not as robust to noise as the FST-Conv.</p><p xml:id="_tSSNGQr">Even though the S-Conv is the weakest type, the comparison between the S-CLDNN (log-Mels) and the LDNN (MFCCs) shows a significant performance gap between them, which can mostly be attributed to the difference between the S-Conv and the discrete cosine transformation. On the other hand, the FST-CLDNN (MFCCs) is still able to achieve a reasonably good accuracy. These two experiments suggest that although DCT may discard certain amount of affective information, the loss does not entirely account for the performance gap. However, we may link the mediocre performance of the LDNN (MFCCs) to the inability of DCT to adequately disentangle the affective information from other correlated irrelevant factors of variations such as speaker and gender differences and those caused by noise. Based on previous studies of deep neural networks, it is likely the shallow and structurally simple architecture of the DCT-DNN and its task-independent nature leads to such incapability of DCT.</p><p xml:id="_NMYzUfm">Meanwhile, we also found that the temporal convolution and the temporal recurrence are able to learn complementary information, and the combination of both results in a robust model such as the FST-CLDNN. Nevertheless, we only consider the architecture of a CNN module followed by a BLSTM module. It would be interesting to see if an architecture of a BLSTM module followed by a CNN module would make any difference.</p><p xml:id="_rMp7rdg">In order to understand the internal mechanism within a CLDNN model, we quantitatively analyzed the modulewise discriminative power by training a SVM on the ex-tracted activations from the output of modules. The reported accuracy can be viewed as an approximated measure of quality in the sense of readiness to exploit the affective information. From the results in Table <ref type="table" coords="13,226.08,79.66,3.53,9.42" target="#tab_3">5</ref>, we found the CNN module, the BLSTM module and the MLP module contribute a refinement of 27.43 ± 5.18%, 35.63 ± 3.61% and 2.85 ± 2.32% to the quality, respectively. This ranking is not surprising as studies from psychology <ref type="bibr" coords="13,209.76,125.86,15.76,9.42" target="#b17">[18]</ref> or computational paralinguistics <ref type="bibr" coords="13,113.40,137.38,14.57,9.42" target="#b18">[19]</ref>, <ref type="bibr" coords="13,135.12,137.38,14.57,9.42" target="#b19">[20]</ref>, <ref type="bibr" coords="13,156.84,137.38,14.57,9.42" target="#b20">[21]</ref>, <ref type="bibr" coords="13,178.56,137.38,15.88,9.42" target="#b21">[22]</ref> all point out emotion is characterized by temporally dependent dynamics. Nevertheless, our findings have shown that the CNN module is capable of significantly enhancing the separability for emotional classes compared to raw features, particularly when under a noisy condition.</p><p xml:id="_sPMdFYm">In addition, we visualize three types of information along the depth of the proposed models, including the affective, speaker and gender information. From the visualization, we observe that the model is progressively learning to discriminate different emotional patterns, in which anger and sadness are two of the most prominent emotional classes across all models. What's more interesting is that other irrelevant factors of variations are integrated out at a varying rate from one module to another. Specifically, the CNN and the BLSTM modules still keep a moderate portion of the gender and speaker information but in the end the MLP module refines the learnt representations by drastically reducing other type of variations. (g) Fig. <ref type="figure" coords="15,66.84,525.82,3.59,9.42">4</ref>: The visualization for the modules in the S-CLDNN (log-Mels). The first, second and third rows correspond to the affective, speaker and gender information, while the first, second and third columns denote the output of the CNN, the BLSTM and the MLP modules, respectively. In each subplot, every dot indicates an utterance, where utterances within the same class are painted with the same color and their centers of classes are marked with according labels such as hap, s07 and female. The title of each subplot is the ρ value, i.e. the quality measure of a clustering, for the distributions in the subplot.</p><p xml:id="_NY2wXPA">(a) Fig. <ref type="figure" coords="16,66.84,525.82,3.59,9.42">5</ref>: The visualization for the modules in the T-CLDNN (log-Mels). The first, second and third rows correspond to the affective, speaker and gender information, while the first, second and third columns denote the output of the CNN, the BLSTM and the MLP modules, respectively. In each subplot, every dot indicates an utterance, where utterances within the same class are painted with the same color and their centers of classes are marked with according labels such as hap, s07 and female. The title of each subplot is the ρ value, i.e. the quality measure of a clustering, for the distributions in the subplot.</p><p xml:id="_xaFQuVy">(a) Fig. <ref type="figure" coords="17,66.60,525.82,3.59,9.42">6</ref>: The visualization for the modules in the ST-CLDNN (log-Mels). The first, second and third rows correspond to the affective, speaker and gender information, while the first, second and third columns denote the output of the CNN, the BLSTM and the MLP modules, respectively. In each subplot, every dot indicates an utterance, where utterances within the same class are painted with the same color and their centers of classes are marked with according labels such as hap, s07 and female. The title of each subplot is the ρ value, i.e. the quality measure of a clustering, for the distributions in the subplot.</p><p xml:id="_UysEVJz">(a) Fig. <ref type="figure" coords="18,67.20,525.82,3.59,9.42">7</ref>: The visualization for the modules in the FST-CLDNN (log-Mels). The first, second and third rows correspond to the affective, speaker and gender information, while the first, second and third columns denote the output of the CNN, the BLSTM and the MLP modules, respectively. In each subplot, every dot indicates an utterance, where utterances within the same class are painted with the same color and their centers of classes are marked with according labels such as hap, s07 and female. The title of each subplot is the ρ value, i.e. the quality measure of a clustering, for the distributions in the subplot.</p><p xml:id="_Dj3hvQE">(a) Fig. <ref type="figure" coords="19,67.08,525.82,3.59,9.42">8</ref>: The visualization for the modules in the T-CLDNN (MFCCs). The first, second and third rows correspond to the affective, speaker and gender information, while the first, second and third columns denote the output of the CNN, the BLSTM and the MLP modules, respectively. In each subplot, every dot indicates an utterance, where utterances within the same class are painted with the same color and their centers of classes are marked with according labels such as hap, s07 and female. The title of each subplot is the ρ value, i.e. the quality measure of a clustering, for the distributions in the subplot.</p><p xml:id="_8Hvb3vY">(a) </p><p xml:id="_N7F99UB">Fig. <ref type="figure" coords="20,66.60,525.82,3.59,9.42">9</ref>: The visualization for the modules in the FST-CLDNN (MFCCs). The first, second and third rows correspond to the affective, speaker and gender information, while the first, second and third columns denote the output of the CNN, the BLSTM and the MLP modules, respectively. In each subplot, every dot indicates an utterance, where utterances within the same class are painted with the same color and their centers of classes are marked with according labels such as hap, s07 and female. The title of each subplot is the ρ value, i.e. the quality measure of a clustering, for the distributions in the subplot.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,48.00,422.14,516.00,9.42;4,48.00,433.69,218.41,9.77;4,272.04,433.05,21.64,9.96;4,293.64,437.05,11.76,6.40;4,306.00,433.05,32.77,10.40;4,339.24,433.05,42.70,10.40;4,382.68,433.05,181.31,10.16;4,48.00,445.30,515.98,9.42;4,48.00,456.09,515.95,10.16;4,48.00,467.61,515.95,10.16;4,48.00,479.13,516.11,10.16;4,48.00,490.65,516.00,10.16;4,48.00,503.02,515.92,9.42;4,48.00,514.54,516.03,9.42;4,48.00,526.06,337.53,9.42"><head>Fig. 1 :</head><label>1</label><figDesc xml:id="_rHPnkRT">Fig. 1: An overview of the proposed neural networks for speech emotion recognition. (a) Four types of the convolutional operation over a given two-dimensional input X t = [x t−l , • • • , x t , • • • , x t+r ] are defined, including the full-spectrum temporal convolution (FST-Conv), the spectral-temporal convolution (ST-Conv), the temporal only convolution (T-Conv), the spectral only convolution (S-Conv). The shape (height h and width w together) of a filter determines the type of a convolutional operation. Filters of shape M × w (FST-Conv) consider all (M ) frequency bands for w frames per scan. Filters of shape h × w (ST-Conv) only process local spectral-temporal information. Filters of shape 1 × w (T-Conv) and of shape h× 1 (S-conv) only observe local information along their designated direction, respectively. (b) An LDNN model, consisting of a bi-directional long short-term memory (BLSTM) gated recurrent neural layer followed by four fully connected feedforward neural layers (FC), serves to be the common sub-network architecture for each of the proposed models. Two X-Conv layers together with the LDNN sub-network forms a X-CLDNN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,326.28,394.78,87.31,9.42;7,356.28,412.30,150.67,9.42;7,355.80,423.82,89.44,9.42;7,356.76,435.34,93.40,9.42;7,326.28,452.86,115.87,9.42;7,356.28,470.38,104.20,9.42;7,355.80,481.90,114.88,9.42;7,356.76,493.54,108.64,9.42;7,355.20,505.06,109.84,9.42;7,356.40,516.58,113.92,9.42;7,357.84,528.10,117.76,9.42"><head>1 )</head><label>1</label><figDesc xml:id="_Qn23vzg">Baseline models a) SVM with openSMILE features b) LDNN (MFCCs) c) LDNN (log-Mels) 2) CLDNN-based models a) T-CLDNN (MFCCs) b) FST-CLDNN (MFCCs) c) T-CLDNN (log-Mels) d) S-CLDNN (log-Mels) e) ST-CLDNN (log-Mels) f) FST-CLDNN (log-Mels)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,48.00,167.02,251.92,9.42;10,48.00,178.66,252.04,9.42;10,48.00,189.45,252.01,10.65;10,48.00,201.70,252.01,9.42;10,48.00,213.22,54.10,9.42"><head>Fig. 2 :</head><label>2</label><figDesc xml:id="_G3yZhsd">Fig. 2: Unweighted accuracy, UA (%), of the S-CLDNN (log-Mels) model on the validation partition under noisy and clean conditions with respect to different kernel sizes h 1 in the first convolutional layer. The curves in red are median filtered UAs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,312.00,555.58,252.17,124.32"><head>TABLE 3 :</head><label>3</label><figDesc xml:id="_pbMqeAD">The SVM baseline performance (UA (%)) based on the leave-one-subject-out (LOSO) cross validation and on the training-validation-testing (TVT) partitions using the acoustic feature sets from past INTERSPEECH challenges.</figDesc><table coords="8,329.04,611.36,215.61,37.22"><row><cell></cell><cell>EC</cell><cell>PC</cell><cell>SSC</cell><cell>STC</cell><cell>ComParE</cell></row><row><cell>LOSO</cell><cell>66.61</cell><cell>73.87</cell><cell cols="2">79.19 81.18</cell><cell>80.45</cell></row><row><cell>TVT</cell><cell>70.83</cell><cell>71.66</cell><cell cols="2">77.92 80.00</cell><cell>80.83</cell></row><row><cell>Yan et al. [29]</cell><cell>-</cell><cell>74.21</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note xml:id="_Vk4eAXw">* Emotion Challenge (EC), Paralinguistic Challenge (PC), Speaker State Challenge (SSC), Speaker Trait Challenge (STC), Computational Paralinguistic ChallengE (ComParE)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,48.00,41.62,252.23,379.38"><head>TABLE 4 :</head><label>4</label><figDesc xml:id="_48W86Zu">The performances (UA (%)) of the optimal SVM model, the LDNN-based models and the CLDNN-based models. The sparse kernel reduced rank regression (SKRRR)<ref type="bibr" coords="9,48.00,76.18,15.88,9.42" target="#b28">[29]</ref> is one of the state-of-the-art models on the eNTER-FACE'05 corpus.</figDesc><table coords="9,48.00,108.92,252.12,219.81"><row><cell>Model (features)</cell><cell cols="2">noisy clean</cell></row><row><cell>SVM (ComParE)</cell><cell>-</cell><cell>80.83</cell></row><row><cell>SKRRR [29]</cell><cell>-</cell><cell>87.46</cell></row><row><cell>LDNN (MFCCs)</cell><cell>75.51</cell><cell>88.33</cell></row><row><cell>LDNN (log-Mels)</cell><cell>78.87</cell><cell>90.42</cell></row><row><cell>T-CLDNN (MFCCs)</cell><cell>83.44</cell><cell>87.92</cell></row><row><cell>FST-CLDNN (MFCCs)</cell><cell>84.45</cell><cell>92.92</cell></row><row><cell>T-CLDNN (log-Mels)</cell><cell>84.23</cell><cell>92.92</cell></row><row><cell>S-CLDNN (log-Mels)</cell><cell>82.73</cell><cell>91.67</cell></row><row><cell>ST-CLDNN (log-Mels)</cell><cell>84.26</cell><cell>93.75</cell></row><row><cell>FST-CLDNN (log-Mels)</cell><cell>86.21</cell><cell>94.58</cell></row><row><cell cols="3">LDNN (log-Mels) models are able to accurately classify</cell></row><row><cell cols="3">75.51% and 78.87% of the testing samples, respectively.</cell></row><row><cell cols="3">Under the clean condition, they give a performance of</cell></row><row><cell cols="3">88.33% and 90.42%, respectively. One can easily observe</cell></row><row><cell cols="3">that there is a gap of 3.36% and 2.09%, respectively, be-</cell></row><row><cell cols="3">tween LDNN (MFCCs) and LDNN (log-Mels) under each</cell></row><row><cell cols="3">condition. Since MFCCs are DCT transformed log-Mels, it</cell></row><row><cell cols="3">implies that DCT may have removed a certain amount of</cell></row></table><note xml:id="_zHbNssn">affective information when transforming the log-Mels into the MFCCs. The widened gap under the noisy condition also suggests MFCCs are more sensitive to noise compared to log-Mels, which renders learning from MFCCs a more challenging task. Nevertheless, both LDNN models achieve promising results comparable to that by one of the stateof-the-art models on the eNTERFACE'05 corpus, the sparse kernel reduced rank regression (SKRRR)<ref type="bibr" coords="9,221.64,411.58,14.48,9.42" target="#b28">[29]</ref>.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,312.00,348.22,252.15,113.27"><head>TABLE 5 :</head><label>5</label><figDesc xml:id="_CubH94B">The performances (UA (%)) of a SVM classifier trained on the spliced log-Mels, the spliced MFCCs and the output of each module from all CLDNN-based models under the clean condition.</figDesc><table coords="10,329.64,404.12,216.57,57.37"><row><cell>Model (features)</cell><cell>Raw</cell><cell cols="2">CNN BLSTM</cell><cell>MLP</cell></row><row><cell>T-CLDNN (MFCCs)</cell><cell>23.75</cell><cell>52.50</cell><cell>88.75</cell><cell>88.75</cell></row><row><cell>FST-CLDNN (MFCCs)</cell><cell>23.75</cell><cell>56.25</cell><cell>88.75</cell><cell>92.50</cell></row><row><cell>T-CLDNN (log-Mels)</cell><cell>27.92</cell><cell>59.17</cell><cell>93.33</cell><cell>93.33</cell></row><row><cell>S-CLDNN (log-Mels)</cell><cell>27.92</cell><cell>45.83</cell><cell>88.33</cell><cell>91.67</cell></row><row><cell>ST-CLDNN (log-Mels)</cell><cell>27.92</cell><cell>55.83</cell><cell>89.17</cell><cell>93.75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,327.48,463.64,218.68,7.69"><head>FST-CLDNN (log-Mels) 27.92 54.17 89.17 94.58</head><label></label><figDesc xml:id="_dBnjHHV"></figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_eQ86uBU"><p xml:id="_a66HgF9">• This work is supported by NSF.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_NAcYVq4">APPENDIX A VISUALIZATION OF ALL MODELS</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="13,66.12,387.45,233.72,7.94;13,66.12,396.45,233.78,7.94;13,66.12,405.45,233.89,7.94;13,66.12,414.45,17.84,7.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_6r6UB77">Behavioral signal processing: Deriving human behavioral informatics from speech and language</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Georgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_BXvsN8s">Proceedings of IEEE</title>
				<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2013-05">May 2013</date>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="1203" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,66.12,423.45,233.99,7.94;13,66.12,432.45,233.84,7.94;13,66.12,441.45,60.32,7.94" xml:id="b1">
	<monogr>
		<title level="m" type="main" xml:id="_yE6ndJJ">A scale for the measurement of the psychological magnitude pitch</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Volkmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">B</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1937</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,66.12,450.45,233.90,7.94;13,66.12,459.45,172.40,7.94" xml:id="b2">
	<monogr>
		<title level="m" type="main" xml:id="_5mHEHt6">An experimental automatic wordrecognition system</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Bridle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<idno>No. 1003</idno>
		<imprint>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
	<note type="report_type">JSRU Report</note>
</biblStruct>

<biblStruct coords="13,66.12,468.45,233.96,7.94;13,66.12,477.45,233.84,7.94;13,66.12,486.45,233.84,7.94;13,66.12,495.45,233.90,7.94;13,66.12,504.45,233.94,7.94;13,66.12,513.45,58.64,7.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_Pahfj8n">The INTERSPEECH 2013 computational paralinguistics challenge: social signals, conflict, emotion, autism</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chetouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Marchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mortillaro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Salamin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Polychroniou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Valente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_mCfWHWZ">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,66.12,522.45,233.72,7.94;13,66.12,531.45,233.72,7.94;13,66.12,540.45,233.80,7.94;13,66.12,549.45,233.89,7.94;13,66.12,558.45,89.72,7.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_VvRKtCf">The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sundberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Andre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Laukka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Truong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_DwfBzC3">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,66.12,567.45,233.57,7.94;13,66.12,576.45,233.90,7.94;13,66.12,585.45,233.80,7.94;13,66.12,594.45,91.16,7.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_6yAM5Fz">Parallel distributed processing: Explorations in the microstructure of cognition</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_BB58jXH">Learning Internal Representations by Error Propagation</title>
				<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="318" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,66.12,603.45,233.96,7.94;13,66.12,612.45,233.86,7.94;13,66.12,621.45,234.13,7.94;13,66.12,630.45,131.60,7.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_KKpaPum">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Habbard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_UtNw3Cr">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,66.12,639.45,233.92,7.94;13,66.12,648.45,120.32,7.94" xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_bGKX9zW">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_3uey8B8">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,66.12,657.45,233.66,7.94;13,66.12,666.45,233.94,7.94;13,66.12,675.47,233.84,7.86;13,66.12,684.45,71.72,7.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_M4EKXNT">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_UTNbbPn">Proceedings of the International Conference on Neural Information Processing Systems</title>
				<meeting>the International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,66.12,693.45,233.90,7.94;13,66.12,702.45,233.89,7.94;13,66.12,711.45,77.48,7.94" xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_y5yZ8Rf">Visualizing and understanding convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_AZAjv28">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,66.12,720.45,233.90,7.94;13,66.12,729.45,233.87,7.94;13,66.12,738.45,197.96,7.94" xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_QuCMnK3">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_G2TDFyX">Proceedings of the International Conference on Learning Representations</title>
				<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,46.05,233.84,7.94;13,330.12,55.05,233.86,7.94;13,330.12,64.05,233.87,7.94;13,330.12,73.05,124.76,7.94" xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_9gk6SXx">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_bCPRqec">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,82.89,233.77,7.94;13,330.12,91.89,233.89,7.94;13,330.12,100.89,159.44,7.94" xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_k2uaAaX">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_cukwn2w">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,110.73,233.78,7.94;13,330.12,119.73,233.81,7.94;13,330.12,128.73,233.48,7.94;13,330.12,137.73,17.84,7.94" xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_jkS5Byf">Learning salient features for speech emotion recognition using convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_p9QTcxZ">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2203" to="2213" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,147.57,233.85,7.94;13,330.12,156.57,233.94,7.94;13,330.12,165.57,113.36,7.94" xml:id="b14">
	<monogr>
		<title level="m" type="main" xml:id="_ZEBpSZr">Convoluted feelings convolutional and recurrent nets for detecting emotion from audio data</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Verma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="13,330.12,175.41,233.90,7.94;13,330.12,184.41,233.80,7.94;13,330.12,193.41,233.96,7.94;13,330.12,202.41,17.84,7.94" xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_mEnTjFU">Multimodal deep convolutional neural network for audio-visual emotion recognition</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_p5uVvJb">Proceedings of the International Conference on Multimedia Retrieval</title>
				<meeting>the International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,212.25,233.78,7.94;13,330.12,221.25,233.94,7.94;13,330.12,230.25,58.64,7.94" xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_rZeBjU3">Using representation learning and outof-domain data for a paralinguistic speech task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Milde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Fwfut9q">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,240.09,234.08,7.94;13,330.12,249.09,56.36,7.94" xml:id="b17">
	<monogr>
		<title level="m" type="main" xml:id="_s47B2dh">Understanding emotions</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Oatley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keltner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jenkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Blackwell</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,258.93,233.73,7.94;13,330.12,267.93,233.94,7.94;13,330.12,276.93,141.20,7.94" xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_SVtydQm">Hidden markov model-based speech emotion recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ABtCaWW">Proceedings of the International Conference on Multimedia and Expo</title>
				<meeting>the International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,286.77,233.85,7.94;13,330.12,295.77,233.74,7.94;13,330.12,304.77,233.78,7.94;13,330.12,313.77,138.68,7.94" xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_SA4RqUN">Context-sensitive multimodal emotion recognition from speech and facial expression using bidirectional lstm modeling</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Öllmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Metallinou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_MywHcxX">Proceedings of Interspeechi</title>
				<meeting>Interspeechi</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,323.61,233.84,7.94;13,330.12,332.61,233.78,7.94;13,330.12,341.61,233.99,7.94;13,330.12,350.61,122.00,7.94" xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_jTXMFHu">Context-sensitive learning for enhanced audiovisual emotion classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Metallinou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wollmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Katsamanis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PUv7Gr6">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,360.45,233.77,7.94;13,330.12,369.45,233.86,7.94;13,330.12,378.45,107.60,7.94" xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_mPDbjKS">High-level feature representation using recurrent neural network for speech emotion recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Tashev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_6Pw3xzT">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,388.17,233.78,7.94;13,330.12,397.17,233.81,7.94;13,330.12,406.17,233.89,7.94;13,330.12,415.17,153.92,7.94" xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_aMw2fjC">Convolutional, long short-term memory, fully connected deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_DNaV54z">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,425.01,233.72,7.94;13,330.12,434.01,233.86,7.94;13,330.12,443.01,107.60,7.94" xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_dV6Bu89">Learning the speech front-end with raw waveform CLDNNs</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_QZPXMV6">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,452.85,233.85,7.94;13,330.12,461.85,233.90,7.94;13,330.12,470.85,233.99,7.94;13,330.12,479.85,103.40,7.94" xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_MGSmjDc">Deep convolutional recurrent neural network with attention mechanism for robust speech emotion recognition</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_AzMJgmx">Proceedings of the IEEE International Conference on Multimedia and Expo</title>
				<meeting>the IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,489.69,233.77,7.94;13,330.12,498.69,233.94,7.94;13,330.12,507.71,233.94,7.86;13,330.12,516.69,100.28,7.94" xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_8GPmYSg">Speech emotion recognition using convolutional and recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_j9GyFDF">Proceedings of the Asia-Pacific Signal and Information Processing Association Annual Summit and Conference</title>
				<meeting>the Asia-Pacific Signal and Information Processing Association Annual Summit and Conference</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,526.53,233.86,7.94;13,330.12,535.53,233.90,7.94;13,330.12,544.53,82.64,7.94" xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_Ngn6h62">Modeling time-frequency patterns with LSTM vs. convolutional architectures for LVCSR tasks</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_WaccJhw">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,554.37,233.57,7.94;13,330.12,563.37,233.79,7.94;13,330.12,572.37,181.76,7.94" xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_98EHwvc">Convolutional neural networks with data augmentation for classifying speakers native language</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Keren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pohjalainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_rNVgWbh">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,582.21,233.93,7.94;13,330.12,591.21,233.74,7.94;13,330.12,600.21,234.08,7.94;13,330.12,609.21,121.04,7.94" xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_ft6kmAn">Sparse kernel reduced-rank regression for bimodal emotion recognition from facial expression and speech</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ub8Hccw">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1319" to="1329" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,619.05,233.92,7.94;13,330.12,628.05,233.94,7.94;13,330.12,637.05,165.92,7.94" xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_zsnMM2v">The eNTERFACE&apos;05 audio-visual emotion database</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">M</forename><surname>Macq</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_RE4wZBE">Proceedings of the International Conference on Data Engineering Workshops</title>
				<meeting>the International Conference on Data Engineering Workshops</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,646.89,233.77,7.94;13,330.12,655.89,233.90,7.94;13,330.12,664.89,82.64,7.94" xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_6eTHvCK">Speech emotion recognition using deep neural network and extreme learning machine</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Tashev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_jkcVW2s">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,674.73,233.85,7.94;13,330.12,683.73,233.80,7.94;13,330.12,692.73,233.84,7.94;13,330.12,701.73,74.72,7.94" xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_avwgmZX">Convolutional neural networks for speech recognition</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Penn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Sg9XwCD">IEEE/ACM Trans. Audio, Speech and Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1533" to="1545" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,330.12,711.45,233.90,7.94;13,330.12,720.45,233.71,7.94;13,330.12,729.45,233.97,7.94;13,330.12,738.45,233.72,7.94" xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_CHMYgGR">A deep convolutional neural network using heterogeneous pooling for trading acoustic invariance with phonetic confusion</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_D2s6NzJ">Proceedings of the IEEE International Conference on Audio, Speech and Signal Processing</title>
				<meeting>the IEEE International Conference on Audio, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,46.05,233.80,7.94;14,66.12,55.05,233.94,7.94;14,66.12,64.07,233.94,7.86;14,66.12,73.05,56.72,7.94" xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_KcWbyzg">Deep convolutional neural networks for acoustic modeling in low resource languages</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_jkNAEfg">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,82.89,233.84,7.94;14,66.12,91.89,233.84,7.94;14,66.12,100.89,233.78,7.94;14,66.12,109.89,176.72,7.94" xml:id="b34">
	<monogr>
		<title level="m" type="main" xml:id="_TnjjpbK">Cnn architectures for largescale audio classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09430</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,119.73,233.89,7.94;14,66.12,128.73,205.52,7.94" xml:id="b35">
	<monogr>
		<title level="m" type="main" xml:id="_GGarhr3">AEnet: Learning deep audio features for video analysis</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00599</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,138.45,233.77,7.94;14,66.12,147.45,233.57,7.94;14,66.12,156.45,233.83,7.94;14,66.12,165.45,74.60,7.94" xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_wjEdBhm">Learning affective features with a hybrid deep model for audio-visual emotion recognition</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_v5WYZ2x">IEEE Trans. Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,175.29,233.84,7.94;14,66.12,184.29,233.74,7.94;14,66.12,193.29,233.85,7.94;14,66.12,202.31,233.92,7.86;14,66.12,211.29,79.88,7.94" xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_7tvBh9n">Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Marchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">N</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_KpGPWXQ">Proceedings of the IEEE International Conference on Audio, Speech and Signal Processing</title>
				<meeting>the IEEE International Conference on Audio, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,221.01,233.84,7.94;14,66.12,230.01,233.68,7.94;14,66.12,239.01,234.01,7.94;14,66.12,248.01,54.08,7.94" xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_WNvSfVb">DepAudioNet: An efficient deep model for audio based depression classification</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_X9JbcnH">Proceedings of the International Workshop on Audio/Visual Emotion Challenge</title>
				<meeting>the International Workshop on Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,257.85,233.85,7.94;14,66.12,266.85,233.77,7.94;14,66.12,275.85,167.12,7.94" xml:id="b39">
	<monogr>
		<title level="m" type="main" xml:id="_rXPkFwZ">End-to-end multimodal emotion recognition using deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tzirakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08619</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,285.69,233.84,7.94;14,66.12,294.69,85.40,7.94" xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_Kx2PWJ4">Finding structure in time</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_j9Y8Zr7">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,304.41,233.86,7.94;14,66.12,313.41,233.80,7.94;14,66.12,322.41,233.94,7.94;14,66.12,331.41,53.12,7.94" xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_XrpnBev">Bidirectional lstm networks for improved phoneme classification and recognition</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_fF5j8kG">Proceedings of the International Conference on Artificial Neural Networks</title>
				<meeting>the International Conference on Artificial Neural Networks</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,341.25,233.62,7.94;14,66.12,350.25,234.02,7.94;14,66.12,359.27,233.87,7.86;14,66.12,368.25,56.72,7.94" xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_ercAwMb">Translation modeling with bidirectional recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Alkhouli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_fBvsV4N">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,377.97,233.85,7.94;14,66.12,386.97,233.81,7.94;14,66.12,395.97,233.84,7.94;14,66.12,404.97,17.84,7.94" xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_ccmuq66">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_wdvv83A">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,414.81,233.86,7.94;14,66.12,423.81,233.90,7.94;14,66.12,432.81,210.20,7.94" xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_R27mMPd">openSMILE: The Munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Öllmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_NSfDXN6">Proceedings of the ACM International Conference on Multimedia</title>
				<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,442.65,233.69,7.94;14,66.12,451.65,233.72,7.94;14,66.12,460.65,55.88,7.94" xml:id="b45">
	<analytic>
		<title level="a" type="main" xml:id="_PH4VVws">Pan-cultural elements in facial displays of emotion</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">R</forename><surname>Sorenson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UUr8MCN">Science</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="issue">3875</biblScope>
			<biblScope unit="page" from="86" to="88" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,470.37,233.79,7.94;14,66.12,479.37,233.90,7.94;14,66.12,488.37,206.36,7.94" xml:id="b46">
	<analytic>
		<title level="a" type="main" xml:id="_RkdTjJD">Early stopping is nonparametric variational inference</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_cr2AT3b">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,498.21,233.69,7.94;14,66.12,507.21,233.87,7.94;14,66.12,516.21,51.44,7.94" xml:id="b47">
	<analytic>
		<title level="a" type="main" xml:id="_wnmjhkD">Group equivariant convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_CjyY4B7">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,525.93,233.83,7.94;14,66.12,534.93,233.87,7.94;14,66.12,543.93,174.68,7.94" xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_QJN65jk">Exploiting cyclic symmetry in convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">De</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_hUUDMQj">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,553.77,233.96,7.94;14,66.12,562.77,162.68,7.94" xml:id="b49">
	<monogr>
		<title level="m" type="main" xml:id="_KWDYFsm">MUSAN: A Music, Speech, and Noise Corpus</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.08484v1</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,572.61,233.72,7.94;14,66.12,581.61,233.84,7.94;14,66.12,590.61,233.84,7.94;14,66.12,599.61,233.92,7.94;14,66.12,608.61,228.44,7.94" xml:id="b50">
	<analytic>
		<title level="a" type="main" xml:id="_eNcqmEA">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_gq6XTeH">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,618.33,233.84,7.94;14,66.12,627.33,233.90,7.94;14,66.12,636.33,234.02,7.94;14,66.12,645.33,150.80,7.94" xml:id="b51">
	<analytic>
		<title level="a" type="main" xml:id="_JJwVR5t">The KALDI speech recognition toolkit</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Stemmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_4xZtKyH">Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding</title>
				<meeting>the IEEE Workshop on Automatic Speech Recognition and Understanding</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,655.17,217.40,7.94" xml:id="b52">
	<monogr>
		<title level="m" type="main" xml:id="_7bnvCkG">Keras</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,664.89,233.80,7.94;14,66.12,673.89,233.60,7.94;14,66.12,682.89,36.08,7.94" xml:id="b53">
	<monogr>
		<title level="m" type="main" xml:id="_guDS6xv">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName coords=""><forename type="first">Theano</forename><surname>Development</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Team</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02688</idno>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,692.73,233.66,7.94;14,66.12,701.73,98.96,7.94" xml:id="b54">
	<monogr>
		<title level="m" type="main" xml:id="_WbYuWBY">Adam: A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,66.12,711.45,233.84,7.94;14,66.12,720.45,233.62,7.94;14,66.12,729.45,233.90,7.94;14,66.12,738.45,131.48,7.94" xml:id="b55">
	<analytic>
		<title level="a" type="main" xml:id="_4jG4vYR">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">T P</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Vu2mx5Z">Proceedings of the International Conference on Learning Representations</title>
				<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,330.12,46.05,233.85,7.94;14,330.12,55.05,233.94,7.94;14,330.12,64.05,153.08,7.94" xml:id="b56">
	<analytic>
		<title level="a" type="main" xml:id="_vuaYQnQ">How to construct deep recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_kAQRcCb">Proceedings of the International Conference on Learning Representations</title>
				<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,330.12,73.05,233.80,7.94;14,330.12,82.05,233.86,7.94;14,330.12,91.05,233.72,7.94" xml:id="b57">
	<analytic>
		<title level="a" type="main" xml:id="_pVcpHD3">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Cm2YQnV">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,330.12,100.05,233.84,7.94;14,330.12,109.05,233.87,7.94;14,330.12,118.07,233.84,7.86;14,330.12,127.05,63.80,7.94" xml:id="b58">
	<analytic>
		<title level="a" type="main" xml:id="_CE8Abhm">Measuring invariances in deep networks</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_zsG9uCY">Proceedings of the International Conference on Neural Information Processing Systems</title>
				<meeting>the International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="646" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,330.12,136.05,233.62,7.94;14,330.12,145.05,233.94,7.94;14,330.12,154.05,195.44,7.94" xml:id="b59">
	<analytic>
		<title level="a" type="main" xml:id="_kwFHGsT">Flow of renyi information in deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_h8xhg7W">Proceedings of the IEEE International Workshop of Machine Learning for Signal Processing</title>
				<meeting>the IEEE International Workshop of Machine Learning for Signal Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,330.12,163.05,233.77,7.94;14,330.12,172.05,185.12,7.94" xml:id="b60">
	<monogr>
		<title level="m" type="main" xml:id="_zdteStv">Understanding intermediate layers using linear classifier probes</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01644</idno>
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,330.12,181.05,233.87,7.94;14,330.12,190.05,93.56,7.94" xml:id="b61">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lebart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Morineau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fenelon</surname></persName>
		</author>
		<title level="m" xml:id="_UsabgDd">Traitement des donnees statistiques. Dunod</title>
				<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,330.12,199.05,233.92,7.94;14,330.12,208.05,233.80,7.94;14,330.12,217.05,131.00,7.94" xml:id="b62">
	<monogr>
		<title level="m" type="main" xml:id="_ugC5Ptn">A New Efficient and Unbiased Approach for Clustering Quality Evaluation</title>
		<author>
			<persName coords=""><forename type="first">J.-C</forename><surname>Lamirel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cuxac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Safi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="209" to="220" />
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
