<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_mMExJN5">SPEECH AND SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,227.08,121.90,75.70,10.29"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
						</author>
						<author>
							<persName coords="1,310.64,121.90,70.97,10.29;1,381.61,120.10,1.41,6.99"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
						</author>
						<author>
							<persName coords="1,223.82,148.60,110.42,10.37"><forename type="first">Université</forename><surname>De Montréal</surname></persName>
						</author>
						<author>
							<persName coords="1,347.94,148.60,70.22,10.37"><forename type="first">Cifar</forename><surname>Fellow</surname></persName>
						</author>
						<title level="a" type="main" xml:id="_DWkZcpD">SPEECH AND SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">621A3982FC55F4F55B69EA6FC6367EDF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-05-07T13:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_my42VPZ">ASR</term>
					<term xml:id="_uTTrdJc">CNN</term>
					<term xml:id="_KYn4hFP">SincNet</term>
					<term xml:id="_dNESPF6">Raw samples</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_wdp2Grw"><p xml:id="_bgVQKxt">Deep neural networks can learn complex and abstract representations, that are progressively obtained by combining simpler ones. A recent trend in speech and speaker recognition consists in discovering these representations starting from raw audio samples directly. Differently from standard hand-crafted features such as MFCCs or FBANK, the raw waveform can potentially help neural networks discover better and more customized representations. The highdimensional raw inputs, however, can make training significantly more challenging.</p><p xml:id="_tKvQCup">This paper summarizes our recent efforts to develop a neural architecture that efficiently processes speech from audio waveforms.</p><p xml:id="_AXXjXEh">In particular, we propose SincNet, a novel Convolutional Neural Network (CNN) that encourages the first layer to discover meaningful filters by exploiting parametrized sinc functions. In contrast to standard CNNs, which learn all the elements of each filter, only low and high cutoff frequencies of band-pass filters are directly learned from data. This inductive bias offers a very compact way to derive a customized front-end, that only depends on some parameters with a clear physical meaning.</p><p xml:id="_qx5JrE8">Our experiments, conducted on both speaker and speech recognition, show that the proposed architecture converges faster, performs better, and is more computationally efficient than standard CNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1." xml:id="_V6WNKJH">INTRODUCTION</head><p xml:id="_SqSqBHA">Deep learning has shown remarkable success in numerous speech tasks <ref type="bibr" coords="1,74.45,506.76,9.52,7.77" target="#b0">[1]</ref>, including speech <ref type="bibr" coords="1,151.70,506.76,9.71,7.77" target="#b1">[2,</ref><ref type="bibr" coords="1,162.32,506.76,7.47,7.77" target="#b2">3]</ref> and speaker recognition <ref type="bibr" coords="1,259.21,506.76,9.71,7.77" target="#b3">[4,</ref><ref type="bibr" coords="1,269.82,506.76,6.47,7.77" target="#b4">5]</ref>. This paradigm exploits the principle of compositionality to efficiently describe the world around us and employs a hierarchy of representations that are progressively learned by combining lower-level abstractions. To fully take advantage of deep learning, it would thus be natural to directly feed neural networks with the lowest possible signal representation (e.g., pixel for images or raw samples for audio), avoiding any kind of pre-computed intermediate representations.</p><p xml:id="_NQwSqqG">Nevertheless, most state-of-the-art neural networks used for speech applications still employ hand-crafted features, such as FBANK and MFCC coefficients. These engineered features are originally designed from perceptual evidence and there are no guarantees that such representations are optimal for all speech-related tasks. Standard features, for instance, smooth the speech spectrum, possibly hindering the extraction of crucial narrow-band speaker characteristics such as pitch and formants. To mitigate this drawback, some recent works have proposed to directly feed the network with spectrogram bins <ref type="bibr" coords="1,139.73,684.06,7.77,7.77" target="#b5">[6]</ref><ref type="bibr" coords="1,147.50,684.06,3.88,7.77" target="#b6">[7]</ref><ref type="bibr" coords="1,151.38,684.06,7.77,7.77" target="#b7">[8]</ref> or even with raw waveforms <ref type="bibr" coords="1,272.06,684.06,7.47,7.77" target="#b8">[9]</ref><ref type="bibr" coords="1,279.53,684.06,3.74,7.77" target="#b9">[10]</ref><ref type="bibr" coords="1,279.53,684.06,3.74,7.77" target="#b10">[11]</ref><ref type="bibr" coords="1,279.53,684.06,3.74,7.77" target="#b11">[12]</ref><ref type="bibr" coords="1,279.53,684.06,3.74,7.77" target="#b12">[13]</ref><ref type="bibr" coords="1,279.53,684.06,3.74,7.77" target="#b13">[14]</ref><ref type="bibr" coords="1,279.53,684.06,3.74,7.77" target="#b14">[15]</ref><ref type="bibr" coords="1,279.53,684.06,3.74,7.77" target="#b15">[16]</ref><ref type="bibr" coords="1,283.26,684.06,11.21,7.77" target="#b16">[17]</ref>. CNNs are the best candidate for processing raw speech samples, since weight sharing, local filters, and pooling help discover robust and invariant representations.</p><p xml:id="_Gn4YFQX">We believe that one of the most critical parts of current waveformbased CNNs is the first convolutional layer. This layer not only deals with high-dimensional inputs, but it is also more affected by vanishing gradient problems, especially when employing very deep architectures. As we will show in this paper, the filters learned by CNNs often take noisy and incongruous multi-band shapes, especially when few training samples are available. These filters certainly make some sense for the neural network, but do not appeal to human intuition, nor appear to lead to an efficient representation of the speech signal.</p><p xml:id="_w4fZCRY">To help the CNNs discover more meaningful filters, we recently proposed a novel convolutional architecture, called SincNet <ref type="bibr" coords="1,541.81,303.17,13.74,7.77" target="#b17">[18]</ref>, that adds some constraints on the filter shape. Compared to standard CNNs, where the filter-bank characteristics depend on several parameters (each element of the filter vector is directly learned), Sinc-Net convolves the waveform with a set of parametrized sinc functions that implement band-pass filters <ref type="bibr" coords="1,455.33,355.23,13.74,7.77" target="#b17">[18]</ref>. The low and high cutoff frequencies of the filters are the only parameters learned from data. This solution still offers considerable flexibility but forces the network to focus on high-level tunable parameters that have a clear physical meaning.</p><p xml:id="_wdaBpfb">In <ref type="bibr" coords="1,340.37,408.04,14.94,7.77" target="#b17">[18]</ref> we obtained promising results on both speaker identification and speaker verification tasks. In particular, SincNet turned out to outperform standard CNNs fed by both raw samples and standard features. Motivated by these encouraging results, this paper extends our recent findings to speech recognition tasks. The speech recognition experiments conducted here are based on the TIMIT and DIRHA <ref type="bibr" coords="1,346.63,470.51,14.19,7.77" target="#b18">[19,</ref><ref type="bibr" coords="1,362.53,470.51,11.95,7.77" target="#b19">20]</ref> datasets. Results confirm that the proposed Sinc-Net converges faster, achieves better performance, is more computationally efficient, and leads to more interpretable filters than a standard CNN. Interestingly, SincNet achieves a competitive performance also on the DIRHA dataset, highlighting the effectiveness of this approach even in challenging scenarios characterized by the presence of both noise and reverberation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2." xml:id="_Khadyqx">THE SINCNET ARCHITECTURE</head><p xml:id="_sufwYT6">The first layer of a standard CNN performs a set of time-domain convolutions between the input waveform and some Finite Impulse Response (FIR) filters <ref type="bibr" coords="1,396.65,603.21,13.74,7.77" target="#b20">[21]</ref>. Each convolution is defined as follows:</p><formula xml:id="formula_0" coords="1,362.44,623.72,196.55,27.06">y[n] = x[n] * h[n] = L−1 l=0 x[l] • h[n − l],<label>(1)</label></formula><p xml:id="_ETTCBCH">where </p><formula xml:id="formula_1" coords="2,136.79,87.73,161.42,8.06">y[n] = x[n] * g[n, θ].<label>(2)</label></formula><p xml:id="_rKEEj2w">A reasonable choice, inspired by standard filtering in digital signal processing, is to define g in order to employ rectangular bandpass filters. In the frequency domain, the magnitude of a generic bandpass filter can be written as the difference between two low-pass filters:</p><formula xml:id="formula_2" coords="2,99.26,155.42,198.95,19.74">G[f, f1, f2] = rect f 2f2 − rect f 2f1 ,<label>(3)</label></formula><p xml:id="_ybRzpDY">where f1 and f2 are the learned low and high cutoff frequencies, and rect(•) is the rectangular function in the magnitude frequency domain 1 . After returning to the time domain (using the inverse Fourier transform <ref type="bibr" coords="2,91.53,213.25,13.44,7.77" target="#b20">[21]</ref>), the reference function g becomes:</p><formula xml:id="formula_3" coords="2,81.06,245.20,217.14,8.06">g[n, f1, f2] = 2f2sinc(2πf2n) − 2f1sinc(2πf1n),<label>(4)</label></formula><p xml:id="_YKur2YB">where the sinc function is defined as sinc(x) = sin(x)/x. The cut-off frequencies can be initialized randomly in the range [0, fs/2], where fs represents the sampling frequency of the input signal. As an alternative, filters can be initialized with the cutoff frequencies of the mel-scale filter-bank, which has the advantage of directly allocating more filters in the lower part of the spectrum, where many crucial speech information is located. Note that the gain of each filter is not learned at this level. This parameter is managed by the subsequent layers, which can easily attribute more or less importance to each filter output.</p><p xml:id="_fDr7tuP">An ideal bandpass filter (i.e., a filter where the passband is perfectly flat and the attenuation in the stopband is infinite) requires an infinite number of elements L. Any truncation of g thus inevitably leads to an approximation of the ideal filter, characterized by ripples in the passband and limited attenuation in the stopband. A popular solution to mitigate this issue is windowing <ref type="bibr" coords="2,226.86,424.22,13.74,7.77" target="#b20">[21]</ref>. Windowing is performed by multiplying the truncated function g with a window function w, which aims to smooth out the abrupt discontinuities at the ends of g. This paper uses the popular Hamming window that is particularly suitable to achieve high frequency selectivity <ref type="bibr" coords="2,258.21,465.87,13.74,7.77" target="#b20">[21]</ref>. However, results not reported here reveals no significant performance difference when adopting other functions, such as Hann, Blackman, and Kaiser windows.</p><p xml:id="_fBH79AT">All operations involved in SincNet are fully differentiable and the cutoff frequencies of the filters can be jointly optimized with other CNN parameters using Stochastic Gradient Descent (SGD) or other gradient-based optimization routines. A standard CNN pipeline (pooling, normalization, activations, dropout) can be employed after the first sinc-based convolution. Multiple standard convolutional, fully-connected or recurrent layers <ref type="bibr" coords="2,236.54,570.72,12.17,7.77" target="#b21">[22]</ref><ref type="bibr" coords="2,248.71,570.72,4.06,7.77" target="#b22">[23]</ref><ref type="bibr" coords="2,252.76,570.72,12.17,7.77" target="#b23">[24]</ref> can then be stacked together to finally perform a classification with a softmax classifier.</p><p xml:id="_RtAP4Fb">Fig. <ref type="figure" coords="2,90.04,602.69,4.48,7.77" target="#fig_0">1</ref> shows some examples of filters learned by a standard CNN and by the proposed SincNet for a speaker identification task trained on Librispeech (the frequency response is plotted between 0 and 4 kHz). As observed in the figures, the standard CNN does not always learn filters with a well-defined frequency response. In some cases, the frequency response looks noisy (see the first CNN filter), while in others it assumes multi-band shapes (see the third CNN filter). SincNet, instead, is specifically designed to implement rectangular bandpass filters, leading to more meaningful CNN filters. 1 The phase of the rect(•) function is considered to be linear. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1." xml:id="_6n7TdaB">Model properties</head><p xml:id="_cD9rEX9">The proposed SincNet has some remarkable properties:</p><p xml:id="_tySJGsV">• Fast Convergence: SincNet forces the network to focus only on the filter parameters with a major impact on performance.</p><p xml:id="_R694qvw">Our architecture actually implements a natural inductive bias, utilizing knowledge about the filter shape (similar to feature extraction methods generally deployed on this task) while retaining flexibility to adapt to data. This prior knowledge makes learning the filter characteristics much easier, helping SincNet to converge significantly faster to a better solution.</p><p xml:id="_38JqWDy">• Few Parameters: SincNet drastically reduces the number of parameters in the first convolutional layer. For instance, if we consider a layer composed of F filters of length L, a standard CNN employs F • L parameters, against the 2F considered by SincNet. If F = 80 and L = 100, we employ 8k parameters for the CNN and only 160 for SincNet. Moreover, if we double the filter length L, a standard CNN doubles its parameter count (e.g., we go from 8k to 16k), while SincNet has an unchanged parameter count. This allows one to derive very selective filters with many taps, without adding learnable parameters.</p><p xml:id="_hZ3kgXD">• Computational Efficiency: The proposed function g is symmetric. This means that we can perform convolution in a very efficient way by only considering one side of the filter and inheriting the results for the other half. This saves 50% of the first-layer computation over a standard CNN.</p><p xml:id="_mmsn8Et">• Interpretability: The SincNet feature maps obtained in the first convolutional layer are definitely more interpretable and human-readable than other approaches. The filters, in fact, only depend on parameters with a clear physical meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3." xml:id="_tWMkK9g">RELATED WORK</head><p xml:id="_BGWdctf">Several works have recently explored the use of low-level speech representations to process audio and speech with CNNs. Most prior attempts exploit magnitude spectrogram features <ref type="bibr" coords="2,494.81,673.65,7.47,7.77" target="#b5">[6]</ref><ref type="bibr" coords="2,502.28,673.65,3.74,7.77" target="#b6">[7]</ref><ref type="bibr" coords="2,506.02,673.65,7.47,7.77" target="#b7">[8]</ref><ref type="bibr" coords="2,515.01,673.65,7.90,7.77" target="#b24">[25]</ref><ref type="bibr" coords="2,522.91,673.65,3.95,7.77" target="#b25">[26]</ref><ref type="bibr" coords="2,526.86,673.65,11.85,7.77" target="#b26">[27]</ref>. Although spectrograms retain more information than standard handcrafted features, their design still requires careful tuning of some crucial hyper-parameters, such as the duration, overlap, and typology of the frame window, as well as the number of frequency bins. For this reason, a more recent trend is to directly learn from raw waveforms, thus completely avoiding any feature extraction step. This approach has shown promising in speech <ref type="bibr" coords="3,208.92,306.69,48.52,7.77">[9-11, 28, 29]</ref>, including emotion tasks <ref type="bibr" coords="3,108.65,317.10,13.74,7.77" target="#b11">[12]</ref>, speaker recognition <ref type="bibr" coords="3,204.63,317.10,13.74,7.77" target="#b14">[15]</ref>, and spoofing detection <ref type="bibr" coords="3,71.20,327.51,13.74,7.77" target="#b13">[14]</ref>. Similar to SincNet, some previous works have proposed to add constraints on the CNN filters, for instance forcing them to work on specific bands <ref type="bibr" coords="3,139.01,348.33,14.19,7.77" target="#b24">[25,</ref><ref type="bibr" coords="3,154.36,348.33,10.65,7.77" target="#b25">26]</ref>. Differently from the proposed approach, the latter works operate on spectrogram features and still learn all the L elements of the CNN filters. An idea related to the proposed method has been recently explored in <ref type="bibr" coords="3,88.75,390.74,13.74,7.77" target="#b26">[27]</ref>, where a set of parameterized Gaussian filters are employed. This approach operates on the spectrogram domain, while SincNet directly considers the raw time domain waveform. This work extends our previous studies on the SincNet <ref type="bibr" coords="3,268.60,422.74,13.74,7.77" target="#b17">[18]</ref>. To the best of our knowledge, this paper is the first that shows the effectiveness of this architecture in a speech recognition application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4." xml:id="_Pwt3Xuv">RESULTS</head><p xml:id="_3Kqyjkt">This section first summarizes the adopted experimental setup. We then discuss our recent speaker recognition results achieved on TIMIT (462 spks) and Librispeech (2484 speakers) <ref type="bibr" coords="3,238.09,513.86,14.94,7.77" target="#b29">[30]</ref> datasets and then we extend the analysis to ASR (for more details on the speaker recognition experiments, see <ref type="bibr" coords="3,158.98,534.68,13.44,7.77" target="#b17">[18]</ref>). Beyond adopting TIMIT, speech recognition experiments also consider the DIRHA dataset for assessing SincNet in challenging noisy and reverberant conditions <ref type="bibr" coords="3,270.81,555.50,13.74,7.77" target="#b18">[19]</ref>. In the spirit of reproducible research, we release the code of SincNet on GitHub [31] 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1." xml:id="_aqrt38M">SincNet Setup</head><p xml:id="_6yZ6QMS">The waveform of each speech sentence was split into chunks of 200 ms (with 10 ms overlap), which were fed into the SincNet architecture. The first layer performs sinc-based convolutions as described in Sec. 2, using 80 filters of length L = 251 samples. The architecture then employs two standard convolutional layers, both using 60 filters of length 5. Layer normalization <ref type="bibr" coords="3,201.24,673.62,14.94,7.77" target="#b31">[32]</ref> was used for both the input samples and for all convolutional layers (including the SincNet input layer). Next, three fully-connected layers composed of 2048 2 at https://github.com/mravanelli/SincNet/.</p><p xml:id="_8eSsqnx">neurons and normalized with batch normalization <ref type="bibr" coords="3,497.46,76.13,14.19,7.77" target="#b32">[33,</ref><ref type="bibr" coords="3,513.04,76.13,11.95,7.77" target="#b33">34]</ref> were applied. All hidden layers used leaky-ReLU non-linearities. The parameters of the sinc-layer were initialized using mel-scale cutoff frequencies, while the rest of the network was initialized with the wellknown "Glorot" initialization scheme <ref type="bibr" coords="3,451.57,117.77,13.74,7.77" target="#b34">[35]</ref>. Frame-level speaker and phoneme classifications were obtained by applying a softmax classifier, providing a set of posterior probabilities over the targets. For speaker-id, a sentence-level classification was simply derived by averaging the frame predictions and voting for the speaker which maximizes the average posterior. Training used the RMSprop optimizer, with a learning rate lr = 0.001, α = 0.95, = 10 − 7, and minibatches of size 128. All the hyper-parameters of the architecture were tuned on development data. Please, refer to the GitHub repository for more details on the SincNet setup.</p><p xml:id="_RxTKPWC">We compared SincNet with several alternative systems. First, we considered a standard CNN fed by the raw waveform. This network is based on the same architecture as SincNet, but replacing the sinc-based convolution with a standard one. A comparison with popular hand-crafted features was also performed. To this end, we computed 39 MFCCs (13 static+∆+∆∆) and 40 FBANKs using the Kaldi toolkit <ref type="bibr" coords="3,364.14,284.53,13.74,7.77" target="#b35">[36]</ref>. A CNN was used for FBANK features, while a Multi-Layer Perceptron (MLP) was used for MFCCs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2." xml:id="_EmbM2Zd">Speaker Recognition</head><p xml:id="_hJ7TE7K">Fig. <ref type="figure" coords="3,332.10,336.23,4.48,7.77" target="#fig_1">2</ref> shows the cumulative frequency response of the filters learned by SincNet and CNN on a speaker-id task trained with Librispeech. The cumulative frequency response is obtained by summing up all the discovered filters and is useful to highlight which frequency bands are covered by the learned filters.</p><p xml:id="_y3nC5nw">Interestingly, there are three main peaks which clearly stand out from the SincNet plot (see the red line in the figure). The first one corresponds to the pitch region (the average pitch is 133 Hz for a male and 234 for a female). The second peak (approximately located at 500 Hz) mainly captures first formants, whose average value over the various English vowels is indeed 500 Hz. Finally, the third peak (ranging from 900 to 1400 Hz) captures some important second formants, such as the second formant of the vowel /a/, which is located on average at 1100 Hz. This filter-bank configuration indicates that SincNet has successfully adapted its characteristics to speaker identification. Conversely, the CNN does not exhibit such a meaningful pattern: its filters tend to focus on the lower part of the spectrum, but peaks tuned on first and second formants do not clearly appear.  <ref type="table" coords="3,352.28,652.83,4.48,7.77" target="#tab_1">1</ref> reports the performance achieved on speaker identification and verification tasks (Classification Error Rates -CER% for speaker-id task and Equal Error Rate -EER% for speaker verification). The table shows that SincNet outperforms other systems on both TIMIT (462 speakers) and Librispeech (2484 speakers) datasets. The speaker verification system was derived from the speaker-id neural network using the d-vector approach <ref type="bibr" coords="3,532.10,715.30,9.71,7.77" target="#b7">[8,</ref><ref type="bibr" coords="3,544.80,715.30,10.65,7.77" target="#b36">37]</ref>, which relies on the output of the last hidden layer and computes the cosine distance between the enrollment and the test speaker d-vectors (see <ref type="bibr" coords="4,106.66,460.78,14.94,7.77" target="#b17">[18]</ref> for more details). Ten utterances from impostors were randomly selected for each sentence from a genuine speaker. Note that to assess our approach on a standard open-set speaker verification task, all the impostors were taken from a speaker pool different from that used for training the speaker-id DNN. The last column of Table <ref type="table" coords="4,119.55,512.83,4.48,7.77" target="#tab_1">1</ref> extends our validation to speaker verification, reporting the EER(%) achieved with Librispeech. All DNN models show promising performance, leading to an EER lower than 1% in all cases. The table highlights that SincNet outperforms the other models, showing a relative improvement of about 11% over the standard CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3." xml:id="_PSYkgxN">Speech Recognition</head><p xml:id="_C77gw6y">To validate the effectiveness of our model, Fig. <ref type="figure" coords="4,224.85,611.19,4.48,7.77" target="#fig_2">3</ref> shows the cumulative frequency response obtained on a noisy speech recognition task. In this experiment, reported here as a showcase, we have artificially corrupted TIMIT with a significant quantity of noise in the band between 2.0 and 2.  For all the datasets, SincNet outperforms both a CNN trained on standard FBANK coefficients and CNN trained or the raw waveform. The latter result confirms the effectiveness of SincNet not only in close-talking scenarios but also in noisy conditions characterized by the presence of both noise and reverberation. As shown in Fig. <ref type="figure" coords="4,338.82,410.80,3.49,7.77" target="#fig_2">3</ref>, SincNet is able to effectively tune its filter-bank front-end to better address the characteristics of the noise. It is worth noting that the relative performance gain obtained in this challenging scenario is slightly higher than that obtained in standard close-talking conditions (6% against 4%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5." xml:id="_SPKgE5Y">CONCLUSIONS AND FUTURE WORK</head><p xml:id="_VWYntMf">This paper proposed SincNet, a neural architecture for directly processing waveform audio. Our model, inspired by the way filtering is conducted in digital signal processing, imposes constraints on the filter shapes through efficient parameterization. SincNet has been extensively evaluated on challenging speaker and speech recognition task, consistently showing some benefits on network convergence, performance, and computational efficiency. Moreover, analysis of the SincNet filters revealed that the learned filter-bank is tuned to the specific task addressed by the neural network.</p><p xml:id="_DMUwCCT">Inspired by the promising results obtained in this paper, we will explore the use of SincNet for supervised and unsupervised speaker/environmental adaptation. We believe that the proposed approach defines a general paradigm to process time-series and can be applied in numerous other fields. Our future effort will be thus devoted to extending to other tasks, such as emotion recognition, speech separation, and music processing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,315.21,216.00,243.78,8.12;2,315.21,226.76,243.78,7.77;2,315.21,237.17,197.11,7.77"><head>Fig. 1 :</head><label>1</label><figDesc xml:id="_GRnJ9Xg">Fig. 1: Filters learned by a standard CNN and by the proposed Sinc-Net. The first row reports the filters in the time domain, while the second one shows their magnitude frequency response.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,54.43,240.56,243.78,8.12;3,54.43,251.32,50.53,7.77"><head>Fig. 2 :</head><label>2</label><figDesc xml:id="_seACEzr">Fig. 2: Cumulative frequency response of SincNet and CNN filters on speaker-id.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,54.43,373.52,243.78,8.12;4,54.43,384.28,243.78,7.77;4,54.43,394.69,243.78,7.77;4,54.43,405.11,227.24,7.77"><head>Fig. 3 :</head><label>3</label><figDesc xml:id="_XFJ4MAk">Fig. 3: Cumulative frequency responses obtained on a noisy speech recognition task. As shown in the spectrogram, noise has been artificially added into the band 2.0-2.5 kHz. Both the CNN and SincNet learn to avoid the noisy band, but SincNet learns it much faster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,114.96,652.83,183.24,7.77;4,54.43,663.24,243.78,7.77;4,54.43,673.65,243.78,7.77;4,54.43,684.06,243.78,7.77;4,54.43,694.47,243.78,7.77;4,54.43,704.89,243.78,7.77;4,54.43,715.30,243.78,7.77;4,315.21,76.13,243.78,7.77;4,315.21,86.54,243.78,7.77;4,315.21,96.95,243.78,7.77;4,315.21,107.36,243.78,7.77;4,315.21,117.77,243.78,7.77;4,315.21,128.18,243.78,7.77;4,315.21,138.59,243.78,7.77;4,315.21,149.00,243.78,7.77;4,315.21,159.41,243.78,7.77;4,315.21,169.82,164.34,7.77"><head></head><label></label><figDesc xml:id="_H9Y7Fjh">5 kHz (see the spectrogram) and we have analyzed how fast a CNN and the SincNet architectures learn to avoid such a useless band. The second row of sub-figures compares the CNN and the SincNet at an early training stage (i.e., after processing only one hour of speech), while the last row shows the cumulative frequency responses at the end of training. From the figures emerges that both CNN and SincNet have correctly learned to avoid the corrupted band at end of training, as highlighted by the visible holes between 2.0 and 2.5 kHz in the cumulative frequency responses. SincNet, however, learns to avoid such a noisy band much earlier. In the second row of sub-figures, in fact, SincNet shows a valley in the cumulative spectrum even after processing only one hour of speech, while CNN will learn it only at a later training stage. At the end of training, the cumulative frequency responses of SincNet and CNN look rather different. SincNet, for instance, seems to exploit a larger bandwidth. Different from the CNN, in fact, it employs with several filters also the part of the spectrum between 0.5-2.0 kHz.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,315.21,534.62,243.78,125.99"><head>Table 1 :</head><label>1</label><figDesc xml:id="_jNPr4aD">Performance of speaker identification (first two columns) and speaker verification (last column) on the considered corpora.</figDesc><table coords="3,330.16,569.33,209.02,91.27"><row><cell></cell><cell>TIMIT</cell><cell cols="2">LibriSpeech LibriSpeech</cell></row><row><cell></cell><cell cols="2">CER(%) CER(%)</cell><cell>EER(%)</cell></row><row><cell>DNN-MFCC</cell><cell>0.99</cell><cell>2.02</cell><cell>0.88</cell></row><row><cell cols="2">CNN-FBANK 0.86</cell><cell>1.55</cell><cell>0.60</cell></row><row><cell>CNN-Raw</cell><cell>1.65</cell><cell>1.00</cell><cell>0.58</cell></row><row><cell>SincNet-Raw</cell><cell>0.85</cell><cell>0.96</cell><cell>0.51</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,315.21,189.75,243.78,166.35"><head>Table 2 :</head><label>2</label><figDesc xml:id="_G94ztGm">ASR performance obtained on the TIMIT and DIRHA.Tab. 2 reports the speech recognition performance obtained by CNN and SincNet using the TIMIT and the DIRHA dataset<ref type="bibr" coords="4,529.82,296.28,13.74,7.77" target="#b18">[19]</ref>. To ensure a more accurate comparison between the architectures, five experiments varying the initialization seeds were conducted for each model and corpus. Table2thus reports the average speech recognition performance. Standard deviations, not reported here, range between 0.15 and 0.2 for all the experiments.</figDesc><table coords="4,366.41,214.06,139.14,54.42"><row><cell></cell><cell>TIMIT</cell><cell>DIRHA</cell></row><row><cell></cell><cell cols="2">PER(%) WER(%)</cell></row><row><cell cols="2">CNN-FBANK 18.3</cell><cell>40.1</cell></row><row><cell>CNN-Raw</cell><cell>18.3</cell><cell>40.1</cell></row><row><cell>SincNet-Raw</cell><cell>18.0</cell><cell>38.2</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_8mPFNAE">Acknowledgment</head><p xml:id="_MsW3XBm">This research was enabled in part by support provided by Calcul Québec and Compute Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="5,74.35,95.66,223.86,7.93;5,74.34,106.23,62.26,7.77" xml:id="b0">
	<monogr>
		<title level="m" type="main" xml:id="_UpAbUyz">Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.35,119.86,223.86,7.93;5,74.34,130.27,130.60,7.93" xml:id="b1">
	<monogr>
		<title level="m" type="main" xml:id="_xuDZD8S">Automatic Speech Recognition -A Deep Learning Approach</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.35,144.05,223.86,7.93;5,74.34,154.61,91.16,7.77" xml:id="b2">
	<monogr>
		<title level="m" type="main" xml:id="_nHsJ8J4">Deep learning for Distant Speech Recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Unitn</publisher>
		</imprint>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct coords="5,74.35,168.39,223.86,7.77;5,74.34,178.65,223.86,7.93;5,74.34,189.06,111.83,7.93" xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_XHfq9GC">Advances in deep neural network approaches to speaker recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mclaren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ferrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_NxkC835">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4814" to="4818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.35,202.99,223.85,7.77;5,74.34,213.25,223.86,7.93;5,74.34,223.66,223.86,7.93;5,74.34,234.23,20.17,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_UnRuhpb">Deep neural network approaches to speaker and language recognition</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rGbWajm">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1671" to="1675" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.35,248.01,223.85,7.77;5,74.34,258.42,223.86,7.77;5,74.34,268.68,223.86,7.93;5,74.34,279.09,176.62,7.93" xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_HApwBtm">Text-independent speaker verification based on triplet convolutional neural network embeddings</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Koishida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_D67Y2zU">IEEE/ACM Trans. Audio, Speech and Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1633" to="1644" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.35,293.02,223.85,7.77;5,74.34,303.28,223.86,7.93;5,74.34,313.69,124.54,7.93" xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_rmcpCMn">Deep speaker embeddings for short-duration speaker verification</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_nB2MFQx">Proc. of Interspeech</title>
				<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1517" to="1521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.35,327.62,223.86,7.77;5,74.34,337.88,223.86,7.93;5,74.34,348.45,20.17,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_4fj6ZvE">Voxceleb: a largescale speaker identification dataset</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ec7duK4">Proc. of Interspech</title>
				<meeting>of Interspech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.35,362.23,223.85,7.77;5,74.34,372.64,223.86,7.77;5,74.34,382.90,126.49,7.93" xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_HqHtYRS">Analysis of CNN-based speech recognition system using raw speech as input</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Palaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Magimai-Doss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ESFYWdy">Proc. of Interspeech</title>
				<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.35,396.83,223.86,7.77;5,74.34,407.24,223.86,7.77;5,74.34,417.50,149.40,7.93" xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_ymarVEx">Learning the speech front-end with raw waveform CLDNNs</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_4eTFNu2">Proc. of Interspeech</title>
				<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.35,431.43,223.86,7.77;5,74.34,441.84,223.86,7.77;5,74.34,452.10,143.03,7.93" xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_9X5Zfdt">Acoustic modeling with deep neural networks using raw time signal for LVCSR</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tüske</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Golik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_VhUGdwd">Proc. of Interspeech</title>
				<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.35,466.04,223.85,7.77;5,74.34,476.45,223.86,7.77;5,74.34,486.86,223.86,7.77;5,74.34,497.12,223.86,7.93" xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_KhZP2uQ">Adieu features? endto-end speech emotion recognition using a deep convolutional recurrent network</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brueckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Marchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_A36qaUA">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5200" to="5204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.35,511.05,223.85,7.77;5,74.34,521.46,223.86,7.77;5,74.34,531.72,96.99,7.93" xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_5sfgCk6">Acoustic modelling from the signal domain using CNNs</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_fwUknsF">Proc. of Interspeech</title>
				<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.35,545.65,223.85,7.77;5,74.34,555.91,223.86,7.93;5,74.34,566.47,20.17,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_5GTXT73">End-to-end spoofing detection with raw waveform CLDNNS</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Dinkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_CfQtsmQ">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.35,580.26,223.86,7.77;5,74.34,590.67,223.86,7.77;5,74.34,600.93,90.61,7.93;5,200.57,601.08,20.17,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_MhabfBU">Towards directly modeling raw speech signal for speaker verification using CNNs</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muckenhirn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Magimai-Doss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_dE7FQQf">Proc</title>
				<meeting>null</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.35,614.86,223.86,7.77;5,74.34,625.27,223.86,7.77;5,74.34,635.68,223.86,7.77;5,74.34,645.94,84.27,7.93" xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_AmXvd3z">A complete end-to-end speaker verification system using deep neural networks: From raw signals to verification result</title>
		<author>
			<persName coords=""><forename type="first">J.-W</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-S</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-J</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_gNhSUR6">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.35,659.87,223.85,7.77;5,74.34,670.28,223.86,7.77;5,74.34,680.69,223.86,7.77;5,74.34,690.95,96.99,7.93" xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_FkRxjyu">Avoiding Speaker Overfitting in End-to-End DNNs using Raw Waveform for Text-Independent Speaker Verification</title>
		<author>
			<persName coords=""><forename type="first">J.-W</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-S</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-J</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_FGkh3tm">Proc. of Interspeech</title>
				<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.35,704.89,223.85,7.77;5,74.34,715.14,172.96,7.93" xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_4mQRzMX">Speaker Recognition from raw waveform with SincNet</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_NCp2gFy">Proc. of SLT</title>
				<meeting>of SLT</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.14,76.13,223.85,7.77;5,335.13,86.54,223.86,7.77;5,335.13,96.95,223.86,7.77;5,335.13,107.21,163.47,7.93" xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_6ZJVqwq">The DIRHA-ENGLISH corpus and related tasks for distant-speech recognition in domestic environments</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cristoforetti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gretter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pellin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sosi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_zQkmw8a">Proc. of ASRU 2015</title>
				<meeting>of ASRU 2015</meeting>
		<imprint>
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.14,121.08,223.85,7.77;5,335.13,131.49,223.86,7.77;5,335.13,141.75,96.99,7.93" xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_4WyNHz3">Realistic multimicrophone data simulation for distant speech recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Svaizer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_TCEbRUk">Proc. of Interspeech</title>
				<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.14,155.48,223.86,7.93;5,335.13,165.89,186.23,7.93" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W</forename><surname>Schafer</surname></persName>
		</author>
		<title level="m" xml:id="_8QNEyPq">Theory and Applications of Digital Speech Processing</title>
				<imprint>
			<publisher>Prentice Hall, NJ</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.14,179.77,223.85,7.77;5,335.13,190.18,223.86,7.77;5,335.13,200.44,106.21,7.93" xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_jw8jM9k">Improving speech recognition by revising gated recurrent units</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_UtUxM6e">Proc. of Interspeech</title>
				<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.14,214.31,223.86,7.77;5,335.13,224.57,223.86,7.93;5,335.13,234.98,223.86,7.93;5,335.13,245.55,117.56,7.77" xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_tSr28pr">Light gated recurrent units for speech recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YUdhDFm">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="92" to="102" />
			<date type="published" when="2018-04">April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.14,259.27,223.85,7.77;5,335.13,269.53,219.86,7.93" xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_SguWjXh">Twin regularization for online speech recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_QCZv8Vv">Proc. of Interspeech</title>
				<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.14,283.41,223.85,7.77;5,335.13,293.82,223.86,7.77;5,335.13,304.08,183.32,7.93" xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_AuN3FXm">Learning filter banks within a deep neural network framework</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_uM424Dq">Proc. of ASRU</title>
				<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="297" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.14,317.96,223.86,7.77;5,335.13,328.21,223.86,7.93;5,335.13,338.62,132.07,7.93" xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_eDJhkHe">DNN Filter Bank Cepstral Coefficients for Spoofing Detection</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MbEFAQD">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="4779" to="4787" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.13,352.50,223.86,7.77;5,335.13,362.91,223.86,7.77;5,335.13,373.17,181.52,7.93" xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_TmQU8gF">A deep neural network integrated with filterbank learning for speech recognition</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_QcjwPAA">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5480" to="5484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.13,387.05,223.86,7.77;5,335.13,397.31,223.86,7.93;5,335.13,407.72,53.55,7.93" xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_Yeuu8aC">Speech acoustic modeling from raw multichannel waveforms</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_644Eb5E">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.13,421.60,223.86,7.77;5,335.13,432.01,223.86,7.77;5,335.13,442.42,223.86,7.77;5,335.13,452.68,164.54,7.93" xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_MdhkDtU">Speaker localization and microphone spacing invariant acoustic modeling from raw multichannel waveforms</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_qn9pMdH">Proc. of ASRU</title>
				<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.14,466.55,223.86,7.77;5,335.13,476.97,223.86,7.77;5,335.13,487.22,182.02,7.93" xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_A5X2e5n">Librispeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_cT9K4De">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.14,501.10,223.86,7.77;5,335.13,511.36,208.10,7.93" xml:id="b30">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07453</idno>
		<title level="m" xml:id="_GDkMXUT">The PyTorch-Kaldi Speech Recognition Toolkit</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.14,525.24,223.86,7.77;5,335.13,535.50,123.36,7.93" xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_fq3MVmr">Layer normalization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hpWBaUx">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.14,549.37,223.86,7.77;5,335.13,559.78,223.86,7.77;5,335.13,570.04,126.12,7.93" xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_9yD65uR">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_af7sUGD">Proc. of ICML</title>
				<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.14,583.92,223.86,7.77;5,335.13,594.33,223.86,7.77;5,335.13,604.59,108.63,7.93" xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_djEkJEY">Batchnormalized joint training for dnn-based distant speech recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_x7NrWd6">Proc. of SLT</title>
				<meeting>of SLT</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.14,618.47,223.86,7.77;5,335.13,628.73,223.86,7.93;5,335.13,639.29,69.49,7.77" xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_EpCu7bV">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_WBQBPYP">Proc. of AISTATS</title>
				<meeting>of AISTATS</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.14,653.02,223.85,7.77;5,335.13,663.27,76.94,7.93" xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_RgDQMtX">The Kaldi Speech Recognition Toolkit</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_pbfn2sm">Proc. of ASRU</title>
				<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.14,677.15,223.86,7.77;5,335.13,687.56,223.86,7.77;5,335.13,697.82,223.86,7.93;5,335.13,708.23,111.83,7.93" xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_yjtrZuG">Deep neural networks for small footprint text-dependent speaker verification</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Variani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">L</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalez-Dominguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_9wXmMTa">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4052" to="4056" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
