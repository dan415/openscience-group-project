<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_d6RGVdN">Speech Emotion Recognition from Spectrograms with Deep Convolutional Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,146.88,138.72,94.75,12.15"><forename type="first">Abdul</forename><forename type="middle">Malik</forename><surname>Badshah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Electronics and Information Engineering</orgName>
								<orgName type="institution">Sejong University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.02,138.72,55.78,12.15"><forename type="first">Jamil</forename><surname>Ahmad</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Electronics and Information Engineering</orgName>
								<orgName type="institution">Sejong University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.94,138.72,53.28,12.15"><forename type="first">Nasir</forename><surname>Rahim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Electronics and Information Engineering</orgName>
								<orgName type="institution">Sejong University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,392.88,138.72,75.70,12.15"><forename type="first">Sung</forename><forename type="middle">Wook</forename><surname>Baik</surname></persName>
							<email>sbaik@sejong.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">College of Electronics and Information Engineering</orgName>
								<orgName type="institution">Sejong University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_XfFMENV">Speech Emotion Recognition from Spectrograms with Deep Convolutional Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E2C30D3D1BFCFA01A3A78B0D222D2EBC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-05-07T13:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_5CZvxKe">speech</term>
					<term xml:id="_9pq2ex3">emotions</term>
					<term xml:id="_2jHSMZn">convolutional neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_MJbMa9y"><p xml:id="_qrtNcmx">This paper presents a method for speech emotion recognition using spectrograms and deep convolutional neural network (CNN). Spectrograms generated from the speech signals are input to the deep CNN. The proposed model consisting of three convolutional layers and three fully connected layers extract discriminative features from spectrogram images and outputs predictions for the seven emotions. In this study, we trained the proposed model on spectrograms obtained from Berlin emotions dataset. Furthermore, we also investigated the effectiveness of transfer learning for emotions recognition using a pre-trained AlexNet model. Preliminary results indicate that the proposed approach based on freshly trained model is better than the fine-tuned model, and is capable of predicting emotions accurately and efficiently.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_DV7srMq">I. INTRODUCTION</head><p xml:id="_85W8ZEp">Speech signal is the most natural way to communicate among human beings. Researchers are constantly working to apply this mode in the domain of human-machine interaction. However, it require machines to interpret human spoken phrases intelligently and understand it semantically. Despite the great progress made in speech recognition, this process still requires lot of efforts to it make it natural interaction between man and machine. One significant challenge in realizing this goal is the inability of machines to understand the emotional state hidden behind spoken words <ref type="bibr" coords="1,251.82,498.49,10.63,11.09" target="#b0">[1]</ref>. In this context, speech emotion recognition (SER) refers to recognition of the emotional state of a speaker by analyzing his/her speech <ref type="bibr" coords="1,109.12,532.99,10.62,11.09" target="#b1">[2]</ref>. It is believed that, SER can be used to extract useful semantics from speech, as well as improve the performance of speech recognition system <ref type="bibr" coords="1,217.24,555.97,10.63,11.09" target="#b2">[3]</ref>.</p><p xml:id="_DXnNns3">SER plays an effective role in areas of natural manmachine interactions such as web movies recommendation and computer tutorial applications where the system response depends upon the emotions of the user <ref type="bibr" coords="1,204.57,607.93,10.62,11.09" target="#b0">[1]</ref>. It can also be used for in-car board system where information of the mental condition of the driver may be provided to the system to initiate safety procedures, if required <ref type="bibr" coords="1,222.45,642.37,10.64,11.09" target="#b3">[4]</ref>. Furthermore, medical use of speech emotion recognition includes diagnostic tool for therapists <ref type="bibr" coords="1,120.72,665.35,10.63,11.09" target="#b4">[5]</ref>. In aircraft cockpits, speech recognition system that were trained using stressed speech can exhibit better results compared to those trained by normal speech <ref type="bibr" coords="1,552.67,197.23,10.57,11.09" target="#b5">[6]</ref>. Further applications include determination of situational seriousness in emergency call centers based of human emotional analysis from speech data and other mobile communication areas <ref type="bibr" coords="1,404.36,243.19,10.63,11.09" target="#b6">[7]</ref>. The main theme of SER systems is to detect particular characteristics of speaker's voice in varying emotional conditions . Typical SER systems work by extracting features from speech, followed by a classification procedure to predict emotions. There are several challenges being faced by researchers which include selection of appropriate speech features, robustness to tone changes, speaking styles, speaking rates, and the way emotions are expressed in different cultures and environments. One of the major research issues is the extraction of discriminative, robust, and affect-salient features from speech. Features used for SER systems are generally categorized into acoustic, linguistic, context information, and hybrid features combining acoustic and other features <ref type="bibr" coords="1,552.58,398.53,10.64,11.09" target="#b0">[1]</ref>. Reliable recognition of emotion heavily depend on the extracted features because they represent the acoustic contents of speech. Besides hand-engineered features including prosodic features (pitch, energy, zero-crossings) <ref type="bibr" coords="1,539.22,444.49,7.86,11.09" target="#b7">[8]</ref><ref type="bibr" coords="1,547.08,444.49,3.93,11.09" target="#b8">[9]</ref><ref type="bibr" coords="1,551.00,444.49,11.78,11.09" target="#b9">[10]</ref>, spectral features (linear predictor coefficients (LPC), linear predictor cepstral coefficients (LPCC), mel-frequency cepstral coefficients (MFCC) <ref type="bibr" coords="1,406.66,478.99,15.84,11.09" target="#b10">[11,</ref><ref type="bibr" coords="1,427.72,478.99,11.83,11.09" target="#b11">12]</ref>, and non-linear features like Teager-energy-operator (TEO) <ref type="bibr" coords="1,455.12,490.45,15.36,11.09" target="#b12">[13]</ref>, automated features extraction methods related to deep learning has also been used. These methods have shown promising results in the fields of speech recognition, emotion recognition, and other speech analysis applications <ref type="bibr" coords="1,434.19,536.41,12.16,11.09" target="#b13">[14]</ref><ref type="bibr" coords="1,446.35,536.41,4.05,11.09" target="#b14">[15]</ref><ref type="bibr" coords="1,450.41,536.41,12.16,11.09" target="#b15">[16]</ref>. Inspired by the success of these methods, we propose a convolutional neural network (CNN) architecture to extract salient discriminative features from spectrograms for performing SER. Different aspects of the features extraction, content representation, and classification are analyzed and discussed in the context of SER applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_rURRbpu">II. PROPOSED METHODOLOGY</head><p xml:id="_TP3J4uU">The proposed framework attempts to utilize feature learning schemes for spectrograms generated from speech. This mode of feature learning paradigm bypasses the traditional feature engineering pipeline. Robust and discriminative features are learnt from spectrograms automatically forming the basis for SER. The main components of the proposed scheme are illustrated in the subsequent sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_rHYY45D">A. Spectrograms</head><p xml:id="_W5UvChn">A spectrogram is the visual representation of a signal strength over time at different frequencies present in certain waveform. It is represented by a two-dimensional graph in which time is shown along the horizontal axis, frequency along the vertical axis, and the amplitude of the frequency components at a particular time is indicated by the intensity or color of that point in the graph. Low amplitudes are represented by dark blue colors and stronger (or louder) amplitudes are represented by brighter colors up through red. It is computed from the speech signal by applying Fast Fourier transform (FFT) to speech signal, which form time-frequency representation. In order to discover the frequencies at a moment in the signal, it is divided into small chunks and FFT is applied to the speech waveform for each chunk. Sample spectrograms are shown in Fig. <ref type="figure" coords="2,63.73,287.53,3.76,11.09" target="#fig_0">1</ref>.</p><p xml:id="_YTdSH3W">Spectrograms have been used in a variety of speech analysis tasks including sound event classification <ref type="bibr" coords="2,277.78,316.51,15.31,11.09" target="#b16">[17]</ref>, speaker recognition <ref type="bibr" coords="2,130.56,328.03,15.35,11.09" target="#b17">[18]</ref>, SER <ref type="bibr" coords="2,177.14,328.03,15.35,11.09" target="#b13">[14]</ref>, and speech recognition <ref type="bibr" coords="2,45.36,339.49,15.32,11.09" target="#b15">[16]</ref>. Their suitability for acoustic content representation has been exhibited in these prior works. In this work, the aim is to use spectrograms to represent audio from telephone speech at emergency centers. Phone calls in emergency situations are often made in unconstrained environmental conditions. Consequently, significant background noise, low transmission quality, and Lombard effect, makes SER more challenging. In this preliminary part of the work, we aim to perform SER based on existing emotional speech datasets and later enhance the model to perform SER using telephone speech data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_TpNvC73">B. Convolutional Neural Networks</head><p xml:id="_n95Gj6K">Convolutional neural network is a hierarchical neural network which consist of a variety of layers in sequence. A typical model usually consist of several convolutional layers where the visual contents (i.e. spectrograms) are represented as a set of feature maps obtained after convolving the input with a variety of filters which are learned during the training phase. Pooling layers may be introduced after convolutional layers to accumulate maximum activation features from convolutional feature maps. As a result of pooling, spatial resolution of these maps is reduced. Furthermore, CNNs may also contain fully connected (FC) layers where each neuron of the input layer is connected with every neuron in the layer. A sequence of convolutional, pooling, and FC layers form the features extraction pipeline which models the input data in abstract form. Finally, a Softmax layer performs the final classification task based on this representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_h3PpyhU">C. Model Architecture</head><p xml:id="_C6MWKRE">The proposed CNN model, shown in Fig. <ref type="figure" coords="2,497.20,184.15,5.00,11.09">2</ref> consist of three convolutional layers, three fully connected layers and a Softmax layer. The input of the network is a 256 x 256 spectrogram generated from emotional speech signals. The initial convolutional layers extract features from these spectrograms using convolution operations. Layer C1 has 120 (11 x 11) kernels which are applied at a stride setting of 4 pixels. It is followed by rectified linear units (ReLU) and a max pooling layer of size 3 x 3 with stride 2. ReLU act as activation functions instead of the typical sigmoid functions which improves efficiency of the training process. Layer C2 has 256 kernels of size 5 x 5 and they are applied on the input with a stride 1. Similarly, C3 has 384 kernels of size 3 x 3. Each of these conv. layers are followed by ReLU units. Layer C3 is followed by three FC layers having 2048, 2048, and 7 neurons, respectively. In order to avoid overfitting, the first two FC layers are followed by dropout layers having a dropout ratio of 50%. Compared to the natural images, it is relatively difficult to extract discriminative features from spectrograms for robust emotions recognition. Fig. <ref type="figure" coords="2,333.04,570.07,4.17,11.09">2</ref> Proposed CNN architecture for SER using spectrograms</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_eZR7Zfy">D. Model Training</head><p xml:id="_PhmYP6q">The proposed CNN model was specified in Caffe <ref type="bibr" coords="2,547.49,608.11,15.34,11.09" target="#b18">[19]</ref>, using NVidia DIGITS 3.0 as a frontend <ref type="bibr" coords="2,477.05,619.57,15.32,11.09" target="#b19">[20]</ref>. The spectrogram images were generated in MATLAB from the Berlin Speech Emotion dataset and resized to 256 x 256. More than 3000 spectrograms were generated from all the audio files in the dataset. Seventy five percent of this data was used for training and the rest was used in the testing phase. For each emotion, we had collected about 500 images in the dataset.</p><p xml:id="_AHtgFEn">The training process was run for 30 epochs with a batch size set to 100. Initial learning rate was set to 0.01 with a decay of 0.1 after each 10 epochs. The training was performed on a single NVidia GeForce GTX Titan X GPU with 12 GB onboard memory. The training took around 40 minutes and the best accuracy was achieved after 28 epochs. On the training set, a loss of 0.71 was achieved, whereas 0.95 loss was recorded on the test set. An accuracy of 61.75 % was achieved per spectrogram. It is important to notice here that the overall accuracy is very low. However, each audio file consist of multiple spectrograms, and if our method can classify more than 30% of the spectrograms correctly, then the chances of predicting emotions in the entire file are sufficiently high. An aggregation mechanism governed is employed to combine the individual predictions into an overall prediction result for the entire audio call.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_fkGKHRW">E. Emotion Prediction using CNN</head><p xml:id="_4Tkryh5">The trained model is used to obtain predictions for each spectrogram generated for the incoming speech signals. The Softmax layer of the model outputs predictions for the seven different emotions in the form of probabilities which are used in the mean prediction based reasoning process to obtain overall prediction scores for these emotions. Prediction reported by the model for each individual spectrogram act as evidence to update the belief values for all emotions. In the current scenario with seven classes, if roughly 25-30% predictions for the audio files are made correctly, then there is a good chance that the particular emotion may be predicted accurately based on the collected evidence from multiple spectrograms. The predictions from CNN model are accumulated to determine the probabilities for individual emotions by computing mean predictions from the gathered evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_9UNMcTs">III. EXPERIMENTAL RESULTS &amp; ANALYSIS</head><p xml:id="_2tnwV5h">This section provides details about the experimental setup, conducted experiments, and analysis of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Fe83bC4">A. Dataset</head><p xml:id="_t6HCTEZ">Berlin dataset <ref type="bibr" coords="3,117.64,532.87,16.68,11.09" target="#b20">[21]</ref> was used to assess performance of the proposed SER system. The dataset consist of expert annotated speech data from four different users. Every audio file is annotated using one of the seven different emotions including neutral, fear, anger, happy, sadness, disgust, and boredom. Seventy five percent of this data was used for training and the remaining was used for testing. Five folds cross validation was performed to obtain the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_D7Mqj69">B. Experiments</head><p xml:id="_PAGMzue">Several experiments were carried out to assess the suitability of proposed method for emotion recognition. Two different set of experiments were performed. In the first experiment, the training dataset was used to train a fresh CNN model and its prediction performance is assessed. In the second experiment, transfer learning approach is explored to determine its suitability for the task of emotions prediction using spectrograms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1." xml:id="_cShrTp5">Fresh Trained CNN based SER</head><p xml:id="_Fthv9MC">The model presented in Fig. <ref type="figure" coords="3,430.17,115.69,5.00,11.09">2</ref> was trained on the spectrograms generated from the Berlin emotions dataset. Results of the trained model on the test set are shown in Table <ref type="table" coords="3,519.24,138.67,3.74,11.09" target="#tab_0">1</ref>. It can be seen that the model was able to perform predictions with accuracy above 50% for emotions like anger, boredom, disgust, and sad. However, prediction performance for fear, happy, and neutral emotions were lower than 50%. Fear emotions are often confused with anger, disgust, and happy. Though the confusion rate (around 19% in each case) is lower than the correct predictions (25.33%). This makes it more probable that the fear emotion may be correctly predicted if sufficient evidence can be gathered from the speech stream. On the other hand neutral emotions are heavily confused with boredom. Similarly, happy emotion is confused with anger which adversely affect it prediction performance. In case of these three emotions, further work needs to be done to decrease their confusion with the other emotions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2." xml:id="_4gSf83b">Fine-tuned CNN based SER</head><p xml:id="_Fhc8H5k">In the recent years, transfer learning has been used to utilize the learning from a relatively different domain and apply it to solve a particular problem more effectively. In such a case, the learned weights from the pre-trained model are used to initialize the model before tuning the parameters according to the new dataset. Learning rate is usually kept very small (one tenth of the normal learning rate) so that the weights are adjusted slightly. Though in several cases, transfer learning may help achieve better performance than the fresh trained model, in our case, it wasn't very helpful because of the huge difference in the types of datasets used to train or fine-tune the model. The confusion matrix obtained by fine-tuned AlexNet model <ref type="bibr" coords="3,345.54,594.01,16.69,11.09" target="#b21">[22]</ref> (trained on ImageNet data <ref type="bibr" coords="3,487.26,594.01,15.99,11.09" target="#b22">[23]</ref>) for emotions recognition is provided in Table <ref type="table" coords="3,459.49,605.47,3.77,11.09" target="#tab_1">2</ref>. This fine-tuned model improves the prediction performance in case of anger, neutral, and sad emotions. However, prediction performance in case of the other four emotions got decreased. In the current scenario, we opted to use the freshly trained CNN due to lesser complexity and better performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_PFr9veW">C. Prediction Performance</head><p xml:id="_WMAwd9Y">Prediction results (i.e. probabilities of true class) for spectrograms generated for various emotional audio files from the test set are provided in Fig. <ref type="figure" coords="4,188.64,205.75,3.77,11.09" target="#fig_3">3</ref>. Results reveal that the proposed framework is capable of correctly predicting most of the emotions by generating high confidence outputs more than 50% of the time. In case of fear and happy emotions, the performance is acceptable but not very good because some of the spectrograms are confused other emotions.</p><p xml:id="_DyPQ6wb">The mean predictions for anger, disgust, and sad emotions are above 0.68, whereas for the boredom, fear, happy, and neutral are 0.48, 0.33, 0.35, and 0.44 respectively. In all the cases mean prediction value for all the emotions was greater than the other emotions. Overall, the proposed method helped us achieve an accuracy of 84.3% for all the speakers on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_k5vSh59">IV. CONCLUSIONS</head><p xml:id="_CE7WAZC">In this paper, we attempt to solve the problem of SER using feature learning scheme based on deep convolutional neural networks. Speech signal is represented as spectrograms which act as the input to deep CNNs. The CNN model consisting of three convolutional and three fully connected layers extract features from these spectrograms and output predictions for the seven emotion classes. In this regard, two different set of experiments were performed. In the first experiment, we trained a fresh CNN model based on the spectrograms generated for the Berlin emotions dataset. Satisfactory results were achieved using this model for most of the emotions except fear. In the second experiment, we fine-tuned a pre-trained AlexNet model to determine the suitability of transfer learning in solving the problem of SER. However, the results were not satisfactory. Further work is needed to improve the proposed framework so that all emotions are recognized effectively in a robust manner. We plan to use more data with relatively complex models to improve the SER performance even further.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,106.62,591.01,141.28,11.09"><head>Fig. 1</head><label>1</label><figDesc xml:id="_8B7ZGWP">Fig. 1 Sample speech spectrograms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,128.04,625.87,86.23,11.09;4,45.36,643.39,251.69,11.09;4,45.36,654.85,251.60,11.09;4,45.36,666.37,251.63,11.09;4,45.36,677.83,93.90,11.09"><head>ACKNOWLEDGEMENT</head><label></label><figDesc xml:id="_xVSYR9T">This work was supported by the ICT R&amp;D program of MSIP/IITP. (No. R0126-15-1119, Development of a solution for situation-awareness based on the analysis of speech and environmental sounds).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,315.42,436.57,250.80,11.09;4,321.30,448.03,239.10,11.09;4,425.82,459.55,30.04,11.09"><head>Fig. 3 .</head><label>3</label><figDesc xml:id="_ReNtbh8">Fig. 3. Prediction performance on test audio files for (a) anger, (b) boredom, (c) disgust, (d) fear, (e) happy, (f) neutral, and (g) sad.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,315.00,328.51,251.69,100.29"><head>Table 1 .</head><label>1</label><figDesc xml:id="_b93UnsW">Confusion matrix for emotion prediction using fresh trained CNN</figDesc><table coords="3,321.54,352.11,236.24,76.69"><row><cell></cell><cell cols="7">Anger Boredom Disgust Fear Happy Neutral Sad</cell></row><row><cell>Anger</cell><cell>84.21</cell><cell>0.00</cell><cell>1.75</cell><cell cols="2">3.51 10.53</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell cols="2">Boredom 0.00</cell><cell>53.91</cell><cell>7.83</cell><cell>4.35</cell><cell>2.61</cell><cell cols="2">14.78 16.52</cell></row><row><cell>Disgust</cell><cell>3.80</cell><cell>7.59</cell><cell cols="2">68.35 7.59</cell><cell>7.59</cell><cell>3.80</cell><cell>1.27</cell></row><row><cell>Fear</cell><cell>18.67</cell><cell>14.67</cell><cell cols="3">16.00 25.33 18.67</cell><cell>2.67</cell><cell>4.00</cell></row><row><cell cols="2">Happy 46.59</cell><cell>0.00</cell><cell>4.55</cell><cell cols="2">10.23 36.36</cell><cell>1.14</cell><cell>1.14</cell></row><row><cell cols="2">Neutral 0.00</cell><cell>41.05</cell><cell>1.05</cell><cell>3.16</cell><cell>2.11</cell><cell cols="2">42.11 10.53</cell></row><row><cell>Sad</cell><cell>0.00</cell><cell>13.85</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>3.08</cell><cell>83.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,45.36,54.25,251.75,101.25"><head>Table 2 .</head><label>2</label><figDesc xml:id="_wsBXytr">Confusion matrix for emotion prediction using finetuned CNN</figDesc><table coords="4,52.80,78.75,235.76,76.75"><row><cell></cell><cell cols="7">Anger Boredom Disgust Fear Happy Neutral Sad</cell></row><row><cell>Anger</cell><cell>92.98</cell><cell>0.00</cell><cell>0.00</cell><cell>3.51</cell><cell>1.75</cell><cell>1.75</cell><cell>0.00</cell></row><row><cell cols="2">Boredom 3.48</cell><cell>37.39</cell><cell>6.96</cell><cell>0.00</cell><cell>0.00</cell><cell cols="2">41.74 10.43</cell></row><row><cell cols="2">Disgust 22.78</cell><cell>2.53</cell><cell cols="2">49.37 3.80</cell><cell>3.80</cell><cell cols="2">11.39 6.33</cell></row><row><cell>Fear</cell><cell>34.67</cell><cell>2.67</cell><cell cols="3">0.00 46.67 1.33</cell><cell cols="2">13.33 1.33</cell></row><row><cell cols="2">Happy 73.86</cell><cell>1.14</cell><cell>3.41</cell><cell cols="2">3.41 17.05</cell><cell>1.14</cell><cell>0.00</cell></row><row><cell>Neutral</cell><cell>5.26</cell><cell>8.42</cell><cell>1.05</cell><cell>2.11</cell><cell>3.16</cell><cell cols="2">75.79 4.21</cell></row><row><cell>Sad</cell><cell>0.00</cell><cell>10.00</cell><cell>1.54</cell><cell>2.31</cell><cell>0.00</cell><cell cols="2">10.77 75.38</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="978" xml:id="foot_0">-1-5090-5140-3/17/$31.00 ©2017 IEEE</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,351.00,511.41,215.65,8.83;4,351.00,520.59,215.66,8.83;4,351.00,529.77,193.52,8.83" xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_yvk2m4v">Survey on speech emotion recognition: Features, classification schemes, and databases</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">El</forename><surname>Ayadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Karray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dheUxgK">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="572" to="587" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,350.98,538.95,215.71,8.83;4,351.00,548.13,215.70,8.83;4,351.00,557.37,215.66,8.83;4,351.00,566.55,215.63,8.83;4,351.00,575.73,155.83,8.83" xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_wQ9jtpg">Active learning for speech emotion recognition using conditional random fields</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_fpesuyQ">Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD), 2013 14th ACIS International Conference on</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="127" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,350.95,584.91,215.71,8.83;4,351.00,594.09,215.64,8.83;4,351.00,603.33,148.04,8.83" xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_amTsVv7">The production and recognition of emotions in speech: features and algorithms</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pierre-Yves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Jzt9vrZ">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="157" to="183" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,350.95,612.51,215.72,8.83;4,351.00,621.69,215.65,8.83;4,351.00,630.87,215.62,8.83;4,351.00,640.05,215.63,8.83;4,351.00,649.29,196.64,8.83" xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_bdRNKr2">Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_hgUqaUC">Acoustics, Speech, and Signal Processing</title>
				<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">80</biblScope>
		</imprint>
	</monogr>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct coords="4,351.00,658.47,215.62,8.83;4,351.00,667.65,215.61,8.83;4,351.00,676.83,215.63,8.83;4,351.00,686.01,130.52,8.83" xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_Ct6zt9m">Acoustical properties of speech as indicators of depression and suicidal risk</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>France</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">G</forename><surname>Shiavi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4ZaTcBn">IEEE transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="829" to="837" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,350.97,695.25,215.59,8.83;4,351.00,704.43,215.65,8.83;5,81.36,57.15,215.60,8.83;5,81.36,68.01,34.03,8.83" xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_v2S3nru">Icarus: Source generator based real-time recognition of speech in noisy stressful and lombard effect environments☆</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Cairns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WZj2DkR">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="391" to="422" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,81.34,77.19,215.61,8.83;5,81.36,86.37,215.64,8.83;5,81.36,95.55,215.63,8.83;5,81.36,104.73,189.97,8.83" xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_R9Zwx8a">Dempster-Shafer Fusion Based Gender Recognition for Speech Analysis Applications</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">I</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">W</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_6nrdhTz">2016 International Conference on Platform Technology and Service</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,81.32,113.97,215.65,8.83;5,81.36,123.15,215.57,8.83;5,81.36,132.33,215.72,8.83;5,81.36,141.51,18.07,8.83" xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_rcNS58f">Automatic speaker age and gender recognition using acoustic and prosodic level information fusion</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_aBdVVV2">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="151" to="167" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,81.33,150.69,215.68,8.83;5,81.36,159.93,215.59,8.83;5,81.36,169.11,68.71,8.83" xml:id="b8">
	<monogr>
		<title level="m" type="main" xml:id="_r7zvM2C">Age and gender classification using fusion of acoustic and prosodic features</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Meinedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Trancoso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="2818" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,81.36,178.29,215.64,8.83;5,81.36,187.47,215.65,8.83;5,81.36,196.65,210.43,8.83" xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_zAsXc8Q">Modeling prosodic feature sequences for speaker recognition</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kajarekar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9UHYX3Q">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="455" to="472" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,81.34,205.89,215.70,8.83;5,81.36,215.07,215.59,8.83;5,81.36,224.25,215.66,8.83;5,81.36,233.43,68.71,8.83" xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_62AnGZk">Environmental sound recognition with time-frequency audio features</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_c5uyBk2">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1142" to="1158" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,81.31,242.67,215.62,8.83;5,81.36,251.85,215.72,8.83;5,81.36,261.03,215.71,8.83;5,81.36,270.21,215.60,8.83;5,81.36,279.39,34.03,8.83" xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_Z34Dn3K">Gender Identification using MFCC for Telephone Applications -A Comparative Study</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fiaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>-I. Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sodanil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">W</forename><surname>Baik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5FkBjr8">International Journal of Computer Science and Electronics Engineering</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="351" to="355" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,81.31,288.63,215.65,8.83;5,81.36,297.81,215.65,8.83;5,81.36,306.99,215.64,8.83;5,81.36,316.17,32.71,8.83" xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_JCp4RKd">Speech enhancement using perceptual wavelet packet decomposition and teager energy operator</title>
		<author>
			<persName coords=""><forename type="first">S.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_CZuYuqu">Real World Speech Processing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="51" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,81.31,325.35,215.78,8.83;5,81.36,334.59,215.68,8.83;5,81.36,343.77,215.72,8.83;5,81.36,352.95,38.05,8.83" xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_45nWrKJ">Learning salient features for speech emotion recognition using convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_BGwsVex">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2203" to="2213" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,81.33,362.14,215.69,8.83;5,81.36,371.32,215.65,8.83;5,81.36,380.56,215.64,8.83;5,81.36,389.74,130.69,8.83" xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_FHu7Z3x">Deep convolutional neural networks for LVCSR</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Gt9PXRc">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8614" to="8618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,81.26,398.92,215.73,8.83;5,81.36,408.10,215.69,8.83;5,81.36,417.28,147.79,8.83" xml:id="b15">
	<monogr>
		<title level="m" type="main" xml:id="_q2JJyNQ">Feature learning in deep neural networks-studies on speech recognition tasks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3605</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,81.33,426.52,215.78,8.83;5,81.36,435.70,215.70,8.83;5,81.36,444.88,147.61,8.83" xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_y2QrCV4">Spectrogram image feature for sound event classification in mismatched conditions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_h9uUNYZ">IEEE signal processing letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="130" to="133" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,81.33,454.06,215.71,8.83;5,81.36,463.24,215.67,8.83;5,81.36,472.48,215.68,8.83;5,81.36,481.66,97.15,8.83" xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_ertyBG2">Unsupervised feature learning for audio classification using convolutional deep belief networks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Largman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_TcmqNZx">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,81.31,490.84,215.70,8.83;5,81.36,500.02,215.55,8.83;5,81.36,509.20,215.63,8.83;5,81.36,518.44,148.69,8.83" xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_8PDpA5N">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_EpDHcVU">Proceedings of the 22nd ACM international conference on Multimedia</title>
				<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,81.35,527.62,23.35,8.83;5,141.00,527.62,22.62,8.83;5,199.93,527.62,27.30,8.83;5,263.58,527.62,33.35,8.83;5,81.36,536.80,117.46,8.83" xml:id="b19">
	<monogr>
		<ptr target="https://github.com/NVIDIA/DIGITS" />
		<title level="m" xml:id="_xtnn4TH">NVidia DIGITS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,81.31,545.98,215.65,8.83;5,81.36,555.15,215.65,8.84;5,81.36,564.39,68.71,8.83" xml:id="b20">
	<monogr>
		<title level="m" type="main" xml:id="_xnaPCSK">A database of German emotional speech</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paeschke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rolfes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">F</forename><surname>Sendlmeier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Interspeech</publisher>
			<biblScope unit="page" from="1517" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,81.33,573.57,215.69,8.83;5,81.36,582.75,215.58,8.83;5,81.36,591.93,215.71,8.83;5,81.36,601.11,36.73,8.83" xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_GTaUjU3">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_upHEWWY">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,81.30,610.35,215.64,8.83;5,81.36,619.53,215.55,8.83;5,81.36,628.71,215.66,8.83;5,81.36,637.89,18.07,8.83" xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_7uEpEAA">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hDQ7EUT">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
