<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_AGftMSf">A Comparative Study Of Text Compression Algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="2,130.86,159.44,115.79,9.30"><forename type="first">Senthil</forename><surname>Shanmugasundaram</surname></persName>
							<email>senthil_udt@rediffmail.com</email>
						</author>
						<author>
							<persName coords="2,381.60,154.34,82.98,9.30"><forename type="first">Robert</forename><surname>Lourdusamy</surname></persName>
							<email>robert_lourdes@yahoo.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Vidyasagar College of Arts and Science</orgName>
								<address>
									<settlement>Udumalpet</settlement>
									<region>Tamilnadu</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Computer Science &amp; Info. System Department</orgName>
								<orgName type="institution">Community College in Al-Qwaiya</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Shaqra University</orgName>
								<orgName type="institution" key="instit2">KSA (Government Arts College</orgName>
								<address>
									<settlement>Coimbatore-641 018)</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_KAzjHz6">A Comparative Study Of Text Compression Algorithms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7F3DD81F3C6EF2BB7F5B1EE888F6C985</idno>
					<idno type="DOI">10.21917/ijct.2011.0062</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-05-07T13:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_54fqppW"><p xml:id="_P8BHkmC">Data Compression is the science and art of representing information in a compact form. For decades, Data compression has been one of the critical enabling technologies for the ongoing digital multimedia revolution. There are lot of data compression algorithms which are available to compress files of different formats. This paper provides a survey of different basic lossless data compression algorithms. Experimental results and comparisons of the lossless compression algorithms using Statistical compression techniques and Dictionary based compression techniques were performed on text data. Among the statistical coding techniques the algorithms such as Shannon-Fano Coding, Huffman coding, Adaptive Huffman coding, Run Length Encoding and Arithmetic coding are considered. Lempel Ziv scheme which is a dictionary based technique is divided into two families: those derived from LZ77 (LZ77, LZSS, LZH and LZB) and those derived from LZ78 (LZ78, LZW and LZFG). A set of interesting conclusions are derived on their basis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_6rWpGXW">I. INTRODUCTION</head><p xml:id="_ehvYTmC">Data compression refers to reducing the amount of space needed to store data or reducing the amount of time needed to transmit data. The size of data is reduced by removing the excessive information. The goal of data compression is to represent a source in digital form with as few bits as possible while meeting the minimum requirement of reconstruction of the original. Data compression can be lossless, only if it is possible to exactly reconstruct the original data from the compressed version. Such a lossless technique is used when the original data of a source are so important that we cannot afford to lose any details. Examples of such source data are medical images, text and images preserved for legal reason, some computer executable files, etc.</p><p xml:id="_yVNp9Yd">Another family of compression algorithms is called lossy as these algorithms irreversibly remove some parts of data and only an approximation of the original data can be reconstructed. Approximate reconstruction may be desirable since it may lead to more effective compression. However, it often requires a good balance between the visual quality and the computation complexity. Data such as multimedia images, video and audio are more easily compressed by lossy compression techniques because of the way human visual and hearing systems work.</p><p xml:id="_RFc9WqU">Lossy algorithms achieve better compression effectiveness than lossless algorithms, but lossy compression is limited to audio, images, and video, where some loss is acceptable.</p><p xml:id="_rhgvYnE">The question of the better technique of the two, "lossless" or "lossy" is pointless as each has its own uses with lossless techniques better in some cases and lossy technique better in others.</p><p xml:id="_Mw4UPsV">There are quite a few lossless compression techniques nowadays, and most of them are based on dictionary or probability and entropy. In other words, they all try to utilize the occurrence of the same character/string in the data to achieve compression. This paper examines the performance of statistical compression techniques such as Shannon-Fano Coding, Huffman coding, Adaptive Huffman coding, Run Length Encoding and Arithmetic coding. The Dictionary based compression technique Lempel-Ziv scheme is divided into two families: those derived from LZ77 (LZ77, LZSS, LZH and LZB) and those derived from LZ78 (LZ78, LZW and LZFG).</p><p xml:id="_EBQUs4t">The paper is organized as follows: Section I contains a brief Introduction about Compression and its types, Section II presents a brief explanation about Statistical compression techniques, Section III discusses about Dictionary based compression techniques, Section IV has its focus on comparing the performance of Statistical coding techniques and Lempel Ziv techniques and the final section contains the Conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_pYyEPPv">II. STATISTICAL COMPRESSION TECHNIQUES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1" xml:id="_B52sRqw">RUN LENGTH ENCODING TECHNIQUE (RLE)</head><p xml:id="_Yt9sVZM">One of the simplest compression techniques known as the Run-Length Encoding (RLE) is created especially for data with strings of repeated symbols (the length of the string is called a run). The main idea behind this is to encode repeated symbols as a pair: the length of the string and the symbol. For example, the string 'abbaaaaabaabbbaa' of length 16 bytes (characters) is represented as 7 integers plus 7 characters, which can be easily encoded on 14 bytes (as for example '1a2b5a1b2a3b2a'). The biggest problem with RLE is that in the worst case the size of output data can be two times more than the size of input data. To eliminate this problem, each pair (the lengths and the strings separately) can be later encoded with an algorithm like Huffman coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2" xml:id="_FRyh2cM">SHANNON FANO CODING</head><p xml:id="_ut6Jh3Y">Shannon -Fano algorithm was simultaneously developed by Claude Shannon (Bell labs) and R.M. Fano (MIT) <ref type="bibr" coords="3,100.37,241.60,12.20,8.48" target="#b2">[3,</ref><ref type="bibr" coords="3,112.56,241.60,12.20,8.48" target="#b14">15]</ref>. It is used to encode messages depending upon their probabilities. It allots less number of bits for highly probable messages and more number of bits for rarely occurring messages. The algorithm is as follows:</p><p xml:id="_drhqhSP">1. For a given list of symbols, develop a frequency or probability table. 2. Sort the table according to the frequency, with the most frequently occurring symbol at the top. 3. Divide the table into two halves with the total frequency count of the upper half being as close to the total frequency count of the bottom half as possible. 4. Assign the upper half of the list a binary digit '0' and the lower half a '1'. 5. Recursively apply the steps 3 and 4 to each of the two halves, subdividing groups and adding bits to the codes until each symbol has become a corresponding leaf on the tree.</p><p xml:id="_gkXMja9">Generally, Shannon-Fano coding does not guarantee that an optimal code is generated. Shannon -Fano algorithm is more efficient when the probabilities are closer to inverses of powers of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3" xml:id="_sAXCGvR">HUFFMAN CODING</head><p xml:id="_9EWAK37">The Huffman coding algorithm <ref type="bibr" coords="3,218.27,533.74,11.02,8.48" target="#b5">[6]</ref> is named after its inventor, David Huffman, who developed this algorithm as a student in a class on information theory at MIT in 1950. It is a more successful method used for text compression. Huffman's idea is to replace fixed-length codes (such as ASCII) by variable-length codes, assigning shorter codewords to the more frequently occuring symbols and thus decreasing the overall length of the data. When using variable-length codewords, it is desirable to create a (uniquely decipherable) prefix-code, avoiding the need for a separator to determine codeword boundaries. Huffman coding creates such a code.</p><p xml:id="_kW3TCvn">Huffman algorithm is not very different from Shannon -Fano algorithm. Both the algorithms employ a variable bit probabilistic coding method. The two algorithms significantly differ in the manner in which the binary tree is built. Huffman uses bottom-up approach and Shanon-Fano uses Top-down approach.</p><p xml:id="_HwjAKQK">The Huffman algorithm is simple and can be described in terms of creating a Huffman code tree. The procedure for building this tree is:</p><p xml:id="_6F46cca">1. Start with a list of free nodes, where each node corresponds to a symbol in the alphabet. 2. Select two free nodes with the lowest weight from the list. 3. Create a parent node for these two nodes selected and the weight is equal to the weight of the sum of two child nodes. 4. Remove the two child nodes from the list and the parent node is added to the list of free nodes. 5. Repeat the process starting from step-2 until only a single tree remains. After building the Huffman tree, the algorithm creates a prefix code for each symbol from the alphabet simply by traversing the binary tree from the root to the node, which corresponds to the symbol. It assigns 0 for a left branch and 1 for a right branch.</p><p xml:id="_crNKqT5">The algorithm presented above is called as a semiadaptive or semi-static Huffman coding as it requires knowledge of frequencies for each symbol from alphabet. Along with the compressed output, the Huffman tree with the Huffman codes for symbols or just the frequencies of symbols which are used to create the Huffman tree must be stored. This information is needed during the decoding process and it is placed in the header of the compressed file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4" xml:id="_QQWtcX4">ADAPTIVE HUFFMAN CODING</head><p xml:id="_346sBYP">The basic Huffman algorithm suffers from the drawback that to generate Huffman codes it requires the probability distribution of the input set which is often not available. Moreover it is not suitable to cases when probabilities of the input symbols are changing. The Adaptive Huffman coding technique was developed based on Huffman coding first by Newton Faller <ref type="bibr" coords="3,504.34,544.60,11.02,8.48" target="#b1">[2]</ref> and by Robert G. Gallager <ref type="bibr" coords="3,400.31,555.34,11.85,8.48" target="#b4">[5]</ref> and then improved by Donald Knuth <ref type="bibr" coords="3,340.30,566.20,11.01,8.48" target="#b7">[8]</ref> and Jefferey S. Vitter <ref type="bibr" coords="3,436.38,566.20,14.95,8.48" target="#b16">[17,</ref><ref type="bibr" coords="3,451.32,566.20,11.21,8.48" target="#b17">18]</ref>. In this method, a different approach known as sibling property is followed to build a Huffman tree. Here, both sender and receiver maintain dynamically changing Huffman code trees whose leaves represent characters seen so far. Initially the tree contains only the 0-node, a special node representing messages that have yet to be seen. Here, the Huffman tree includes a counter for each symbol and the counter is updated every time when a corresponding input symbol is coded. Huffman tree under construction is still a Huffman tree if it is ensured by checking whether the sibling property is retained. If the sibling property is violated, the tree has to be restructured to ensure this property. Usually this algorithm generates codes that are more effective than static Huffman coding. Storing Huffman tree along with the Huffman codes for symbols with the Huffman tree is not needed here. It is superior to Static Huffman coding in two aspects: It requires only one pass through the input and it adds little or no overhead to the output. But this algorithm has to rebuild the entire Huffman tree after encoding each symbol which becomes slower than the static Huffman coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5" xml:id="_ZmTtgVk">ARITHMETIC CODING</head><p xml:id="_N4Z78y9">Huffman and Shannon-Fano coding techniques suffer from the fact that an integral value of bits is needed to code a character. Arithmetic coding completely bypasses the idea of replacing every input symbol with a codeword. Instead it replaces a stream of input symbols with a single floating point number as output. The basic concept of arithmetic coding was developed by Elias in the early 1960's and further developed largely by Pasco <ref type="bibr" coords="4,100.03,306.52,15.99,8.48" target="#b10">[11]</ref>, Rissanen <ref type="bibr" coords="4,158.63,306.52,15.70,8.48" target="#b12">[13,</ref><ref type="bibr" coords="4,174.32,306.52,11.77,8.48" target="#b13">14]</ref> and Langdon <ref type="bibr" coords="4,236.36,306.52,13.66,8.48" target="#b8">[ 9]</ref>.</p><p xml:id="_pdjhnXd">The main aim of Arithmetic coding is to assign an interval to each potential symbol. Then a decimal number is assigned to this interval. The algorithm starts with an interval of 0.0 and 1.0. After each input symbol from the alphabet is read, the interval is subdivided into a smaller interval in proportion to the input symbol's probability. This subinterval then becomes the new interval and is divided into parts according to probability of symbols from the input alphabet. This is repeated for each and every input symbol. And, at the end, any floating point number from the final interval uniquely determines the input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_kt8Uce7">III. DICTIONARY BASED COMPRESSION TECHNIQUES</head><p xml:id="_g4k64Xj">Arithmetic algorithms as well as Huffman algorithms are based on a statistical model, namely an alphabet and the probability distribution of a source. Dictionary coding techniques rely upon the observation that there are correlations between parts of data (recurring patterns). The basic idea is to replace those repetitions by (shorter) references to a "dictionary" containing the original.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" xml:id="_cdnKNMj">LEMPEL ZIV ALGORITHMS</head><p xml:id="_S7N7QtV">The Lempel Ziv Algorithm is an algorithm for lossless data compression. It is not a single algorithm, but a whole family of algorithms, stemming from the two algorithms proposed by Jacob Ziv and Abraham Lempel in their landmark papers in 1977 and 1978.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1" xml:id="_wTW5UPC">LZ77:</head><p xml:id="_USVZets">Jacob Ziv and Abraham Lempel have presented their dictionary-based scheme in 1977 for lossless data compression <ref type="bibr" coords="4,370.44,222.46,14.38,8.48" target="#b19">[20]</ref>. Today this technique is much remembered by the name of the authors and the year of implementation of the same.</p><p xml:id="_GPqgU8j">LZ77 exploits the fact that words and phrases within a text file are likely to be repeated. When there is repetition, they can be encoded as a pointer to an earlier occurrence, with the pointer accompanied by the number of characters to be matched. It is a very simple adaptive scheme that requires no prior knowledge of the source and seems to require no assumptions about the characteristics of the source.</p><p xml:id="_9rQdbGe">In the LZ77 approach, the dictionary is simply a portion of the previously encoded sequence. The encoder examines the input sequence through a sliding window which consists of two parts: a search buffer that contains a portion of the recently encoded sequence and a look ahead buffer that contains the next portion of the sequence to be encoded. The algorithm searches the sliding window for the longest match with the beginning of the look-ahead buffer and outputs a reference (a pointer) to that match. It is possible that there is no match at all, so the output cannot contain just pointers. In LZ77 the reference is always output as a triple &lt;o,l,c&gt;, where 'o' is an offset to the match, 'l' is length of the match, and 'c' is the next symbol after the match. If there is no match, the algorithm outputs a null-pointer (both the offset and the match length equal to 0) and the first symbol in the look-ahead buffer <ref type="bibr" coords="4,433.92,525.40,10.90,8.48" target="#b6">[7]</ref>.</p><p xml:id="_QAMa6xH">The values of an offset to a match and length must be limited to some maximum constants. Moreover the compression performance of LZ77 mainly depends on these values. Usually the offset is encoded on 12-16 bits, so it is limited from 0 to 65535 symbols. So, there is no need to remember more than 65535 last seen symbols in the sliding window. The match length is usually encoded on 8 bits, which gives maximum match length equal to 255 <ref type="bibr" coords="4,326.49,633.57,16.04,8.48" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_PC5NwAG">The LZ77 algorithm is given below:</head><p xml:id="_GaawQ23">With regard to other algorithms the time for compression and decompression is just the same. In LZ77 encoding process one reference (a triple) is transmitted for several input symbols and hence it is very fast. The decoding is much faster than the encoding in this process and it is one of the important features of this process. In LZ77, most of the LZ77 compression time is, however, used in searching for the longest match, whereas the LZ77 algorithm decompression is quick as each reference is simply replaced with the string, which it points to.</p><p xml:id="_NAWzNDe">There are lots of ways that LZ77 scheme can be made more efficient and many of the improvements deal with the efficient encoding with the triples. There are several variations on LZ77 scheme, the best known are LZSS, LZH and LZB. LZSS which was published by Storer and Szymanksi <ref type="bibr" coords="5,169.55,447.21,15.76,8.48" target="#b15">[16]</ref> removes the requirement of mandatory inclusion of the next non-matching symbol into each codeword. Their algorithm uses fixed length codewords consisting of offset and length to denote references. They propose to include an extra bit (a bit flag) at each coding step to indicate whether the output code represents a pair (a pointer and a match length) or a single symbol.</p><p xml:id="_K4j3fx3">LZH is the scheme that combines the Ziv -Lempel and Huffman techniques. Here coding is performed in two passes. The first is essentially same as LZSS, while the second uses statistics measured in the first to code pointers and explicit characters using Huffman coding.</p><p xml:id="_jWmZ7mg">LZB was published by Mohammad Banikazemi <ref type="bibr" coords="5,280.23,609.45,17.36,8.48" target="#b9">[10]</ref> uses an elaborate scheme for encoding the references and lengths with varying sizes. Regardless of the length of the phrase it represents, every LZSS pointer is of the same size. In practice a better compression is achieved by having different sized pointers as some phrase lengths are much more likely to occur than others. LZB is a technique that uses a different coding for both components of the pointer. LZB achieves a better compression than LZSS and has the added virtue of being less sensitive to the choice of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2" xml:id="_6nMwCXh">LZ78</head><p xml:id="_kpXA4B8">In 1978 Jacob Ziv and Abraham Lempel presented their dictionary based scheme <ref type="bibr" coords="5,435.28,155.07,14.43,8.48" target="#b20">[21]</ref>, which is known as LZ78. It is a dictionary based compression algorithm that maintains an explicit dictionary. This dictionary has to be built both at the encoding and decoding side and they must follow the same rules to ensure that they use an identical dictionary. The codewords output by the algorithm consists of two elements &lt;i,c&gt; where 'i' is an index referring to the longest matching dictionary entry and the first non-matching symbol. In addition to outputting the codeword for storage / transmission the algorithm also adds the index and symbol pair to the dictionary. When a symbol that is not yet found in the dictionary, the codeword has the index value 0 and it is added to the dictionary as well. The algorithm gradually builds up a dictionary with this method. The algorithm for LZ78 is given below: LZ78 algorithm has the ability to capture patterns and hold them indefinitely but it also has a serious drawback. The dictionary keeps growing forever without bound. There are various methods to limit dictionary size, the easiest being to stop adding entries and continue like a static dictionary coder or to throw the dictionary away and start from scratch after a certain number of entries has been reached. The encoding done by LZ78 is fast, compared to LZ77, and that is the main advantage of dictionary based compression. The important property of LZ77 that the LZ78 algorithm preserves is the decoding is faster than the encoding. The decompression in LZ78 is faster compared to the process of compression. LZW Terry Welch has presented his LZW (Lempel-Ziv-Welch) algorithm in 1984 <ref type="bibr" coords="5,412.07,674.30,16.34,8.48" target="#b18">[19]</ref>, which is based on LZ78. It basically applies the LZSS principle of not explicitly transmitting the next non-matching symbol to LZ78 algorithm. The dictionary has to be initialized with all While (lookAheadBuffer not empty) { get a reference (position, length) to longest match; if (length &gt; 0) { output (position, length, next symbol); shift the window length+1 positions along; } else { output (0, 0, first symbol in the lookahead buffer); shift the window 1 character along; } } w := NIL; while ( there is input ) { K := next symbol from input; if (wK exists in the dictionary) { w := wK; } else { output (index(w), K); add wK to the dictionary; w := NIL; } } possible symbols from the input alphabet. It guarantees that a match will always be found. LZW would only send the index to the dictionary. The input to the encoder is accumulated in a pattern 'w' as long as 'w' is contained in the dictionary. If the addition of another letter 'K' results in a pattern 'w*K' that is not in the dictionary, then the index of 'w' is transmitted to the receiver, the pattern 'w*K' is added to the dictionary and another pattern is started with the letter 'K'. The algorithm then proceeds as follows:</p><p xml:id="_8gM448b">In the original proposal of LZW, the pointer size is chosen to be 12 bits, allowing for up to 4096 dictionary entries. Once the limit is reached, the dictionary becomes static.</p><p xml:id="_cNHQCNf">LZFG which was developed by Fiala and Greene <ref type="bibr" coords="6,80.04,414.75,10.05,8.48" target="#b3">[4]</ref>, gives fast encoding and decoding and good compression without undue storage requirements. This algorithm uses the original dictionary building technique as LZ78 does but the only difference is that it stores the elements in a trie data structure. Here, the encoded characters are placed in a window (as in LZ77) to remove the oldest phrases from the dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_WVPnHmG">IV. EXPERIMENTAL RESULTS</head><p xml:id="_TWjrUnp">In this section we focus our attention to compare the performance of various Statistical compression techniques (Run Length Encoding, Shannon-Fano coding, Huffman coding, Adaptive Huffman coding and Arithmetic coding), LZ77 family algorithms (LZ77, LZSS, LZH and LZB) and LZ78 family algorithms (LZ78, LZW and LZFG). Research works done to evaluate the efficiency of any compression algorithm are carried out having two important parameters. One is the amount of compression achieved and the other is the time used by the encoding and decoding algorithms. We have tested several times the practical performance of the above mentioned techniques on files of Canterbury corpus and have found out the results of various Statistical coding techniques and Lempel -Ziv techniques selected for this study. Also, the comparative functioning and the compression ratio are presented in the tables given below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" xml:id="_Sgb2TVW">PRACTICAL COMPARISON OF STATISTICAL COMPRESSION ZECHNIQUES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_AEErQ4Q">Table -I shows the comparative analysis between various Statistical compression techniques discussed above.</head><p xml:id="_jGCCk34">As per the results shown in Table -I, for Run Length Encoding, for most of the files tested, this algorithm generates compressed files larger than the original files. This is due to the fewer amount of runs in the source file. For the other files, the compression rate is less. The average BPC obtained by this algorithm is 7.93. So, it is inferred that this algorithm can reduce on an average of about 4% of the original file. This can not be considered as a significant improvement.  The overall behaviour of Shannon-Fano coding, Static Huffman coding and Adaptive Huffman coding is very similar with Arithmetic coding achieving the best average compression. The reason for this is the ability of this algorithm to keep the coding and the modeler separate. Unlike Huffman coding, no code tree needs to be transmitted to the receiver. Here, encoding is done to a group of symbols, not symbol by symbol, which leads to higher compression ratios. One more reason is its use of fractional values which leads to no code waste.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" xml:id="_SWA3nad">PRACTICAL COMPARISON OF LEMPEL ZIV ALGORITHMS</head><p xml:id="_cWcVC3x">This section deals with comparing the performance of Lempel-Ziv algorithms. LZ algorithms considered here are divided into two categories: those derived from LZ77 and those derived from LZ78. The BPC measurements are referred from <ref type="bibr" coords="8,203.82,165.88,10.00,8.48" target="#b0">[1]</ref>. Table -II shows the comparison of various algorithms that are derived from LZ77(LZ77, LZSS, LZH and LZB). Table -III shows the comparative analysis of algorithms that are derived from LZ78 (LZ78, LZW and LZFG). The BPC values that are referred from <ref type="bibr" coords="8,172.53,220.00,11.06,8.48" target="#b0">[1]</ref> are based on the following parameters.</p><p xml:id="_x2mXT49">The main parameter for LZ77 family is the size of the window on the text. Compression is best if the window is as big as possible but not bigger than the text, in general. Nevertheless, larger windows yield diminishing returns. A window as small as 8000 characters will perform much better, and give a result nearly as good as the ones derived from the larger windows. Another parameter which limits the number of characters is needed for some algorithms belonging to LZ family. Generally a limit of around 16 may work well. For LZ77, LZSS and LZB the storage (characters in window) were assumed to be of 8 KB and for LZH it was assumed as 16 KB.</p><p xml:id="_tKe25pa">Regarding LZ78 family, most of the algorithm requires one parameter to denote the maximum number of phrases stored. For the above mentioned LZ78 schemes, except LZ78 a limit of 4096 phrases was used. Table <ref type="table" coords="8,201.47,317.37,5.77,8.48" target="#tab_0">II</ref>.</p><p xml:id="_x79KdWv">Comparison of BPC for the different LZ77 variants</p><p xml:id="_y6uBEnZ">The output of Table -II reveals that the Bits Per Character is significant and most of the files have been compressed to a little less than half of the original size. Of LZ77 family, the performance of LZB is significant compared to LZ77, LZSS and LZH. The average BPC which is significant as, shown in Table -II, which is 3.11.</p><p xml:id="_EuvRxFX">Amongst the performance of the LZ77 family, LZB outperforms LZH. This is because, LZH generates an optimal Huffman code for pointers whereas LZB uses a fixed code.    <ref type="table" coords="9,199.91,371.43,8.79,8.48" target="#tab_0">III</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_vjMMMrt">S.No File Names</head><p xml:id="_Exvz36R">Comparison of BPC for the different LZ78 variants</p><p xml:id="_KnW88we">We have tried to infer from Table -III the compression performance of LZ78 family. Most of the ASCII files are compressed to just less than half of the original size and within each file the amount of compression is consistent. The LZW method, having no boundary, accepts phrases and so the compression expands the file 'obj2' by 25%, which is considered as a weakness of this approach. Also from Table -III it is obvious that the performance of LZFG is the best amongst these methods, giving an average BPC of 2.89 which is really significant. Amongst LZ78 family, LZFG's performance is the best because the scheme that  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_cB5vhPS">V. CONCLUSION</head><p xml:id="_7k2w3K4">We have taken up Statistical compression techniques and Lempel Ziv algorithms for our study to examine the performance in compression. In the Statistical compression techniques, Arithmetic coding technique outperforms the rest with an improvement of 1.15% over Adaptive Huffman coding, 2.28% over Huffman coding, 6.36% over Shannon-Fano coding and 35.06% over Run Length Encoding technique. LZB outperforms LZ77, LZSS and LZH to show a marked compression, which is 19.85% improvement over LZ77, 6.33% improvement over LZSS and 3.42% improvement over LZH, amongst the LZ77 family. LZFG shows a significant result in the average BPC compared to LZ78 and LZW. From the result it is evident that LZFG has outperformed the other two with an improvement of 32.16% over LZ78 and 41.02% over LZW.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,331.38,274.05,200.54,8.48;8,314.46,284.91,117.35,8.48"><head>Fig 2</head><label>2</label><figDesc xml:id="_SWpHdCs">Fig 2 shows a comparison of the compression rates for the different LZ77 variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,159.60,90.16,309.62,8.48;9,96.96,100.96,2.36,8.48"><head>Fig 2</head><label>2</label><figDesc xml:id="_uMZCsEW">Fig 2 shows a comparison of the compression rates for the different LZ77 variants .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,191.46,349.83,228.97,8.48;9,176.16,371.43,35.47,8.48;9,242.16,371.43,193.67,8.48"><head>Fig 2 .</head><label>2</label><figDesc xml:id="_Gqvfbah">Fig 2. Chart showing Compression rates for the LZ77 family TableIII.Comparison of BPC for the different LZ78 variants</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,192.54,338.97,226.93,8.48"><head>Fig 3 .</head><label>3</label><figDesc xml:id="_tvjbBDJ">Fig 3. Chart showing compression rates for the LZ78 family</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,141.48,100.96,329.00,552.26"><head>Table I .</head><label>I</label><figDesc xml:id="_XK8XYJQ">Comparison of BPC for different Statistical Compression techniques</figDesc><table coords="7,141.48,123.22,329.00,530.00"><row><cell>S.No</cell><cell>File</cell><cell>File</cell><cell cols="2">RLE Shannon</cell><cell>Huffman</cell><cell>Adaptive</cell><cell>Arithmetic</cell></row><row><cell></cell><cell>names</cell><cell>Size</cell><cell></cell><cell>Fano</cell><cell>coding</cell><cell>Huffman</cell><cell>coding</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>coding</cell><cell></cell><cell>coding</cell></row><row><cell></cell><cell></cell><cell></cell><cell>BPC</cell><cell>BPC</cell><cell>BPC</cell><cell>BPC</cell><cell>BPC</cell></row><row><cell>1.</cell><cell>bib</cell><cell cols="2">111261 8.16</cell><cell>5.56</cell><cell>5.26</cell><cell>5.24</cell><cell>5.23</cell></row><row><cell>2.</cell><cell cols="3">book1 768771 8.17</cell><cell>4.83</cell><cell>4.57</cell><cell>4.56</cell><cell>4.55</cell></row><row><cell>3.</cell><cell cols="3">book2 610856 8.16</cell><cell>5.08</cell><cell>4.83</cell><cell>4.83</cell><cell>4.78</cell></row><row><cell>4.</cell><cell>news</cell><cell cols="2">377109 7.98</cell><cell>5.41</cell><cell>5.24</cell><cell>5.23</cell><cell>5.19</cell></row><row><cell>5.</cell><cell>obj1</cell><cell cols="2">21504 7.21</cell><cell>6.57</cell><cell>6.45</cell><cell>6.11</cell><cell>5.97</cell></row><row><cell>6.</cell><cell>obj2</cell><cell cols="2">246814 8.05</cell><cell>6.50</cell><cell>6.33</cell><cell>6.31</cell><cell>6.07</cell></row><row><cell>7.</cell><cell>paper1</cell><cell cols="2">53161 8.12</cell><cell>5.34</cell><cell>5.09</cell><cell>5.04</cell><cell>4.98</cell></row><row><cell>8.</cell><cell>paper2</cell><cell cols="2">82199 8.14</cell><cell>4.94</cell><cell>4.68</cell><cell>4.65</cell><cell>4.63</cell></row><row><cell>9.</cell><cell>progc</cell><cell cols="2">39611 8.10</cell><cell>5.47</cell><cell>5.33</cell><cell>5.26</cell><cell>5.23</cell></row><row><cell>10.</cell><cell>progl</cell><cell cols="2">71646 7.73</cell><cell>5.11</cell><cell>4.85</cell><cell>4.81</cell><cell>4.76</cell></row><row><cell>11.</cell><cell>progp</cell><cell cols="2">49379 7.47</cell><cell>5.28</cell><cell>4.97</cell><cell>4.92</cell><cell>4.89</cell></row><row><cell>12.</cell><cell>trans</cell><cell cols="2">93695 7.90</cell><cell>5.88</cell><cell>5.61</cell><cell>5.58</cell><cell>5.49</cell></row><row><cell></cell><cell cols="3">Average BPC 7.93</cell><cell>5.50</cell><cell>5.27</cell><cell>5.21</cell><cell>5.15</cell></row><row><cell cols="8">Fig 1. Chart showing Compression rates for various Statistical Compression techniques</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_T4r25bK"><p xml:id="_v46mRtt">BPC and amount of compression achieved for Shannon-Fano algorithm is presented in Table-I. The compression ratio for Shannon-Fano algorithm is in the range of 0.60 to 0.82 and the average BPC is 5.50.</p><p xml:id="_eyWbDCe">Compression ratio for Huffman coding algorithm falls in the range of 0.57 to 0.81. The compression ratio obtained by this algorithm is better compared to Shannon-Fano algorithm and the average Bits per character is 5.27.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_MDy3NVQ">The amount of compression achieved by applying Adaptive Huffman coding is shown in Table -I. The adaptive version of Huffman coding builds a statistical model of the text being compressed as the file is read.</head><p xml:id="_JQrKeY6">From Table -I it can be seen that, it differs a little from the Shannon-Fano coding algorithm and Static Huffman coding algorithm in the compression ratio achieved and the range is between 0.57 and 0.79. On an average the number of bits needed to code a character is 5.21. Previous attempts in this line of research make it clear that compression and decompression times are relatively high for this algorithm because the dynamic tree used in this algorithm has to be modified for each and every character in the source file.</p><p xml:id="_ddDA94J">Arithmetic coding has been shown to compress files down to the theoretical limits as described by Information theory. Indeed, this algorithm proved to be one of the best performers among these methods based on compression ratio. It is clear that the amount of compression achieved by Arithmetic coding lies within the range of 0.57 to 0.76 and the average bits per character is 5.15.</p><p xml:id="_SXWU2mR">The overall performance in terms of average BPC of the above referred Statistical coding methods are shown in Fig 1 <ref type="figure" coords="6,342.61,706.77,3.55,8.48">.</ref> w := NIL; while ( there is input ) { K := next symbol from input; if (wK exists in the dictionary) { w := wK; } else { output (index(w)); add wK to the dictionary; w := k; } }</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,96.96,576.51,200.52,6.76;10,96.96,585.15,135.93,6.76" xml:id="b0">
	<monogr>
		<title level="m" type="main" xml:id="_5fpZXGj">Text Compression</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Cleary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>Upper Saddle River, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.96,593.79,200.43,6.76;10,96.96,602.43,200.51,6.76;10,96.96,611.13,189.21,6.76" xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_EXb3ySe">An adaptive system for data compression</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Faller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_kBdwGWB">Record of the 7th Asilornar Conference on Circuits, Systems and Computers</title>
				<meeting><address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1973">1973</date>
			<biblScope unit="page" from="593" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.96,619.77,200.52,6.76;10,96.96,628.41,200.53,6.76;10,96.96,637.11,38.72,6.76" xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_8eK7x7r">The Transmission of Information</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Fano</surname></persName>
		</author>
		<idno>No. 65</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_m66qj5S">Research Laboratory of Electronics</title>
				<meeting><address><addrLine>Cambridge, Mass</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1949">1949</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="10,96.98,645.75,200.49,6.76;10,96.96,654.39,190.07,6.76" xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_BBsXAd2">Data Compression with finite windows</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">R</forename><surname>Fiala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Greene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rhrGSGg">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="490" to="505" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.97,663.09,200.48,6.76;10,96.96,671.73,200.59,6.76;10,96.96,680.37,50.61,6.76" xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_DftxhZr">Variations on a theme by Huffman</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">G</forename><surname>Gallager</surname></persName>
		</author>
		<idno>IT-24</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UXQtPYM">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="668" to="674" />
			<date type="published" when="1978-11">November 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.96,689.01,200.41,6.76;10,96.96,697.65,200.53,6.76;10,96.96,706.29,156.21,6.76" xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_X6zH5gr">A method for the construction of minimumredundancy codes</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Huffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_XyV7XCY">Proceedings of the Institute of Radio Engineers</title>
				<meeting>the Institute of Radio Engineers</meeting>
		<imprint>
			<date type="published" when="1952-09">September 1952</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1098" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,333.24,358.84,198.64,8.07;10,331.38,368.79,138.63,6.76" xml:id="b6">
	<monogr>
		<title level="m" type="main" xml:id="_DkHxY55">Introduction to Data Compression</title>
		<author>
			<persName coords=""><forename type="first">Khalid</forename><surname>Sayood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
	<note>2 nd Edition</note>
</biblStruct>

<biblStruct coords="10,331.39,377.43,200.52,6.76;10,331.38,386.07,76.15,6.76" xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_fAjwKQA">Dynamic Huffman coding</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Knuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_x9mEEBu">Journal of Algorithms</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="180" />
			<date type="published" when="1985-06">June 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.38,394.77,200.41,6.76;10,331.38,403.41,200.60,6.76;10,331.38,412.05,38.48,6.76" xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_T9hRhWS">An introduction to arithmetic coding</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Langdon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NaafrwA">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="149" />
			<date type="published" when="1984-03">March 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.40,420.75,202.44,6.76;10,331.38,429.39,200.53,6.76;10,331.38,438.03,131.96,6.76" xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_NFAmvKp">LZB: Data Compression with Bounded References</title>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Banikazemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_RGMZaun">Proceedings of the 2009 Data Compression Conference</title>
				<meeting>the 2009 Data Compression Conference</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,330.00,446.67,201.84,6.76;10,331.38,455.31,200.58,6.76;10,331.38,463.95,52.54,6.76" xml:id="b10">
	<monogr>
		<title level="m" type="main" xml:id="_YH4gu9T">Source coding algorithms for fast data compression</title>
		<author>
			<persName coords=""><forename type="middle">R</forename><surname>Pasco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical Engineering, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D thesis</note>
</biblStruct>

<biblStruct coords="10,331.38,472.65,200.50,6.76;10,331.38,481.29,200.39,6.76;10,331.38,489.93,200.47,6.76;10,331.38,498.63,48.26,6.76" xml:id="b11">
	<monogr>
		<title level="m" type="main" xml:id="_4XT5GMp">Reversible Data transforms that improve effectiveness of universal lossless data compression</title>
		<author>
			<persName coords=""><forename type="first">Przemyslaw</forename><surname>Skibinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Department of Mathematics and Computer Science, University of Wroclaw</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D thesis</note>
</biblStruct>

<biblStruct coords="10,327.67,507.27,204.16,6.76;10,331.38,515.91,200.55,6.76;10,331.38,524.61,17.00,6.76" xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_C4KBYgd">Generalised Kraft inequality and arithmetic coding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_x8kQC7Q">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="198" to="203" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.39,533.25,200.57,6.76;10,331.38,541.89,174.56,6.76" xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_EqUGbQM">Arithmetic coding</title>
		<author>
			<persName coords=""><forename type="first">Rissanen J</forename><surname>Langdon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7CRh4g7">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="162" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,328.61,550.59,203.19,6.76;10,331.38,559.23,150.33,6.76" xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_PdDVMep">A mathematical theory of communication</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zbwdWC8">Bell Sys. Tech. Jour</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="398" to="403" />
			<date type="published" when="1948-07">July, 1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.38,567.81,200.48,6.76;10,331.38,576.51,173.78,6.76" xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_r6F9D3q">Data compression via textual substitution</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Storer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">G</forename><surname>Szymanski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_m5S23RD">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="928" to="951" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,330.72,585.15,201.18,6.76;10,331.38,593.79,154.76,6.76" xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_V8rThzm">Design and analysis of dynamic Huffman codes</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Vitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_N4NfyNH">Journal of the A CM</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="825" to="845" />
			<date type="published" when="1987-10">October 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,326.48,602.43,205.41,6.76;10,331.38,611.13,154.05,6.76" xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_K5sr8Gt">Dynamic Huffman coding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Vitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_38Vv7h7">ACM Transactions on Mathematical Software</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="158" to="167" />
			<date type="published" when="1989-06">June 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,335.05,619.77,196.86,6.76;10,331.38,628.41,155.54,6.76" xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_rUEY9zj">A technique for high-performance data compression</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4f4Qy7T">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="8" to="19" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.04,637.11,202.79,6.76;10,331.38,645.75,200.54,6.76;10,331.38,654.39,74.06,6.76" xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_TbMHjYz">A Universal Algorithm for Sequential Data Compression</title>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">J</forename><surname>Ziv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lempel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zEMRtWk">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="342" />
			<date type="published" when="1977-05">May 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,333.25,663.09,198.65,6.76;10,331.38,671.73,200.56,6.76;10,331.38,680.37,137.61,6.76" xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_VF4gPDA">Compression of Individual Sequences via Variable-Rate Coding</title>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">J</forename><surname>Ziv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lempel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xgtKmBx">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="536" />
			<date type="published" when="1978-09">September 1978</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
