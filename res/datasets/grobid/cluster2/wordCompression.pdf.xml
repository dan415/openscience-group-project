<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_GdPu2WE">Constructing Word-Based Text Compression Algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,152.17,132.80,91.67,10.80"><forename type="first">R</forename><forename type="middle">Nigel</forename><surname>Horspool</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Victoria</orgName>
								<address>
									<postBox>P.O. Box 3055</postBox>
									<postCode>V8W 3P6</postCode>
									<settlement>Victoria</settlement>
									<region>B.C</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,361.94,132.80,104.11,10.80"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
							<email>gvcormack@waterloo.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Waterloo Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<region>Ont</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_HrqtfhE">Constructing Word-Based Text Compression Algorithms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1D8E75E5BB829A1F6AF77152FFB357B2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-05-07T13:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_924kXAm"><p xml:id="_hs3xuk7">Text compression algorithms are normally defined in terms of a source alphabet Σ of 8-bit ASCII codes. We consider choosing Σ to be an alphabet whose symbols are the words of English or, in general, alternate maximal strings of alphanumeric characters and non-alphanumeric characters. The compression algorithm would be able to take advantage of longer-range correlations between words and thus achieve better compression. The large size of Σ leads to some implementation problems, but these are overcome to construct word-based LZW, word-based Adaptive Huffman, and wordbased Context Modelling compression algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_FvhNy9h">Introduction</head><p xml:id="_Uk6kS4z">Most text compression algorithms perform compression at the character level. If the algorithm is adaptive (as, for example, with any of the Ziv-Lempel methods), the algorithm slowly learns correlations between adjacent pairs of characters, then triples, quadruples and so on. The algorithm rarely has a chance to take advantage of longer range correlations before either the end of input is reached or the tables maintained by the algorithm are filled to capacity. If text compression algorithms were to use larger units than single characters as the basic storage element, they would be able to take advantage of the longer range correlations and, perhaps, achieve better compression performance. Faster compression may also be possible by working with larger units.</p><p xml:id="_ByBZScK">In this paper, we explore the use of words as the basic unit. When the source file is an English-language document, say, we have no difficulty in recognizing a word as consisting of a sequence of consecutive letters. Each word is separated from the next by space and/or punctuation characters. Following the same approach as Bentley et al. <ref type="bibr" coords="1,462.26,590.85,12.74,10.80" target="#b1">[2]</ref>, we generalize slightly by considering a text file to consist of alternating alphanumeric-strings and punctuation-strings, where a word-string is a maximal sequence of alphanumeric characters and a punctuation-string is a maximal sequence of non-alphanumeric characters. We use the generic name word to refer to either an alphanumeric string or a punctuation string. The generalization permits us to decompose all kinds of text files -program source code, input to a word-processor, etc. -into sequences of words.</p><p xml:id="_CQX8VbY">Existing compression algorithms that consider the input as a sequence of words are ad hoc in nature. The scheme described by Bentley et al. <ref type="bibr" coords="2,364.27,76.81,13.99,10.80" target="#b1">[2]</ref> maintains a list of words sorted into least-recently used order. A word is encoded by its position in this dynamically changing list. Words near the front of the list tend to have shorter codes than those near the end and, assuming words in frequent use stay near the front of the list, compression is achieved. The general approach is called Move-To-Front or MTF in <ref type="bibr" coords="2,418.74,132.82,12.74,10.80" target="#b0">[1]</ref>. Generalizations of the scheme that use other heuristics than MTF to manage the list appear in <ref type="bibr" coords="2,462.95,146.83,13.99,10.80" target="#b4">[5]</ref> and <ref type="bibr" coords="2,500.27,146.83,12.74,10.80" target="#b5">[6]</ref>.</p><p xml:id="_v7vyvSg">A less ad hoc approach would be to consider words as forming the symbols of an alphabet. Such an alphabet can, in principle, be used as the basis of any existing compression algorithm. For example, LZW (aka the UNIX compress command) <ref type="bibr" coords="2,434.08,194.84,13.99,10.80" target="#b6">[7]</ref> could work by encoding sequences of words instead of sequences of characters. If particular sequences tend to recur in the source text, compression would be achieved.</p><p xml:id="_thmKB2N">However, we need to overcome a major problem with word-based compression algorithms. The number of distinct words that the compression algorithm has to cope with is, for all practical purposes, unbounded. Thus it makes no sense to implement an algorithm that requires a pre-determined finite alphabet. To use LZW as an example again, we cannot initialize the LZW string table with all sequences of length one, as required in the usual implementations of LZW. Instead, we have to modify the algorithms so that they either pre-determine the set of words used in the source input (an inherently two-pass strategy) or they dynamically expand the source alphabet as each new word occurs. We, of course, advocate single-pass strategies as being more useful for practical applications.</p><p xml:id="_Un3spxM">The following sections of this paper will consider the problem of generalizing a compression algorithm to be word-based, then particular word-based algorithms will be described, and finally some experimental results will be reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" xml:id="_qEYFQbg">Using Word-Based Alphabets</head><p xml:id="_XXBfTPQ">Following the scheme of <ref type="bibr" coords="2,211.99,466.80,12.74,10.80" target="#b1">[2]</ref>, we can decompose textual input into a sequence of words, where alternate words are composed from alphanumeric characters and from non-alphanumeric characters. For example, a line of a Pascal source code file that reads xCoord2 := xCoord2 + delta; would be decomposed into the following elements, where spaces are made visible and the line-feed character at the end of line is shown as a C-style character constant '\n'.</p><p xml:id="_QSMAyMa">It is easy to transform an existing compression algorithm to operate on the alphabet of words if an extra pass over the source data is permitted. An initial pass enters the words (both alphanumeric and non-alphanumeric) into a dictionary. Once the pass is complete, we know all the symbols of the word alphabet and it should now be possible to construct a version of the compression algorithm that uses this new alphabet. Of course, the dictionary (or, more likely, a compressed form of the dictionary) must be transmitted with the output from the compression algorithm. "_ _ _ _" "xCoord2" "_ := _" "xCoord2" "_ + _" "delta" ";\n" punct punct punct punct alpha alpha alpha</p><p xml:id="_6RUZsF8">For the majority of applications, two passes over the source text are undesirable. An adaptive scheme that dynamically expands the dictionary as new words are encountered is preferable. A general mechanism for handling a new word involves the use of an escape code. When the compression algorithm hits a new word, it can output an escape code followed by some representation of the text of the new word. Then it can add the new word to the next available slot in the dictionary and continue as though the word had been present in the dictionary all the time. This general escape mechanism, however, does not extract the maximum amount of redundancy from the compression algorithm. As the following examples show, it is possible to integrate the escape mechanism into the compression algorithm more tightly and do better. For one thing, we should be able to take advantage of the fact the the alphanumeric and non-alphanumeric words strictly alternate. Thus, we should use two word-based source alphabets Σ A , the alphabet of alphanumeric words, and Σ P , the alphabet of punctuation strings. If the symbols from the two alphabets are identified by symbol numbers, the numberings need not be disjoint, as the decoding algorithm should always know by context which source alphabet to expect. A second way in which the escape mechanism can be better integrated is by making either an occurrence of the escape code or the symbol that corresponds to a new word implicit. Such integration may increase the complexity of the implementation somewhat.</p><p xml:id="_ekhEuqs">3 Some Word-Based Algorithms</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" xml:id="_T9nyUrz">Word-Based Adaptive Huffman Coding</head><p xml:id="_hsdH3RE">Adaptive Huffman coding is the basis of the UNIX compact program. The compression program maintains a count of how many times each symbol has occurred so far in the source text. To encode the next symbol, the symbol counts are used as estimates of the relative probabilities of the symbols and a table of Huffman codes based on these frequencies is constructed. The Huffman code in the table is used to encode the next symbol. (A minor detail that needs to be taken into account is that symbols in the alphabet that have not yet occurred in the source text must be assigned a non-zero probability estimate.) The decoding algorithm can re-create the same set of symbol frequencies from its de-compressed text and use the table to re-construct the same table of Huffman codes. Thus it can uniquely decode one symbol, update the frequency count of that symbol, update its table of Huffman codes and then decode the next symbol, and so on.</p><p xml:id="_JdWjJPB">Algorithms exist for efficiently updating the Huffman codes when small incremental changes to the probability estimates are made (as is the case here) <ref type="bibr" coords="3,423.59,578.85,12.74,10.80" target="#b2">[3]</ref>, <ref type="bibr" coords="3,443.58,578.85,12.74,10.80" target="#b3">[4]</ref>. In spite of the widespread use of these algorithms in implementations, Adaptive Huffman coding is not renowned for its speed (nor for its compression performance).</p><p xml:id="_3pA3YdP">The overall structure of a word-based Adaptive Huffman algorithm may take the form shown in Figure <ref type="figure" coords="3,197.33,640.87,4.50,10.80">1</ref>. The algorithm uses two tables of frequencies, AFreq and PFreq, and two tables of Huffman codes, AHuffman and PHuffman, for the two different alphabets Σ A and Σ P . Details concerning the initialization and the proper termination of the algorithm at the end of input are omitted for brevity.</p><p xml:id="_mpfdP3v">In what form should the text of new words be transmitted? New words occur in short input files with a relatively high frequency and efficient encoding of them is highly desirable. To be consistent with the top-level word-based compression strategy, we propose that adaptive Huffman coding be used for the individual characters of the words. Since an algorithm for updating the Huffman codes must already be available for the two source alphabets Σ A and Σ P , it would not impose a burden on the implementer to use it for four different source alphabets. (Four because the decoder knows whether a new word is going to be composed from alphanumeric characters or non-alphanumeric characters, and thus the two alphabets may be encoded separately.)</p><p xml:id="_gDQDBM6">The compression performance of word-based Adaptive Huffman coding is excellent, as the experimental data at the end of this paper shows. The execution speed is not so good however. As the Σ W and Σ P alphabets grow in size, the time required to update the tables of Huffman codes slowly and inexorably increases. The average time complexity of the update algorithms appears to be O(n), where n is the size of the alphabet; the worstcase time complexity is O(n log n). If word-based Adaptive Huffman coding were to become practical , some technique would be needed to prune infrequently used symbols from the Σ W and Σ P alphabets. (Certainly we should delete words whose Huffman codes become so long that fewer bits would be needed to re-transmit the word as a new word.) Several pruning strategies based on an analogy with page replacement algorithms in virtual memory systems are suggested in <ref type="bibr" coords="4,275.32,615.28,12.74,10.80" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" xml:id="_V5MW569">Word-Based LZW</head><p xml:id="_Cg585av">The LZW (Lempel-Ziv-Welch) compression algorithm <ref type="bibr" coords="4,356.65,671.26,13.99,10.80" target="#b6">[7]</ref> is the basis of the UNIX compress program and of the compression strategies implemented in many commercial prod- ucts, both hardware and software. Its main virtue is speed, while simultaneously achieving good, but not spectacular, compression performance.</p><p xml:id="_NxV9qA4">LZW is easy to explain. The algorithm maintains a string table that associates a unique integer with each string. To encode the next segment of source text, the compression algorithm reads the longest possible sequence of characters that comprises a string in the table. It outputs the number associated with the string, using a simple binary numbering system. If the string that was read was ω and Κ is the following character in the source text, the new string ωΚ is added to the string table and the next unused number is associated with the string. The compression algorithm then continues, reading input characters starting with Κ looking for the longest string that is contained in the table.</p><p xml:id="_bEHNcPE">The string table is initialized with all strings of length one. This guarantees that at least one character can be read from the input and matched against a string in the table. The strategy for adding new strings guarantees that if a string ω is in the table, then all prefixes of ω must also be present in the table. This property simplifies the task of matching a maximal length sequence of input characters against the strings in the table, and also permits the table to be implemented by an efficient data structure (such as a trie). As stated above, the encoding of string numbers is simple (and is another reason for the speed of LZW implementations). If, at some moment, the table holds N strings (and these will be numbered 0 through N-1), a binary number comprised of log(N+1) bits is used to encode the next string number to be output. 1   A word-based LZW implementation cannot initialize the string table with all strings of length one. This is because the Σ W and Σ P alphabets may be very large and the symbols are not known in advance (unless a pre-pass over the text source is performed). Thus, we will again advocate the use of an escape mechanism.</p><p xml:id="_ZvcEkuJ">The word-based LZW algorithm builds up two kinds of strings of symbols. All strings will consist of alternate symbols from Σ W and Σ P , but we can segregate strings whose initial symbol is an element of Σ W in a separate table from those strings whose initial symbol is an element of Σ P . Numbering of strings in the two tables need not be disjoint because the decoding algorithm can always deduce whether the next string it receives should begin with a Σ W or a Σ P symbol.</p><p xml:id="_UaSYB2V">The structure of the main body of the algorithm has the form shown in Figure <ref type="figure" coords="5,501.62,512.81,4.50,10.80">2</ref>. We use λ to denote the empty string and &lt;&lt;α,b&gt;&gt; to denote a string constructed by appending the symbol b to the string α. The two tables of strings are named ATable and PTable. A string numbering scheme that reserves a code (presumably 0) for Escape must be used. The empty string λ may be implemented by the same number because an encoding of λ is never output. Again, we must decide how the characters of a new word should be encoded. As before, the compression performance is compromised for small files unless a reasonably efficient coding scheme is used for the characters. Again, it is possible to apply the same 1. The original LZW algorithm, as described in <ref type="bibr" coords="5,282.43,672.34,10.62,9.00" target="#b6">[7]</ref>, proposes a maximum table size of 4096 strings and that 12-bit numbers be used regardless of table occupancy. that the previous word to have been encoded is W i , we can use the observed frequencies to estimate the conditional probabilities P(W j |W i ) for the next word. These probabilities may passed to an arithmetic coding subroutine for transmitting the next word.</p><p xml:id="_gpMHTQ6">If the compression algorithm is to be targeted to compression of natural language, we should treat the two alphabets differently. There should be strong correlations between successive alphanumeric words but, presumably, weak correlations between successive punctuation strings. Also, we can expect correlations between the punctuation and the alphanumeric words -for example, a punctuation string that contains a period would signify the end of a sentence and would therefore predict that the next alphanumeric word would begin with an upper-case letter (and would be quite likely to be an article). The algorithm structure shown in Figure <ref type="figure" coords="7,265.54,208.84,6.00,10.80" target="#fig_0">3</ref> assumes only correlations between successive symbols from Σ A . In the algorithm, the variable PrevW represents the previous alphanumeric word to have been transmitted. The arithmetic coding algorithm uses the frequency count Freq[PrevW,X] to estimate the relative probability for each word X in Σ Α .</p><p xml:id="_bjGbVNt">The natural choice for encoding the text of punctuation strings and words is arithmetic coding applied to individual characters. The probability estimates needed as input to the algorithm can be based on the frequency of occurrence of each character. Compression performance is improved if probabilities obtained from the first-order model are blended with probabilities from the 0th-order model when the number of observations is too low to make reliable first-order predictions. Blending the two models is a scheme used in PPMC <ref type="bibr" coords="7,90.00,354.83,12.74,10.80" target="#b0">[1]</ref>, for example. Blended probabilities are used to obtain the results reported in Table <ref type="table" coords="7,504.40,354.83,4.50,10.80">1</ref>.</p><p xml:id="_gyBGaCT">Another way to improve the significance of the statistics and also a way of reducing the volume of data that must be retained is to merge statistics for similar words. An obvious way to group words into a small number of classes is by their parts of speech in the English language. This gives rise to our final word-based algorithm. It is described in the following section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4" xml:id="_MCsVQvt">First-Order Context Modelling by Part-of-Speech</head><p xml:id="_6cJS26R">If we assume that the source text contains natural language, the sequence of words in the source should obey the rules of grammar for that language. For example, one possible form for an English language sentence is: Subject Verb Object where the Subject and the Object may be constructed as an Article followed by a Noun. The rules of grammar strongly influence the probabilities of certain words appearing in certain contexts. For example, after the word 'the' (an article), we would not expect to find another occurrence of an article or a verb, but we could find a noun or an adjective there. (However, the existence of a music group named 'The The' reminds us that we should not assume a zero probability for any combination of words, no matter how strange it seems.)</p><p xml:id="_hqERTzx">We therefore propose that a state-based model <ref type="bibr" coords="8,347.87,240.84,13.99,10.80" target="#b0">[1]</ref> be used for modelling the source text and be used for generating the prediction probabilities used by an arithmetic coding subroutine. For simplicity, we assume that there are just five parts of speech named Article, Noun, Adjective, Verb and Other. (We include pronouns such as 'she' and 'him' in the Noun category.) This yields a state diagram like the following: Each state provides a set of probability estimates for the symbols in Σ A . If the current state is Article and the next symbol is the word 'funny', we would use the probability estimates associated with the Article state to encode the word 'funny'. Then we would make a transition to the Adjective state, since that is the part of speecf for the word 'funny'.</p><p xml:id="_5BnytSz">Implementation of this state-based model requires that we know or can determine the parts of speech for all the symbols in Σ A . This requires that the compression algorithm be supplied with an initial vocabulary that specifies the appropriate part of speech for words in the vocabulary. (Presumably some heuristics, such as deciding that a word ending with the letters 'ly' is probably an adverb could also be useful.) But it is unreasonable to require every word that might appear in any document to be included in the initial vocabulary. Therefore, we need a mechanism for inferring an appropriate part of speech for a new word after an occurrence in the text.</p><p xml:id="_FYkqkjE">The first time a word occurs, and the word is not contained in the vocabulary, we should assign that word the part of speech that is most likely to follow the part of speech for the preceding word. Subsequently, we keep statistics on how well that word fits into its assigned part of speech. For example, if the new word is W, we keep track of how efficiently those words that immediately follow subsequent occurrences of W would be encoded if W were assigned to the Noun class, to the Adjective class, and so on. If better compression would have been achieved with W in a different class, W is dynamically re-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_gsyUW2T">Article Adjective Noun</head><p xml:id="_D5zSgCQ">Verb Other gle entry and the end-of-sentence test could predict which form to use. The words 'A' and 'An' could be segregated into separate article classes and thus predict words that start with vowels or consonants. Word suffixes, such as 'ly', could be used to make better guesses as to a new word's part of speech, etc. Much further experimentation is required. For now, we recommend use of a simple and fast word-based algorithm for text file compression. And of the possibilities considered, word-based LZW would seem to be the closest fit.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,117.13,458.13,377.74,10.80;7,120.60,481.38,96.00,7.86;7,120.60,493.38,36.00,7.86;7,142.20,505.38,312.00,7.86;7,142.20,517.38,192.00,7.86;7,142.20,529.38,30.00,7.86;7,178.20,523.55,21.66,15.64;7,199.86,528.97,6.93,12.51;7,212.79,529.38,24.00,7.86;7,163.80,541.38,324.00,7.86;7,163.80,547.55,7.10,15.64;7,170.90,552.97,6.93,12.51;7,183.84,553.38,12.00,7.86;7,201.84,547.55,7.10,15.64;7,208.94,552.97,6.93,12.51;7,221.87,547.55,9.22,15.64;7,237.09,553.38,30.00,7.86;7,163.80,565.38,120.00,7.86;7,163.80,577.38,270.00,7.86;7,142.20,589.38,24.00,7.86;7,163.80,601.38,186.00,7.86;7,163.80,613.38,222.00,7.86;7,142.20,625.38,30.00,7.86;7,142.20,637.38,72.00,7.86;7,120.60,649.38,114.00,7.86"><head>Figure 3 Arithmetic</head><label>3</label><figDesc xml:id="_NGnBhHP">Figure 3 Arithmetic Coding with Word-Based First-Order Context Model PrevW := Escape; repeat read next punctuation word AP and output text of AP; read next alphanumeric word, AW. if AW ∉ Σ Α then output Escape by arithmetic coding; output text of AW; Σ Α := Σ Α ∪ {AW}; Freq[PrevW,AW] := 1; Freq[PrevW,Escape] := Freq[PrevW,Escape] + 1; else output AW by arithmetic coding; Freq[PrevW,AW] := Freq[PrevW,AW] + 1; endif PrevW := AW; until end of input;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,120.60,64.13,363.60,259.11"><head>Figure 1 Word-Based Adaptive Huffman Algorithm</head><label></label><figDesc xml:id="_ggraRny"></figDesc><table coords="4,120.60,87.38,363.60,235.86"><row><cell>repeat</cell><cell></cell></row><row><cell cols="2">read one alphanumeric word, AW; if AW ∉ Σ A then</cell></row><row><cell cols="2">output AHuffman[Escape];</cell></row><row><cell cols="2">output text of AW; Σ A := Σ A ∪ {AW};</cell></row><row><cell cols="2">AFreq[AW] := 1;</cell></row><row><cell cols="2">AFreq[Escape] := AFreq[Escape] + 1;</cell></row><row><cell>else</cell><cell></cell></row><row><cell cols="2">output AHuffman[AW];</cell></row><row><cell cols="2">AFreq[AW] := AFreq[AW] + 1;</cell></row><row><cell>endif</cell><cell></cell></row><row><cell cols="2">AHuffman := recomputed table of Huffman codes constructed</cell></row><row><cell cols="2">from the frequency table, AFreq;</cell></row><row><cell cols="2">read one non-alphanumeric word, PW; if PW ∉ Σ P then</cell></row><row><cell>...</cell><cell></cell></row><row><cell>...</cell><cell>(* continuing similarly to the above *)</cell></row><row><cell>...</cell><cell></cell></row><row><cell cols="2">until end of input is reached;</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_YBd6mqs">Acknowledgements</head><p xml:id="_BGqQjcD">We are grateful to Kenny Wong for programming the word-based Adaptive Huffman coding algorithm whose results are reported here. We also thank the Natural Science and Engineering Research Council of Canada for their financial support.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_UZ3F2Ha"><p xml:id="_hGYqbCU">basis algorithm for the lower-level encoding as at the higher word-based coding level. There is a caveat however. LZW encodes sequences of characters. We must treat the end of each new word as being equivalent to the end-of-file, otherwise the encoding of parts of two consecutive new words would have to be combined. (We expect correlations between characters at the end of one new word and characters at the beginning of the next new word to be weak.)</p><p xml:id="_QyK2Kp8">Compression performance is again excellent. Speed degradation as the two wordstring tables fill up is hardly noticeable when they are implemented as very large hash tables. However, a pruning strategy to prevent the tables from reaching too high an occupancy to permit fast look-ups (greater than 80% occupancy, say) is again desirable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3" xml:id="_TrdV94z">Word-Based First-Order Context Compression</head><p xml:id="_4UGXzcw">The Adaptive Huffman coding algorithm, described above, uses a zero-order Markov model to predict properties of the source text. Much better compression may be achieved if a higher-order Markov model is used. Higher-order models form the basis of the PPM (prediction by partial match) family of compression algorithms <ref type="bibr" coords="6,395.96,603.25,12.74,10.80" target="#b0">[1]</ref>, for example.</p><p xml:id="_AtU88cP">When the symbols are English words, we can expect correlations between successive symbols. For example, the word 'the' would have a high probability of being followed by a word that is a noun or adjective but a low probability of being followed by another article or a verb. An algorithm based on a first-order model could maintain statistics on the observed frequencies of word pairs in the text compressed so far. Then given assigned to that other class. The decoding algorithm performs the same calculations and can therefore perform the same re-assignments. The overall structure of the compression algorithm is therefore as shown in Figure <ref type="figure" coords="9,288.04,364.83,4.50,10.80">4</ref>. For simplicity, punctuation strings are ignored in the presentation of the algorithm -they should be encoded by some other means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" xml:id="_gKZPz3f">Experimental Results and Discussion</head><p xml:id="_gRAMET9">The four word-based algorithms described in this paper have been implemented and tested on several text files, using the UNIX compress program as a benchmark. The compression results are summarized in Table <ref type="table" coords="9,245.46,464.83,5.53,10.80">1</ref>.The pair of numbers shown for the state-based context compression reflects the fact that the algorithm normally requires initialization with a vocabulary giving the parts of speech for words. In our experiments, the initial vocabulary contained all the words used in the first test document (the csh manual description). The upper number in each pair shows compression performance when that vocabulary is used; the lower number shows performance when no vocabulary at all is used. I.e., the algorithm simply assigns words to one of five classes as it seems to find appropriate.</p><p xml:id="_tYpfmbZ">The overall result is that performance is consistently better than our benchmark, the UNIX compress program, no matter which word-based method is used. Words apparently recur sufficiently often in the test files that any scheme for compressing repeated references to words will perform well. The compression scheme using a state model based on parts of speech in the English language has probably the most potential for improvement. The current crude scheme could be improved by monitoring the punctuation to check for a period (or other characters that terminate a sentence) and be used to reset the model state to the start of a new sentence. At present, two strings with different capitalization (e.g. 'the' and 'The') are treated as two different words, but they could be combined into a sin- </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,115.20,473.80,386.60,10.80" xml:id="b0">
	<monogr>
		<title level="m" type="main" xml:id="_ZPGA4ft">Text Compression</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Cleary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,115.20,491.80,395.65,10.80;10,115.20,505.81,308.32,10.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_gprVtQT">A Locally Adaptive Data Compression Scheme</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Sleator</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">K</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EDqNTWk">CACM</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="320" to="330" />
			<date type="published" when="1986-04">April 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,115.20,523.81,403.93,10.80;10,115.20,537.81,251.63,10.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_Hzx3uuk">Algorithms for Adaptive Huffman Codes</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">N</forename><surname>Horspool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_eyXMJVR">Inf. Processing Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="159" to="166" />
			<date type="published" when="1983-03">March 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,115.20,555.81,406.86,10.80;10,115.20,569.82,151.87,10.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_mYS2uuR">Variations on a Theme by Huffman</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">G</forename><surname>Gallager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WYcGRza">IEEE Trans. on Inf. Theory, IT</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="668" to="674" />
			<date type="published" when="1978-11">Nov. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,115.20,587.82,394.18,10.80;10,115.20,601.82,389.82,10.80;10,115.20,615.82,91.00,10.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_VPhSBtF">A General-Purpose Data Compression Technique with Practical Applications</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">N</forename><surname>Horspool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_eGXSuZ8">Proc. of CIPS Session &apos;84</title>
				<meeting>of CIPS Session &apos;84<address><addrLine>Calgary, Alberta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="138" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,115.20,633.82,377.52,10.80;10,115.20,647.83,383.96,10.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_sX2jxmx">Technical Correspondence on &apos;A Locally Adaptive Data Compression Scheme&apos;</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">N</forename><surname>Horspool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YrjrReq">CACM</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="792" to="794" />
			<date type="published" when="1987-09">Sept. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,115.20,665.83,396.20,10.80;10,115.20,679.83,160.64,10.80;10,179.35,150.84,253.30,10.80;10,187.00,176.84,40.00,10.80;10,196.67,190.84,20.66,10.80;10,188.17,204.85,37.67,10.80;10,298.85,172.84,158.32,10.80;10,247.81,194.84,29.99,10.80;10,240.14,208.84,45.32,10.80;10,296.75,194.84,47.33,10.80;10,298.86,208.84,43.10,10.80" xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_RnfMf3Q">Table 1 Comparative Compression Performance Original Size in bytes Relative Size After Compression UNIX compress WB-Adpt Huffman</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7rnTHRe">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="8" to="19" />
			<date type="published" when="1984-06">June 1984</date>
		</imprint>
	</monogr>
	<note>A Technique for High-Performance Data Compression</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
