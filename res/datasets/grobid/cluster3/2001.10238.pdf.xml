<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_DSnEBw7">OUS FACTORS OF VARIATIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-01-28">28 Jan 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,116.47,137.54,84.40,8.96;1,200.87,136.03,1.55,6.12"><forename type="first">Antoine</forename><surname>Plumerault</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire Analyse Sémantique Texte et Image</orgName>
								<orgName type="institution" key="instit1">CEA</orgName>
								<orgName type="institution" key="instit2">LIST</orgName>
								<address>
									<postCode>F-91191</postCode>
									<settlement>Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Université Paris-Saclay, CentraleSupélec</orgName>
								<address>
									<postCode>91190</postCode>
									<settlement>Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,214.09,137.54,72.85,8.96;1,286.93,136.03,1.36,6.12"><forename type="first">Hervé</forename><surname>Le Borgne</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire Analyse Sémantique Texte et Image</orgName>
								<orgName type="institution" key="instit1">CEA</orgName>
								<orgName type="institution" key="instit2">LIST</orgName>
								<address>
									<postCode>F-91191</postCode>
									<settlement>Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,296.49,137.54,63.93,8.96;1,360.42,136.03,1.83,6.12"><forename type="first">Céline</forename><surname>Hudelot</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Université Paris-Saclay, CentraleSupélec</orgName>
								<address>
									<postCode>91190</postCode>
									<settlement>Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_UKnUVyT">OUS FACTORS OF VARIATIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-01-28">28 Jan 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">26CD1B63CE08003843D919E9498742AC</idno>
					<idno type="arXiv">arXiv:2001.10238v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-05-07T13:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_7pH7cCa"><p xml:id="_VXkhCgJ">Recent deep generative models are able to provide photo-realistic images as well as visual or textual content embeddings useful to address various tasks of computer vision and natural language processing. Their usefulness is nevertheless often limited by the lack of control over the generative process or the poor understanding of the learned representation. To overcome these major issues, very recent work has shown the interest of studying the semantics of the latent space of generative models. In this paper, we propose to advance on the interpretability of the latent space of generative models by introducing a new method to find meaningful directions in the latent space of any generative model along which we can move to control precisely specific properties of the generated image like the position or scale of the object in the image. Our method does not require human annotations and is particularly well suited for the search of directions encoding simple transformations of the generated image, such as translation, zoom or color variations. We demonstrate the effectiveness of our method qualitatively and quantitatively, both for GANs and variational auto-encoders.</p><p xml:id="_VQ9Ug3w">Figure <ref type="figure" coords="1,136.74,618.48,3.91,8.64">1</ref>: Images generated with our approach and a BigGAN model <ref type="bibr" coords="1,386.77,618.48,76.58,8.64" target="#b5">(Brock et al., 2018)</ref>, showing that the position of the object can be controlled within the image.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_fUh43h7">INTRODUCTION</head><p xml:id="_kwy754h">With the success of recent generative models to produce high-resolution photo-realistic images <ref type="bibr" coords="1,474.73,690.50,29.27,8.64;1,108.00,701.46,45.24,8.64" target="#b15">(Karras et al., 2018;</ref><ref type="bibr" coords="1,155.25,701.46,71.12,8.64" target="#b5">Brock et al., 2018;</ref><ref type="bibr" coords="1,228.38,701.46,72.94,8.64" target="#b27">Razavi et al., 2019)</ref>, an increasing number of applications are emerging, such as image in-painting, dataset-synthesis, and deep-fakes. However, the use of generative models is often limited by the lack of control over the generated images. More control could be used to improve existing approaches which aim at generating new training examples <ref type="bibr" coords="2,410.37,85.34,81.69,8.64" target="#b4">(Bowles et al., 2018)</ref> by allowing the user to choose more specific properties of the generated images.</p><p xml:id="_vHGQJF2">First attempts in this direction showed that one can modify an attribute of a generated image by adding a learned vector on its latent code <ref type="bibr" coords="2,268.52,124.19,83.57,8.64" target="#b26">(Radford et al., 2015)</ref> or by combining the latent code of two images <ref type="bibr" coords="2,138.94,135.15,78.74,8.64" target="#b15">(Karras et al., 2018)</ref>. Moreover, the study of the latent space of generative models provides insights about its structure which is of particular interest as generative models are also powerful tools to learn unsupervised data representations. For example, <ref type="bibr" coords="2,365.49,157.07,86.13,8.64" target="#b26">Radford et al. (2015)</ref> observed on auto-encoders trained on datasets with labels for some factors of variations, that their latent spaces exhibit a vector space structure where some directions encode the said factors of variations.</p><p xml:id="_NDhn24v">We suppose that images result from underlying factors of variation such as the presence of objects, their relative positions or the lighting of the scene. We distinguish two categories of factors of variations. Modal factors of variation are discrete values that correspond to isolated clusters in the data distribution, such as the category of the generated object. On the other hand, the size of an object or its position are described by Continuous factors of variations, expressed in a range of possible values. As humans, we naturally describe images by using factors of variations suggesting that they are an efficient representation of natural images. For example, to describe a scene, one likely enumerates the objects seen, their relative positions and relations and their characteristics <ref type="bibr" coords="2,459.25,272.64,45.99,8.64;2,108.00,283.60,21.10,8.64" target="#b2">(Berg et al., 2012)</ref>. This way of characterizing images is also described in <ref type="bibr" coords="2,351.44,283.60,79.61,8.64" target="#b16">Krishna et al. (2016)</ref>. Thus, explaining the latent space of generative models through the lens of factors of variation is promising. However, the control over the image generation is often limited to discrete factors and requires both labels and an encoder model. Moreover, for continuous factors of variations described by a real parameter t, previous works do not provide a way to get precise control over t.</p><p xml:id="_W8uR7B5">In this paper, we propose a method to find meaningful directions in the latent space of generative models that can be used to control precisely specific continuous factors of variations while the literature has mainly tackled semantic labeled attributes like gender, emotion or object category <ref type="bibr" coords="2,107.67,377.24,85.18,8.64" target="#b26">(Radford et al., 2015;</ref><ref type="bibr" coords="2,195.33,377.24,74.01,8.64">Odena et al., 2016)</ref>. We test our method on image generative models for three factors of variation of an object in an image: vertical position, horizontal position and scale. Our method has the advantage of not requiring a labeled dataset nor a model with an encoder. It could be adapted to other factors of variations such as rotations, change of brightness, contrast, color or more sophisticated transformations like local deformations. However, we focused on the position and scale as these are quantities that can be evaluated, allowing us to measure quantitatively the effectiveness of our method. We demonstrate both qualitatively and quantitatively that such directions can be used to control precisely the generative process and show that our method can reveal interesting insights about the structure of the latent space. Our main contributions are:</p><p xml:id="_3uwu6g4">• We propose a method to find interpretable directions in the latent space of generative models, corresponding to parametrizable continuous factors of variations of the generated image. • We show that properties of generated images can be controlled precisely by sampling latent representations along linear directions. • We propose a novel reconstruction loss for inverting generative models with gradient descent.</p><p xml:id="_Sczy2MD">• We give insights of why inverting generative models with optimization can be difficult by reasoning about the geometry of the natural image manifold. • We study the impacts of disentanglement on the ability to control the generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" xml:id="_jTf2Ych">LATENT SPACE DIRECTIONS OF A FACTOR OF VARIATION</head><p xml:id="_8p9Nvph">We argue that it is easier to modify a property of an image than to obtain a label describing that property. For example, it is easier to translate an image than to determine the position of an object within said image. Hence, if we can determine the latent code of a transformed image, we can compute its difference with the latent code of the original image to find the direction in the latent space which corresponds to this specific transformation as in <ref type="bibr" coords="2,352.32,673.56,81.90,8.64" target="#b26">Radford et al. (2015)</ref>.</p><p xml:id="_M7u86yn">Let us consider a generative model G : z ∈ Z → I, with Z its latent space of dimension d and I the space of images, and a transformations T t : I → I characterized by a continuous parameter t. For example if T is a rotation, then t could be the angle, and if T is a translation, then t could be a component of the vector of the translation in an arbitrary frame of reference. Let z 0 be a vector of Z and I = G(z 0 ) a generated image. Given a transformation T T , we aim at finding z T such that G(z T ) ≈ T T (I) to then use the difference between z 0 and z T in order to estimate the direction encoding the factor of variation described by T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1" xml:id="_sgUfV6R">LATENT SPACE TRAJECTORIES OF AN IMAGE TRANSFORMATION</head><p xml:id="_RbKJCyB">Given an image I ∈ I, we want to determine its latent code. When no encoder is available we can search an approximate latent code ẑ that minimizes a reconstruction error L between I and Î = G( ẑ) ( Î can be seen as the projection of I on G(Z)) i.e. </p><formula xml:id="formula_0" coords="3,257.67,201.52,246.33,16.66">ẑ = arg min z∈Z L(I, G(z)) (1)</formula><formula xml:id="formula_1" coords="3,108.25,304.35,395.75,44.94">L(I, G(z)) (2) 2.1.1 CHOICE OF THE RECONSTRUCTION ERROR L</formula><p xml:id="_skmjSQf">One of the important choice regarding this optimization problem is that of L. In the literature, the most commonly used are the pixel-wise Mean Squared Error (MSE) and the pixel-wise cross-entropy as in <ref type="bibr" coords="3,128.12,382.75,97.36,8.64" target="#b19">Lipton &amp; Tripathi (2017)</ref> and <ref type="bibr" coords="3,243.97,382.75,104.21,8.64">Creswell &amp; Bharath (2016)</ref>. However in practice, pixel-wise losses are known to produce blurry images. To address this issue, other works have proposed alternative reconstruction errors. However, they are based on an alternative neural network <ref type="bibr" coords="3,438.43,404.67,65.56,8.64;3,108.00,415.62,76.64,8.64" target="#b3">(Boesen Lindbo Larsen et al., 2015;</ref><ref type="bibr" coords="3,187.13,415.62,82.74,8.64" target="#b14">Johnson et al., 2016)</ref> making them computationally expensive.</p><p xml:id="_QdSb2H7">The explanation usually given for the poor performance of pixel-wise mean square error is that it favors the solution which is the expected value of all the possibilities <ref type="bibr" coords="3,353.14,443.52,85.06,8.64" target="#b22">(Mathieu et al., 2015)</ref> <ref type="foot" coords="3,438.19,441.85,3.49,6.05" target="#foot_1">2</ref> . We propose to go deeper into this explanation by studying the effect of the MSE on images in the frequency domain.</p><p xml:id="_rQd6jAE">In particular, our hypothesis is that due to its limited capacity and the low dimension of its latent space, the generator can not produce arbitrary texture patterns as the manifold of textures is very high dimensional. This uncertainty over texture configurations explains why textures are reconstructed as uniform regions when using pixel-wise errors. In Appendix A, by expressing the MSE in the Fourier domain and assuming that the phase of high frequencies cannot be encoded in the latent space, we show that the contribution of high frequencies in such a loss is proportional to their square magnitude pushing the optimization to solutions with less high frequencies, that is to say more blurry. In order to get sharper results we therefore propose to reduce the weight of high frequencies into the penalization of errors with the following loss:</p><formula xml:id="formula_2" coords="3,196.19,570.88,307.81,11.72">L(I 1 , I 2 ) = ||F{I 1 − I 2 }F{σ}|| 2 = ||(I 1 − I 2 ) * σ|| 2 (3)</formula><p xml:id="_dXC9zpn">where F is the Fourier transform, * is the convolution operator and σ is a Gaussian kernel. With a reduced importance given to the high frequencies to determine ẑ when one uses this loss in equation 2, it allows to benefit from a larger range of possibilities for G(z), including images with more details (i.e with more high frequencies) and appropriate texture to get more realistic generated images. A qualitative comparison to some reconstruction errors and choices of σ can be found in Appendix C. We also report a quantitative comparison to other losses, based on the Learned Perceptual Image Patch Similarity (LPIPS), proposed by <ref type="bibr" coords="3,263.93,659.19,74.70,8.64" target="#b30">Zhang et al. (2018)</ref>.</p><p xml:id="_cYDMVHw">Algorithm 1: Create a dataset of trajectories in the latent space which corresponds to a transformation T in the pixel space. The transformation is parametrized by a parameter δt which controls a degree of transformation. We typically use N = 10 with (δt n ) (0≤n≤N ) distributed regularly on the interval [0, T ]. Note that z 0 and δt n are retained in D at each step to train the model of Section 2.2. Input: number of trajectories S, generator G, transformation function T , trajectories length N , threshold Θ.</p><formula xml:id="formula_3" coords="4,108.00,157.61,178.33,142.46">Result: dataset of trajectories D D ← {} ; for i ∈ 1, S do z 0 ∼ N (0, I) ; I 0 ← G(z 0 ) ; z δt ← z 0 ; for n ∈ [1, N ] do z δt ← arg min z L(G(z), T δtn (I 0 )) ; if L(G(z), T δtn (I 0 )) &lt; Θ then D ← D ∪ {(z 0 , z δt , δt n )} ; end end end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2" xml:id="_kcN8AZ9">RECURSIVE ESTIMATION OF THE TRAJECTORY</head><p xml:id="_shJU98J">Using equation 2, our problem of finding z T such that G(z T ) ≈ T T (I), given transformation T T , can be solve through the following optimization problem:</p><formula xml:id="formula_4" coords="4,235.13,387.87,268.87,18.41">z T = arg min z∈Z,||z||≤ √ d L(G(z), T T (I))<label>(4)</label></formula><p xml:id="_XehuPfP">In practice, this problem is difficult and an "unlucky" initialization can lead to a very slow convergence. <ref type="bibr" coords="4,108.00,430.83,69.44,8.64" target="#b16">Zhu et al. (2016)</ref> proposed to use an auxiliary network to estimate z T and use it as initialization.</p><p xml:id="_n4jNfF8">Training a specific network to initialize this problem is nevertheless costly. One can easily observe that a linear combination of natural images is usually not a natural image itself, this fact highlights the highly curved nature of the manifold of natural images in pixel space. In practice, the trajectories corresponding to most transforms in pixel space may imply small gradients of the loss that slowdown the convergence of problem of Eq. ( <ref type="formula" coords="4,254.50,485.62,4.15,8.64">2</ref>) (see Appendix D).</p><p xml:id="_QwqAfWN">To address this, we guide the optimization on the manifold by decomposing the transformation T T into smaller transformations [T δt0 , . . . , T δt N ] such that T δt0=0 = Id and δt N = T and solve sequentially:</p><formula xml:id="formula_5" coords="4,155.89,546.02,348.11,16.55">z n = arg min z∈Z L (G (z; z init = z n−1 ) , T δtn (G (z 0 ))) for n = 1, . . . , N<label>(5)</label></formula><p xml:id="_WqaWHAf">each time initializing z with the result of the previous optimization. In comparison to <ref type="bibr" coords="4,466.00,574.93,39.74,8.64;4,107.67,585.89,24.83,8.64" target="#b16">Zhu et al. (2016)</ref>, our approach does not require extra training and can thus be used directly without training a new model. We compare qualitatively our method to a naive optimization in Appendix C.</p><p xml:id="_X2FpwKP">A transformation on an image usually leads to undefined regions in the new image (for instance, for a translation to the right, the left hand side is undefined). Hence, we ignore the value of the undefined regions of the image to compute L. Another difficulty is that often the generative model cannot produce arbitrary images. For example a generative model trained on a given dataset is not expected to be able to produce images where the object shape position is outside of the distribution of object shape positions in the dataset. This is an issue when applying our method because as we generate images from a random start point, we have no guarantee that the transformed images is still on the data manifold. To reduce the impact of such outliers, we discard latent codes that give a reconstruction error above a threshold in the generated trajectories. In practice, we remove one tenth of the latent codes which leads to the worst reconstruction errors. It finally results into Algorithm 1 to generate trajectories in the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2" xml:id="_kaD5Gzm">ENCODING MODEL OF THE FACTOR OF VARIATION IN THE LATENT SPACE.</head><p xml:id="_Egz2amb">After generating trajectories with Algorithm 1, we need to define a model which describes how factors of variations are encoded in the latent space. We make the core hypothesis that the parameter t of a specific factor of variations can be predicted from the coordinate of the latent code along an axis u, thus we pose a model f : Z → R of the form t = f (z) = g( z, u ), with g : R → R and •, • the euclidean scalar product in R d .</p><p xml:id="_SQ25Bu3">When g is a monotonic differentiable function, we can without loss of generality, suppose that u = 1 and that g is an increasing function. Under these conditions, the distribution of t = g( z, u ) when z ∼ N (0, I) is given by ϕ : R → R + :</p><formula xml:id="formula_6" coords="5,239.58,206.05,264.42,22.31">ϕ(t) = N (g −1 (t); 0, 1) d dt g −1 (t)<label>(6)</label></formula><p xml:id="_7SuMEpS">For example, consider the dSprite dataset <ref type="bibr" coords="5,281.93,234.97,89.86,8.64" target="#b23">(Matthey et al., 2017)</ref> and the factor corresponding to the horizontal position of an object x in an image, we have x that follows a uniform distribution U([−0.5, 0.5]) in the dataset while the projection of z onto an axis u follows a normal distribution N (0, 1). Thus, it is natural to adopt g : R → [−0.5, 0.5] and for x = g( z, u ):</p><formula xml:id="formula_7" coords="5,177.74,285.22,326.27,101.91">ϕ(x) = U (x, [−0.5, 0.5]) = N (g −1 (x); 0, 1) d dx g −1 (x) ⇐⇒ 1 = N ( z, u ; 0, 1) d dx g −1 (g( z, u )) ⇐⇒ 1 d dx g −1 (g ( z, u )) = d dx g ( z, u ) = N ( z, u ; 0, 1) ⇐⇒ g( z, u ) = 1 2 erf z, u √ 2 (7)</formula><p xml:id="_ar7xztW">However, in general, the distribution of the parameter t is not known. One can adopt a more general parametrized model g θ of the form:</p><formula xml:id="formula_8" coords="5,217.82,424.11,286.18,9.99">t = f (θ,u) (z) = g θ ( u, z ) with ||u|| = 1 (8)</formula><p xml:id="_p2etsPJ">with g θ : R → R and (θ, u) trainable parameters of the model. We typically used piece-wise linear functions for g θ .</p><p xml:id="_dg8R52m">However, this model cannot be trained directly as we do not have access to t (in the case of horizontal translation the x-coordinate for example) but only to the difference δt = t G(z δt ) − t G(z0) between an image G(z 0 ) and its transformation G(z δt ) (δx or δy in the case of translation). We solve this issue by modeling δt instead of t:</p><formula xml:id="formula_9" coords="5,182.20,522.43,321.80,9.96">δt = f (θ,u) (z δt ) − f (θ,u) (z 0 ) with ||u|| = 1 and g θ (0) = 0 (9)</formula><p xml:id="_78YwnXP">Hence, u and θ are estimated by training f (θ,u) to minimize the MSE between δ t and f (θ,u) (z δt ) − f (θ,u) (z 0 ) with gradient descent on a dataset produced by Algorithm 1 for a given transformation.</p><p xml:id="_944xWcv">An interesting application of this method is the estimation of the distribution of the images generated by G by using Equation <ref type="formula" coords="5,204.78,581.95,3.69,8.64" target="#formula_6">6</ref>. With the knowledge of g θ we can also choose how to sample images. For instance, let say that we want to have t ∼ φ(t), with φ : R → R + an arbitrary distribution, we can simply transform z ∼ N (0, 1) as follows:</p><formula xml:id="formula_10" coords="5,226.21,621.75,277.79,9.68">z ← z − z, u u + (h φ • ψ)( z, u )u (10)</formula><p xml:id="_83aMtZ7">with h φ : [0, 1] → R and ψ such that:</p><formula xml:id="formula_11" coords="5,177.68,656.19,326.32,26.29">ψ(x) = x −∞ N (t; 0, 1)dt ; h −1 φ (x) = x −∞ φ(g θ (t)) d dt g θ (t)dt<label>(11)</label></formula><p xml:id="_A7wP2Gr">These results are interesting to bring control not only on a single output of a generative model but also on the distribution of its outputs. Moreover, since generative models reflect the datasets on which they have been trained, the knowledge of these distributions could be applied to the training dataset to reveal potential bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Z29RDMb">Datasets:</head><p xml:id="_NSzQrMa">We performed experiments on two datasets. The first one is dSprites <ref type="bibr" coords="6,418.38,108.94,82.66,8.64" target="#b23">(Matthey et al., 2017)</ref>, composed of 737280 binary 64 × 64 images containing a white shape on a dark background. Shapes can vary in position, scale and orientations making it ideal to study disentanglement. The second dataset is ILSVRC <ref type="bibr" coords="6,185.64,141.82,104.39,8.64" target="#b28">(Russakovsky et al., 2015)</ref>, containing 1.2M natural images from one thousand different categories.</p><p xml:id="_rVpb5zV">Implementation details: All our experiments have been implemented with TensorFlow 2.0 <ref type="bibr" coords="6,476.34,169.71,27.67,8.64;6,108.00,180.67,48.84,8.64" target="#b0">(Abadi et al., 2015)</ref> and the corresponding code is available on github here. We used a BigGAN model <ref type="bibr" coords="6,107.67,191.63,81.35,8.64" target="#b5">(Brock et al., 2018)</ref> whose weights are taken from TensorFlow-Hub allowing easy reproduction of our results. The BigGAN model takes two vectors as inputs: a latent vector z ∈ R 128 and a one-hot vector to condition the model to generate images from one category. The latent vector z is then split into six parts which are the inputs at different scale levels in the generator. The first part is injected at the bottom layer while next parts are used to modify the style of the generated image thanks to Conditional Batch Normalization layers <ref type="bibr" coords="6,342.75,246.42,88.51,8.64" target="#b8">(de Vries et al., 2017)</ref>. We also trained several β-VAEs <ref type="bibr" coords="6,173.73,257.38,85.57,8.64" target="#b11">(Higgins et al., 2017)</ref> to study the importance of disentanglement in the process of controlling generation. The exact β-VAE architecture used is given in Appendix B. The models were trained on dSprites <ref type="bibr" coords="6,184.78,279.30,85.35,8.64" target="#b23">(Matthey et al., 2017)</ref> with an Adam optimizer during 1e5 steps with a batch size of 128 images and a learning rate of 5e−4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" xml:id="_cyRjCPg">QUANTITATIVE EVALUATION METHOD</head><p xml:id="_HP6Ppkg">Evaluating quantitatively the effectiveness of our method on complex datasets is intrinsically difficult as it is not always trivial to measure a factor of variation directly. We focused our analysis on two factors of variations: position and scale. On simple datasets such as dSprites, the position of the object can be estimated effectively by computing the barycenter of white pixels. However, for natural images sampled with the BigGAN model, we have to use first saliency detection on the generated image to produce a binary image from which we can extract the barycenter. For saliency detection, we used the model provided by <ref type="bibr" coords="6,236.09,401.48,68.78,8.64">Hou et al. (2016)</ref> which is implemented in the PyTorch framework <ref type="bibr" coords="6,107.67,412.44,82.71,8.64" target="#b25">(Paszke et al., 2017)</ref>. The scale is evaluated by the proportion of salient pixels. The evaluation procedure is:</p><p xml:id="_EZYQRBr">1. Get the direction u which should describe the chosen factor of variation with our method. 2. Sample latent codes z from a standard normal distribution.</p><p xml:id="_yH8WDAx">3. Generate images with latent code z − z, u u + tu with t ∈ [−T, T ]. 4. Estimate the real value of the factor of variation for all the generated images. 5. Measure the standard deviation of this value with respect to t. <ref type="bibr" coords="6,107.81,524.48,88.05,8.64" target="#b13">Jahanian et al. (2019)</ref> proposed an alternative method for quantitative evaluation that relies on an object detector. Similarly to us, it allows an evaluation for x and y shift as well as scale but is restricted to image categories that can be recognized by a detector trained on some categories of ILSVRC. The proposed approach is thus more generic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" xml:id="_KpBhgbu">RESULTS ON BIGGAN</head><p xml:id="_J4Xx5yw">We performed quantitative analysis on ten chosen categories of objects of ILSVRC, avoiding non actual objects such as "beach" or 'cliff". Results are presented in Figure <ref type="figure" coords="6,405.19,613.79,5.08,8.64" target="#fig_0">2</ref> (top). We observe that for the chosen categories of ILSVRC, we can control the position and scale of the object relatively precisely by moving along directions of the latent space found by our method. However, one can still wonder whether the directions found are independent of the category of interest. To answer this question, we merged all the datasets of trajectories into one and learned a common direction on the resulting datasets. Results for the ten test categories are shown in Figure <ref type="figure" coords="6,409.04,668.58,5.08,8.64" target="#fig_0">2</ref> (bottom). This figure shows that the directions which correspond to some factors of variations are indeed shared between all the categories. Qualitative results are also presented in Figure <ref type="figure" coords="6,366.29,690.50,4.94,8.64" target="#fig_1">3</ref> for illustrative purposes. We also checked which parts of the latent code are used to encode position and scale. Indeed, BigGAN uses hierarchical latent code which means that the latent code is split into six parts which are injected at different level of the generator. We wanted to see by which part of the latent code these directions are encoded. The squared norm of each part of the latent code is reported in Figure <ref type="figure" coords="7,441.84,439.37,5.00,8.64" target="#fig_2">4</ref> for horizontal position, vertical position and scale. This figure shows that the directions corresponding to spatial factors of variations are mainly encoded in the first part of the latent code. However, for the y position, the contribution of level 5 is higher than for the x position and the scale. We suspect that it is due to correlations between the vertical position of the object in the image and its background that we introduced by transforming the objects because the background is not invariant by vertical translation because of the horizon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3" xml:id="_J76cj3N">THE IMPORTANCE OF DISENTANGLED REPRESENTATIONS</head><p xml:id="_JahaZbF">To test the effect of disentanglement on the performance of our method, we trained several β-VAE <ref type="bibr" coords="7,128.75,561.49,83.10,8.64" target="#b11">(Higgins et al., 2017)</ref> on dSprites <ref type="bibr" coords="7,260.78,561.49,82.81,8.64" target="#b23">(Matthey et al., 2017)</ref>, with different β values. Indeed, β-VAE are known for having more disentangled latent spaces as the regularization parameter β increases.</p><p xml:id="_bnXtUu7">Results can be seen in Figure <ref type="figure" coords="7,230.77,583.40,3.81,8.64" target="#fig_3">5</ref>. The figure shows that it is possible to control the position of the object on the image by moving in the latent space along the direction found with our method. As expected, the effectiveness of the method depends on the degree of disentanglement of the latent space since the results are better with a larger β. Indeed we can see on Figure <ref type="figure" coords="7,421.34,616.28,5.02,8.64" target="#fig_3">5</ref> that as β increases, the standard deviation decreases (red curve), allowing a more precise control of the position of the generated images. This observation motivates further the interest of disentangled representations for control on the generative process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" xml:id="_UWeqCHA">RELATED WORKS</head><p xml:id="_SFjY8kZ">Our work aims at finding interpretable directions in the latent space of generative models to control their generative process. We distinguish two families of generative models: GAN-like models which do not provide an explicit way to get the latent representation of an image and auto-encoders which  provide an encoder to get the latent representation of images. From an architectural point of view, conditional GANs <ref type="bibr" coords="8,181.68,569.95,77.70,8.64">(Odena et al., 2016)</ref> allows the user to choose the category of a generated object or some chosen properties of the generated image but this approach requires a labeled dataset and use a model which is explicitly designed to allow this control. Similarly regarding VAE, <ref type="bibr" coords="8,432.03,591.87,72.64,8.64" target="#b9">Engel et al. (2018)</ref> identified that they suffer from a trade-off between reconstruction accuracy and sample plausibility and proposed to identify regions of the latent space that correspond to plausible samples to improve reconstruction accuracy. They also use conditional reconstruction to control the generative process.</p><p xml:id="_v4WrBw6">In comparison to these approaches, our method does not directly requires labels. With InfoGan, <ref type="bibr" coords="8,483.39,635.71,20.61,8.64;8,108.00,646.66,49.67,8.64" target="#b16">Chen et al. (2016)</ref> shows that adding a code to the the input of the GAN generator and optimizing with an appropriate regularization term leads to disentangle the latent space and make possible to find a posteriori meaningfully directions. In contrast, we show that it is possible to find such directions in several generative models, without changing the learning process (our approach could even be applied to InfoGAN) and with an a priori knowledge of the factor of variation sought. More recently, <ref type="bibr" coords="8,108.00,701.46,65.65,8.64" target="#b1">Bau et al. (2018)</ref> analyze the activations of the network's neurons to determine those that result in the presence of an object in the generated image, and thus allows to control such a presence. In contrast, our work focuses on the latent space and not on the intermediate activations inside the generator. One of our contribution and a part of our global method is a procedure to find the latent representation of an image when an encoder is not available. Several previous works have studied how to invert the generator of a GAN to find the latent code of an image. <ref type="bibr" coords="9,348.43,421.51,109.96,8.64">Creswell &amp; Bharath (2016)</ref> showed on simple datasets (MNIST <ref type="bibr" coords="9,209.91,432.47,79.88,8.64" target="#b18">(Lecun et al., 1998)</ref> and <ref type="bibr" coords="9,309.56,432.47,115.08,8.64">Omniglot (Lake et al., 2015)</ref>) that this inversion process can be achieved by optimizing the latent code to minimize the reconstruction error between the generated image and the target image. <ref type="bibr" coords="9,275.79,454.38,99.26,8.64" target="#b19">Lipton &amp; Tripathi (2017)</ref> introduced tricks to improve the results on a more challenging dataset (CelebA <ref type="bibr" coords="9,296.59,465.34,65.55,8.64" target="#b21">(Liu et al., 2015)</ref>). However we observed that these methods fail when applied on a more complex datasets (ILSVRC <ref type="bibr" coords="9,372.70,476.30,104.15,8.64" target="#b28">(Russakovsky et al., 2015)</ref>). The reconstruction loss introduced in Section 2.1.1 is adapted to this particular problem and improves the quality of reconstructions significantly. We also theoretically justify the difficulties to invert a generative model, compared to other optimization problems. In the context of vector space arithmetic in a latent space, <ref type="bibr" coords="9,176.88,520.14,53.39,8.64" target="#b29">White (2016)</ref> argues that replacing a linear interpolation by a spherical one allows to reduce the blurriness as well. This work also propose an algorithmic data augmentation, named "synthetic attribute", to generate image with less noticeable blur with a VAE. In contrast, we act directly on the loss.</p><p xml:id="_BxpEAkm">The closest works were released on ArXiv very recently <ref type="bibr" coords="9,335.31,569.95,107.83,8.64" target="#b10">(Goetschalckx et al., 2019;</ref><ref type="bibr" coords="9,445.62,569.95,59.62,8.64;9,108.00,580.91,23.71,8.64" target="#b13">Jahanian et al., 2019)</ref> indicating that finding interpretable directions in the latent space of generative models to control their output is of high interest for the community. In these papers, the authors describe a method to find interpretable directions in the latent space of the BigGAN model <ref type="bibr" coords="9,450.94,602.83,54.30,8.64;9,108.00,613.79,21.23,8.64" target="#b5">(Brock et al., 2018)</ref>. If their method exhibits similarities with ours (use of transformation, linear trajectories in the latent space), it also differs on several points. From a technical point of view our training procedure differs in the sense that we first generate a dataset of interesting trajectories to then train our model while they train their model directly. Our evaluation procedure is also more general as we use a saliency model instead of a MobileNet-SSD v1 <ref type="bibr" coords="9,297.08,657.62,64.27,8.64" target="#b20">Liu et al. (2016)</ref> trained on specific categories of the ILSVRC dataset allowing us to measure performance on more categories. We provide additional insight on how auto-encoders can also be controlled with the method, the impact of disentangled representations on the control and on the structure of the latent space of BigGAN. Moreover we also propose an alternative reconstruction error to invert generators. However, the main difference we identify between the two works is the model of the latent space used. Our model allows a more precise control over the generative process and can be being adapted to more cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_GBaduXa">A PENALTY ON THE AMPLITUDE OF FREQUENCIES DUE TO MSE</head><p xml:id="_gv3QBkt">In Section 2.1, we consider a target image I ∈ I and a generated image Î = G(ẑ) to be determined according to a reconstruction loss L (Equation <ref type="formula" coords="13,292.48,121.10,3.53,8.64">1</ref>). Let us note F{•} the Fourier transform. If L is the usual MSE, from the Plancherel theorem, we have || Î − I|| 2 = ||F{ Î} − F{I}|| 2 . Let us consider a particular frequency ω in the Fourier space and compute its contribution to the loss. The Fourier transform of I (resp. Î) having a magnitude r (resp. r) and a phase θ (resp. θ) at ω, we have:</p><formula xml:id="formula_12" coords="13,156.67,173.73,347.33,70.12">|F{ Î}(ω) − F {I}(ω)| 2 = |re i θ − re iθ | 2 = (rcos( θ) − rcos(θ)) 2 + (rsin( θ) − rsin(θ)) 2 = r2 + r 2 − 2rr cos( θ)cos(θ) + sin( θ)sin(θ) = r2 + r 2 − 2rr cos( θ)cos(θ) + sin( θ)sin(θ)<label>(12)</label></formula><p xml:id="_H8SPrXZ">If we model the disability of the generator to model every high frequency patterns as an uncertainty on the phase of high frequency of the generated image, i.e by posing θ ∼ U([0, 2π]), the expected value of the high frequency contributions to the loss is equal to:</p><formula xml:id="formula_13" coords="13,117.38,295.61,386.62,62.35">E |F{ Î}(ω) − F {I}(ω)| 2 = r2 + r 2 − 2rr     E cos( θ) =0 cos(θ) + E sin( θ) =0 sin(θ)     = r2 + r 2 (13)</formula><p xml:id="_4aKWKaN">The term r 2 is a constant w.r.t the optimization of L and can thus be ignored. The contribution to the total loss L thus directly depends on r2 . While minimizing L, the optimization process tends to favor images Î = G(ẑ) with smaller magnitudes in the high frequencies, that is to say smoother images, with less high frequencies.</p><p xml:id="_eAG98tj">B β-VAE  On Fig. <ref type="figure" coords="14,140.77,545.56,4.95,8.64">6</ref> we show qualitative reconstruction results with our method (Eq. 3) for several values of σ.</p><p xml:id="_kgp7zHK">On this representative example, we observe quite good results with σ = 3 and σ = 5. Higher values penalizes too low frequencies that lead to a less accurate reconstruction.</p><p xml:id="_3NpNqPg">We also illustrate on Fig. <ref type="figure" coords="14,206.36,584.41,4.88,8.64">7</ref> a comparison of our approach to two others, namely classical Mean Square Error (MSE) and Structural dissimilarity (DSSIM) proposed by Zhou <ref type="bibr" coords="14,392.63,595.37,73.83,8.64" target="#b31">Wang et al. (2004)</ref>. Results are also presented with an unconstrained latent code during optimization (Eq. 1) and the approach proposed (Eq. 2). This example show the accuracy of the reconstruction obtained with our approach, as well as the fact that the restriction of z to a ball of radius √ d avoids the presence of artifacts.</p><p xml:id="_dt47RRy">We also performed a quantitative evaluation of the performance of our approach. We randomly selected one image for each of the 1000 categories of the ILSVRC dataset and reconstructed it with our method with a budget of 3000 iterations. We then computed the Learned Perceptual Image Patch Similarity (LPIPS), proposed by <ref type="bibr" coords="14,239.16,679.54,74.53,8.64" target="#b30">Zhang et al. (2018)</ref>, between the final reconstruction and the target image. We used the official implementation of the LPIPS paper with default parameters. Results are reported in The curvature of the natural image manifold makes the optimization problem of Equation 2 difficult to solve. This is especially true for factors of variation which correspond to curved walks in pixel-space (for example translation or rotation by opposition to brightness or contrast changes which are linear).</p><p xml:id="_Rs5zq4h">To illustrate this fact, we show that the trajectory described by an image undergoing common transformations is curved in pixel space. We consider three types of transformations, namely translation, rotation and scaling, and get images from the dSprites <ref type="bibr" coords="15,383.06,328.76,89.68,8.64" target="#b23">(Matthey et al., 2017)</ref> dataset which correspond to the progressive transformation (interpolation) of an image. To visualize, we compute the PCA of the resulting trajectories and plot the trajectories on the two main axes of the PCA. The result of this experiment can be seen in Figure <ref type="figure" coords="15,336.96,361.64,3.74,8.64" target="#fig_5">8</ref>. In this figure, we can see that for large translations, the direction of the shortest path between two images in pixel-space is near orthogonal to the manifold. The same problem occurs for rotation and, at a smaller extent, for scale. However this problem does not exist for brightness for example, as its change is a linear transformation in pixel-space. This is problematic during optimization of the latent code because the gradient of the reconstruction loss with respect to the generated image is tangent to this direction. Thus, when we are in the case of near orthogonality, the gradient of the error with respect to the latent code is small.</p><p xml:id="_TWJ9SGT">Indeed, let us consider an ideal case where G is a bijection between Z and the manifold of natural images. Let be z ∈ Z, a basis of vectors tangent to the manifold at point G(z) is given by ∂G(z) ∂z1 , ..., ∂G(z)    It shows that when the direction of descent in pixel space is near orthogonal to the manifold described by the generative model, optimization gets slowed down and can stop if the gradient of the loss with respect to the generated image is orthogonal to the manifold.</p><p xml:id="_b9cX9c2">For example, let assume we have an ideal GAN which generates a small white circle on a black background, with a latent space of dimension 2 that encodes the position of the circle. Let consider a generated image with the circle on the left of the image and we want to move it to the right. Obviously, we thus have ∇ z ||G(z) − T T (G(z 1 ))|| 2 = 0 if the intersection of the two circles is empty (see Figure <ref type="figure" coords="16,136.50,225.12,4.15,8.64" target="#fig_5">8</ref>) since a small translation of the object does not change the reconstruction error.</p><p xml:id="_BPEDbvb">E ADDITIONAL QUALITATIVE EXAMPLES </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,108.00,342.51,396.00,8.64;7,108.00,353.46,396.00,8.64;7,108.00,364.42,396.00,8.64;7,108.00,375.06,396.17,8.96;7,108.00,386.34,396.17,8.64;7,108.00,397.30,396.00,8.64;7,108.00,407.94,119.85,8.96;7,113.42,205.46,382.69,122.61"><head>Figure 2 :</head><label>2</label><figDesc xml:id="_6w4xyCu">Figure 2: Quantitative results on the ten categories of the ILSVRC dataset used for training (Top) and for ten other categories used for validation (Bottom) for three geometric transformations: horizontal and vertical translations and scaling. In blue, the distribution of the measured transformation parameter and in red the standard deviation of the distribution with respect to t. Note that for large scales the algorithm seems to fail. However, this phenomenon is very likely due to the poor performance of the saliency model when the object of interest covers almost the entire image (scale ≈ 1.0). (best seen with zoom)</figDesc><graphic coords="7,113.42,205.46,382.69,122.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,108.00,339.38,397.66,8.64;8,108.00,350.34,213.24,8.64;8,155.94,244.58,297.64,80.36"><head>Figure 3 :</head><label>3</label><figDesc xml:id="_ucKwQc8">Figure 3: Qualitative results for some categories of ILSVRC dataset for three geometric transformations: horizontal and vertical translations and scaling.</figDesc><graphic coords="8,155.94,244.58,297.64,80.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,108.00,510.36,396.00,8.64;8,108.00,521.31,22.41,8.64;8,113.42,378.62,382.67,117.30"><head>Figure 4 :</head><label>4</label><figDesc xml:id="_TnwVACv">Figure 4: Squared norm of each part of the latent code for horizontal position, vertical position and scale.</figDesc><graphic coords="8,113.42,378.62,382.67,117.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,108.00,294.43,396.00,8.96;9,108.00,305.71,396.34,8.64;9,108.00,316.67,346.22,8.64;9,113.42,81.86,382.68,198.45"><head>Figure 5 :</head><label>5</label><figDesc xml:id="_mNhRs3R">Figure 5: Results of our evaluation procedure with four β-VAE for β = 1, 5, 10, 20. Note the erf shape of the results which indicates that the distribution of the shape positions has been correctly learned by the VAE. See Figure 2 for additional information on how to read this figure.</figDesc><graphic coords="9,113.42,81.86,382.68,198.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="14,108.00,249.02,396.00,8.96;14,108.00,260.29,82.20,8.64;14,113.42,139.15,95.67,95.67"><head>CFigure 6 :Figure 7 :</head><label>67</label><figDesc xml:id="_EgZumDu">Figure 6: Reconstruction results with different σ values. We typically used a standard deviation of 3 pixels for the kernel.</figDesc><graphic coords="14,113.42,139.15,95.67,95.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="15,108.00,496.80,397.24,8.64;15,108.00,507.75,396.66,8.64;15,108.00,518.71,396.00,8.64;15,108.00,529.67,354.72,8.64;15,110.93,385.26,127.56,97.10"><head>Figure 8 :</head><label>8</label><figDesc xml:id="_dX3REjm">Figure 8: Two trajectories are shown in the pixel space, between an image and its transformed version, for three types of transformations: translation, scale and orientation. Red: shortest path (interpolation) between the two extremes of the trajectory. Blue: trajectory of the actual transformation. At each position along the trajectories, we report the corresponding image (best seen with zoom).</figDesc><graphic coords="15,110.93,385.26,127.56,97.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="15,160.75,676.51,12.79,6.79;15,185.44,671.35,2.49,8.64;15,108.00,691.95,256.47,9.95;15,176.11,719.71,150.54,9.96;15,329.50,712.97,27.37,8.74;15,335.85,726.54,14.19,9.65;15,364.71,719.71,71.18,9.65;15,487.40,720.03,16.60,8.64;16,107.69,85.34,22.42,8.64;16,163.55,112.82,89.60,9.81;16,262.65,106.08,27.37,8.74;16,270.44,119.65,11.37,8.74;16,291.22,103.11,4.08,6.12;16,295.80,112.82,103.18,9.95;16,417.38,102.41,4.15,6.12;16,413.00,126.61,12.91,6.12;16,432.36,111.43,3.97,6.12;16,432.36,117.64,2.82,6.12;16,439.59,112.82,15.50,8.74;16,487.40,113.14,16.60,8.64"><head></head><label></label><figDesc xml:id="_ST2PGvv">∂z d . If ∇ G(z) L(G(z), I target ) is near orthogonal to the manifold then: ∀i ∈ 1, ..., d : ∇ G(z) L(G(z), I target ), ∂G(z) ∂z i = i with i ≈ 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="16,108.00,710.26,396.00,8.64;16,107.67,721.22,269.07,8.64;16,112.17,492.49,191.34,203.34"><head>Figure 9 :FFigure 10 :</head><label>910</label><figDesc xml:id="_tHY4BqZ">Figure 9: Qualitative results for 10 categories of ILSVRC dataset for three geometric transformations (horizontal and vertical translations and scaling) and for brightness.</figDesc><graphic coords="16,112.17,492.49,191.34,203.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,107.75,228.82,397.99,93.91"><head></head><label></label><figDesc xml:id="_SawYJdk">Solving this problem by optimization leads to solutions located in regions of low likelihood of the distribution used during training. It causes the reconstructed image Î = G( ẑ) to look unrealistic 1 . Since z follows a normal distribution N (0, I d ) in a d-dimensional space, we have ||z|| ∼ χ d . Thus,</figDesc><table coords="3,107.75,256.34,396.25,66.39"><row><cell>lim d→+∞ E [||z||] = approximately equal to √ verify ||z|| ≤ √ d:</cell><cell cols="2">d and lim d→+∞ Var (||z||) = 0. Hence, when d is large, the norm of z is √ d. This can be used to regularize the optimization by constraining z to</cell></row><row><cell></cell><cell>ẑ = arg min z∈Z,||z||≤ √</cell><cell>d</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="13,107.69,431.80,396.31,286.36"><head>Table 1 :</head><label>1</label><figDesc xml:id="_dgeAACV">ARCHITECTUREThe β-VAE framework was introduced by<ref type="bibr" coords="13,279.00,455.09,82.93,8.64" target="#b11">Higgins et al. (2017)</ref> to discover interpretable factorized latent representations for images without supervision. For our experiments, we designed a simple convolutional VAE architecture to generate images of size 64x64, the decoder network is the opposite of the encoder with transposed convolutions. β-VAE architecture used during experiments with the dSprites dataset.</figDesc><table coords="13,128.34,514.07,352.83,180.41"><row><cell>Encoder</cell><cell>Decoder</cell></row><row><cell>Convolution + ReLU filters=32 size=4 stride=2 pad=SAME Convolution + ReLU filters=32 size=4 stride=2 pad=SAME Convolution + ReLU filters=32 size=4 stride=2 pad=SAME Convolution + ReLU filters=32 size=4 stride=2 pad=SAME Dense + ReLU units=256 Dense + ReLU units=256 µ: Dense + Identity σ: Dense + Exponential units=10</cell><cell>Dense + ReLU units=256 Dense + ReLU units=256 Reshape shape=4x4x32 Transposed Convolution + ReLU filters=32 size=4 stride=2 pad=SAME Transposed Convolution + ReLU filters=32 size=4 stride=2 pad=SAME Transposed Convolution + ReLU filters=32 size=4 stride=2 pad=SAME Transposed Convolution + Sigmoid filters=1 size=4 stride=2 pad=SAME</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="14,108.00,701.46,396.00,30.56"><head>Table 2 .</head><label>2</label><figDesc xml:id="_EtXUkh3">It suggests that images reconstructed using our reconstruction error are perceptually closer to the target image than those obtained with MSE or DSSIM. The higher standard deviation for the MSE reconstructed image LPIPS suggests that some images are downgraded in terms of perception. It can be the case for the textured ones in particular, for the reasons explained in the Section A.</figDesc><table coords="15,107.69,122.64,396.31,128.86"><row><cell cols="3">reconstruction error mean LPIPS std LPIPS</cell></row><row><cell>MSE</cell><cell>0.57</cell><cell>0.14</cell></row><row><cell>DSSIM</cell><cell>0.58</cell><cell>0.12</cell></row><row><cell>Our (σ = 3)</cell><cell>0.52</cell><cell>0.12</cell></row><row><cell cols="3">Table 2: Perceptual similarity measurements between an image and its reconstruction for different</cell></row><row><cell>reconstruction errors.</cell><cell></cell><cell></cell></row><row><cell cols="3">D ON THE DIFFICULTY OF OPTIMIZATION ON THE NATURAL IMAGE</cell></row><row><cell>MANIFOLD.</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We could have used a L</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">penalty on the norm of z to encode a centered Gaussian prior on the distribution of z. However the L 2 penalty requires an additional hyper-parameter β that can be difficult to choose.2 Indeed, if we model the value of pixel by a random variable x then arg minx E (x − x) 2 = E [x].In fact, this problem can easily generalized at every pixel-wise loss if we assume that nearby pixels follows approximately the same distribution as arg min x E [L(x, x)] will have the same value for nearby pixels.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" xml:id="_KghrFHX">CONCLUSIONS</head><p xml:id="_9gccaec">Generative models are increasingly more powerful but suffer from little control over the generative process and the lack of interpretability in their latent representations. In this context, we propose a method to extract meaningful directions in the latent space of such models and use them to control precisely some properties of the generated images. We show that a linear subspace of the latent space of BigGAN can be interpreted in term of intuitive factors of variation (namely translation and scale). It is an important step toward the understanding of the representations learned by generative models.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="10,108.00,210.14,397.74,8.64;10,117.96,221.10,386.40,8.64;10,117.96,232.06,386.03,8.64;10,117.96,243.02,387.28,8.64;10,117.96,253.98,386.03,8.64;10,117.60,264.94,387.64,8.64;10,117.96,275.90,386.04,8.64;10,117.96,286.86,386.03,8.64;10,117.96,297.82,81.10,8.64" xml:id="b0">
	<monogr>
		<title level="m" type="main" xml:id="_bahd69Z">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName coords=""><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dandelion</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Derek</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernanda</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/.Softwareavailablefromtensorflow.org" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,316.58,397.25,8.64;10,117.96,327.54,386.03,8.64;10,117.96,338.32,233.18,8.82" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10597</idno>
		<title level="m" xml:id="_2FXZCat">GAN Dissection: Visualizing and Understanding Generative Adversarial Networks. arXiv e-prints, art</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,357.26,397.24,8.64;10,117.96,368.04,386.04,8.82;10,117.63,379.00,340.03,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_pfxN4Pg">Understanding and predicting importance in images</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_6nD5QT4">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2012-06">June 2012</date>
			<biblScope unit="page" from="3562" to="3569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,397.94,397.65,8.64;10,117.96,408.72,386.04,8.82;10,117.96,419.86,22.42,8.64" xml:id="b3">
	<monogr>
		<title level="m" type="main" xml:id="_s7j23fQ">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName coords=""><forename type="first">Anders</forename><surname>Boesen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lindbo</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Søren</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.09300</idno>
		<imprint>
			<date type="published" when="2015-12">Dec 2015</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints, art</note>
</biblStruct>

<biblStruct coords="10,108.00,438.62,397.24,8.64;10,117.96,449.58,386.04,8.64;10,117.60,460.36,387.64,8.82;10,117.96,471.49,132.29,8.64" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Bowles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ricardo</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roger</forename><surname>Gunn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Hammers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Alexander Dickie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Valdés Hernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joanna</forename><surname>Wardlaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10863</idno>
		<title level="m" xml:id="_UJXNQZ7">GAN Augmentation: Augmenting Training Data using Generative Adversarial Networks. arXiv e-prints, art</title>
				<imprint>
			<date type="published" when="2018-10">Oct 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,490.26,396.35,8.64;10,117.96,501.04,387.23,8.82;10,117.66,513.11,62.27,7.01" xml:id="b5">
	<monogr>
		<title level="m" type="main" xml:id="_NbwxK8x">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>CoRR, abs/1809.11096</idno>
		<ptr target="http://arxiv.org/abs/1809.11096" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,530.94,397.38,8.64;10,117.96,541.89,387.78,8.64;10,117.96,552.67,188.47,8.82" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03657</idno>
		<title level="m" xml:id="_VHfd4FT">Ilya Sutskever, and Pieter Abbeel. InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. arXiv e-prints, art</title>
				<imprint>
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,571.62,396.00,8.64;10,117.96,582.40,229.31,8.82" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anil</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bharath</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05644</idno>
		<title level="m" xml:id="_dcjdAnp">Inverting The Generator Of A Generative Adversarial Network. arXiv e-prints, art</title>
				<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,601.34,397.74,8.64;10,117.96,612.12,387.41,8.82" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jérémie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00683</idno>
		<title level="m" xml:id="_SdHp9MP">Modulating early visual processing by language. arXiv e-prints, art</title>
				<imprint>
			<date type="published" when="2017-07">Jul 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,631.06,396.00,8.64;10,117.96,641.84,386.03,8.82;10,117.65,652.80,365.30,8.82" xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_SQAkgcv">Latent constraints: Learning to generate conditionally from unconditional generative models</title>
		<author>
			<persName coords=""><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Sy8XvGb0-" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_bvWu4fQ">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,671.74,396.00,8.64;10,117.96,682.52,365.17,8.82" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Lore</forename><surname>Goetschalckx</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ganalyze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.10112</idno>
		<title level="m" xml:id="_y69ahwj">Toward Visual Definitions of Cognitive Image Properties. arXiv e-prints, art</title>
				<imprint>
			<date type="published" when="2019-06">Jun 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,701.46,397.25,8.64;10,117.96,712.42,386.04,8.64;10,117.96,723.20,387.79,8.82" xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_bWRc43Y">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName coords=""><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Loïc</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_xERr6Jb">International Conference on Learning Representation</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,85.34,396.35,8.64;11,117.96,96.12,387.28,8.82;11,117.22,107.26,34.87,8.64" xml:id="b12">
	<monogr>
		<title level="m" type="main" xml:id="_RaSExjK">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName coords=""><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiao-Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04849</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct coords="11,108.00,127.38,397.74,8.64;11,117.96,138.16,190.13,8.82" xml:id="b13">
	<monogr>
		<title level="m" type="main" xml:id="_9yzUfcT">On the &quot;steerability</title>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucy</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07171</idno>
		<imprint>
			<date type="published" when="2019-07">Jul 2019</date>
		</imprint>
	</monogr>
	<note>of generative adversarial networks. arXiv e-prints, art</note>
</biblStruct>

<biblStruct coords="11,108.00,158.47,396.00,8.64;11,117.96,169.25,270.72,8.82" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08155</idno>
		<title level="m" xml:id="_kUp4ddH">Perceptual Losses for Real-Time Style Transfer and Super-Resolution. arXiv e-prints, art</title>
				<imprint>
			<date type="published" when="2016-03">Mar 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,189.55,396.00,8.64;11,117.60,200.33,282.00,8.82" xml:id="b15">
	<monogr>
		<title level="m" type="main" xml:id="_wMZWSsD">A Style-Based Generator Architecture for Generative Adversarial Networks</title>
		<author>
			<persName coords=""><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04948</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints, art</note>
</biblStruct>

<biblStruct coords="11,108.00,220.64,396.00,8.64;11,117.96,231.60,386.04,8.64;11,117.96,242.38,386.04,8.82;11,117.96,253.34,162.35,8.82" xml:id="b16">
	<monogr>
		<title level="m" type="main" xml:id="_NwhNNgw">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName coords=""><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1602.07332</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,273.64,396.00,8.64;11,117.96,284.42,387.78,8.82;11,117.96,295.56,387.23,8.64;11,117.96,307.45,80.20,7.01" xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_HufmVQs">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aab3050</idno>
		<ptr target="https://science.sciencemag.org/content/350/6266/1332" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_mkrmXgk">Science</title>
		<idno type="ISSN">0036-8075</idno>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,326.64,396.00,8.64;11,117.96,337.42,294.51,8.82" xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_PjUm3HE">Gradient-based learning applied to document recognition</title>
		<author>
			<persName coords=""><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_WzwzhMw">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,357.73,396.00,8.64;11,117.60,368.51,194.54,8.82" xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_xwxAsHP">Precise Recovery of Latent Vectors from Generative Adversarial Networks</title>
		<author>
			<persName coords=""><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Subarna</forename><surname>Tripathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_PSvT8ke">ICLR workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,388.81,396.00,8.64;11,117.60,399.59,387.64,8.82;11,117.96,410.73,106.14,8.64" xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_Xv4WhQn">Ssd: Single shot multibox detector</title>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_T3MyMvj">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,430.86,396.00,8.64;11,117.65,441.64,316.40,8.82" xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_sn68HkH">Deep learning face attributes in the wild</title>
		<author>
			<persName coords=""><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_4AepFVx">Proceedings of International Conference on Computer Vision (ICCV)</title>
				<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,461.94,396.00,8.64;11,117.96,472.72,273.51,8.82" xml:id="b22">
	<monogr>
		<title level="m" type="main" xml:id="_YJCPE4q">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015-11">Nov 2015</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints, art</note>
</biblStruct>

<biblStruct coords="11,108.00,493.03,396.00,8.64;11,117.96,503.99,300.68,8.64" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="https://github.com/deepmind/dsprites-dataset/" />
		<title level="m" xml:id="_jwEQbwy">dsprites: Disentanglement testing sprites dataset</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,524.11,396.35,8.64;11,117.96,534.89,260.13,8.82" xml:id="b24">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09585</idno>
		<title level="m" xml:id="_qBZeMfj">Conditional Image Synthesis With Auxiliary Classifier GANs. arXiv e-prints, art</title>
				<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,555.20,397.25,8.64;11,117.96,566.16,386.04,8.64;11,117.96,577.12,58.83,8.64" xml:id="b25">
	<monogr>
		<title level="m" type="main" xml:id="_TNu3PWD">Automatic differentiation in pytorch</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,597.24,396.00,8.64;11,117.96,608.02,386.71,8.82" xml:id="b26">
	<monogr>
		<title level="m" type="main" xml:id="_jJzCqhE">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints, art</note>
</biblStruct>

<biblStruct coords="11,108.00,628.33,395.99,8.64;11,117.60,639.11,243.55,8.82" xml:id="b27">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00446</idno>
		<title level="m" xml:id="_dxcnvhX">Generating Diverse High-Fidelity Images with VQ-VAE-2. arXiv e-prints, art</title>
				<imprint>
			<date type="published" when="2019-06">Jun 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,659.41,397.25,8.64;11,117.60,670.37,386.39,8.64;11,117.96,681.15,386.04,8.82;11,117.63,692.29,216.55,8.64" xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_t5HW3Mn">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xpGv5Nq">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,712.24,397.25,8.82;11,117.96,723.38,304.57,8.64" xml:id="b29">
	<monogr>
		<title level="m" type="main" xml:id="_uPCyfxs">Sampling generative networks: Notes on a few effective techniques</title>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>White</surname></persName>
		</author>
		<idno>CoRR, abs/1609.04468</idno>
		<ptr target="http://arxiv.org/abs/1609.04468" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,108.00,85.34,396.00,8.64;12,117.96,96.12,386.04,8.82;12,117.96,107.26,22.42,8.64" xml:id="b30">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03924</idno>
		<title level="m" xml:id="_gkqRWwg">The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. arXiv e-prints, art</title>
				<imprint>
			<date type="published" when="2018-01">Jan 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,108.00,126.19,396.16,8.64;12,117.71,136.97,386.29,8.82;12,117.96,148.10,150.86,8.64" xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_hTy5rDE">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Zhou Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">R</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Simoncelli</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2003.819861</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_GVGam3H">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04">April 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,108.00,167.03,397.66,8.64;12,117.96,177.81,336.49,8.82" xml:id="b32">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03552</idno>
		<title level="m" xml:id="_SGcJrPt">Generative Visual Manipulation on the Natural Image Manifold. arXiv e-prints, art</title>
				<imprint>
			<biblScope unit="volume">09</biblScope>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
