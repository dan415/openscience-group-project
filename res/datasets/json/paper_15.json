"{'title': 'Speech Emotion Analysis: Exploring the Role of Context', 'authors': [{'forename': 'Ashish', 'surname': 'Tawari', 'email': 'atawari@ucsd.edu', 'affiliation_name': 'Robotics Research Labora-tory', 'affiliation_country': 'USA'}, {'forename': 'Mohan', 'surname': 'Trivedi', 'email': 'mtrivedi@ucsd.edu', 'affiliation_name': 'Robotics Research Labora-tory', 'affiliation_country': 'USA'}, {'forename': 'Hamid', 'surname': 'Aghajan', 'email': None, 'affiliation_name': 'Robotics Research Labora-tory', 'affiliation_country': 'USA'}], 'abstract': 'Automated analysis of human affective behavior has attracted increasing attention in recent years. With the research shift toward spontaneous behavior, many challenges have come to surface ranging from database collection strategies to the use of new feature sets (e.g., lexical cues apart from prosodic features). Use of contextual information, however, is rarely addressed in the field of affect expression recognition, yet it is evident that affect recognition by human is largely influenced by the context information. Our contribution in this paper is threefold. First, we introduce a novel set of features based on cepstrum analysis of pitch and intensity contours. We evaluate the usefulness of these features on two different databases: Berlin Database of emotional speech (EMO-DB) and locally collected audiovisual database in car settings (CVRRCar-AVDB). The overall recognition accuracy achieved for seven emotions in the EMO-DB database is over 84% and over 87% for three emotion classes in CVRRCar-AVDB. This is based on tenfold stratified cross validation. Second, we introduce the collection of a new audiovisual database in an automobile setting (CVRRCar-AVDB). In this current study, we only use the audio channel of the database. Third, we systematically analyze the effects of different contexts on two different databases. We present context analysis of subject and text based on speaker/text-dependent/-independent analysis on EMO-DB. Furthermore, we perform context analysis based on gender information on EMO-DB and CVRRCar-AVDB. The results based on these analyses are promising.', 'acknowledgements': '<ontology_classes.Aknowledgement object at 0x0000029374A52D90>', 'references': [{'cites': <ontology_classes.Paper object at 0x0000029374AD2650>, 'date': '2004'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD2710>, 'date': '1998'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD2750>, 'date': '2005'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD27D0>, 'date': '2007'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD2850>, 'date': '2002'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD28D0>, 'date': 'Jul. 2002'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD2C50>, 'date': '2006'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD2CD0>, 'date': '2007'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD2D50>, 'date': '2004'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD3090>, 'date': '2004'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD3110>, 'date': '2002'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD3190>, 'date': '2007'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD3210>, 'date': 'Aug. 2005'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD3290>, 'date': 'Jan. 2009'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD3610>, 'date': '2003'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD3690>, 'date': '2006'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD3710>, 'date': '1999'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD3790>, 'date': 'Mar. 1996'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD3810>, 'date': '1998'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD3890>, 'date': '2008'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD3B90>, 'date': '2004'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD3C10>, 'date': 'May 2-7, 2004'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD3C90>, 'date': '2006'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD3D10>, 'date': 'Aug. 2008'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD3D90>, 'date': '2005'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD3F50>, 'date': '2005'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD3FD0>, 'date': 'Mar. 2007'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD8090>, 'date': 'May 2007'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD8110>, 'date': 'Apr. 2009'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD8190>, 'date': '2005'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD8710>, 'date': 'Jun. 2010'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD8790>, 'date': '2010'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD8810>, 'date': '1993'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD8890>, 'date': 'Sep. 2009'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD8910>, 'date': 'May 2008'}, {'cites': <ontology_classes.Paper object at 0x0000029374AD8990>, 'date': None}], 'keywords': ['Affect analysis', 'affective computing', 'context analysis', 'emotional speech', 'emotion intelligence', 'emotion recognition', 'vocal expression'], 'institution': '', 'filename': 'document.xml'}"