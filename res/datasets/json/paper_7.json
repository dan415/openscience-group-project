"{'title': 'Bi-modal Emotion Recognition from Expressive Face and Body Gestures', 'authors': [{'forename': 'Hatice', 'surname': 'Gunes', 'email': 'haticeg@it.uts.edu.au', 'affiliation_name': 'Faculty of Information Technology', 'affiliation_country': 'Australia'}, {'forename': 'Massimo', 'surname': 'Piccardi', 'email': None, 'affiliation_name': 'Faculty of Information Technology', 'affiliation_country': 'Australia'}], 'abstract': 'Psychological research findings suggest that humans rely on the combined visual channels of face and body more than any other channel when they make judgments about human communicative behavior. However, most of the existing systems attempting to analyze the human nonverbal behavior are mono-modal and focus only on the face. Research that aims to integrate gestures as an expression mean has only recently emerged. Accordingly, this paper presents an approach to automatic visual recognition of expressive face and upper-body gestures from video sequences suitable for use in a vision-based affective multi-modal framework. Face and body movements are captured simultaneously using two separate cameras. For each video sequence single expressive frames both from face and body are selected manually for analysis and recognition of emotions. Firstly, individual classifiers are trained from individual modalities. Secondly, we fuse facial expression and affective body gesture information at the feature and at the decision level. In the experiments performed, the emotion classification using the two modalities achieved a better recognition accuracy outperforming classification using the individual facial or bodily modality alone.', 'acknowledgements': '<ontology_classes.Aknowledgement object at 0x00000293745FF890>', 'references': [{'cites': <ontology_classes.Paper object at 0x000002937463C850>, 'date': '2003'}, {'cites': <ontology_classes.Paper object at 0x000002937463CF50>, 'date': '2005'}, {'cites': <ontology_classes.Paper object at 0x000002937463CFD0>, 'date': '2004'}, {'cites': <ontology_classes.Paper object at 0x000002937463D050>, 'date': '2004'}, {'cites': <ontology_classes.Paper object at 0x000002937463D0D0>, 'date': '1992'}, {'cites': <ontology_classes.Paper object at 0x000002937463D2D0>, 'date': '1975'}, {'cites': <ontology_classes.Paper object at 0x000002937463CB90>, 'date': '2002'}, {'cites': <ontology_classes.Paper object at 0x000002937463D390>, 'date': '1992'}, {'cites': <ontology_classes.Paper object at 0x000002937463D410>, 'date': '2005'}, {'cites': <ontology_classes.Paper object at 0x000002937463D450>, 'date': '2002'}, {'cites': <ontology_classes.Paper object at 0x000002937463D4D0>, 'date': '1998'}, {'cites': <ontology_classes.Paper object at 0x000002937463D550>, 'date': None}, {'cites': <ontology_classes.Paper object at 0x000002937463D7D0>, 'date': '1999'}, {'cites': <ontology_classes.Paper object at 0x000002937463D850>, 'date': '1999'}, {'cites': <ontology_classes.Paper object at 0x000002937463D8D0>, 'date': '2002'}, {'cites': <ontology_classes.Paper object at 0x000002937463D950>, 'date': None}], 'keywords': ['Bi-modal emotion recognition', 'Facial expression', 'Expressive body gestures', 'Feature-level fusion', 'Decision-level fusion'], 'institution': '', 'filename': '2007000499.xml'}"