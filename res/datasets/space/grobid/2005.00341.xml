<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_AZuHqV7">Jukebox: A Generative Model for Music</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,66.00,142.20,77.21,8.96"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">OpenAI</orgName>
								<address>
									<region>San Francisco</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,160.15,142.20,52.05,8.96"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">OpenAI</orgName>
								<address>
									<region>San Francisco</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,229.14,142.20,68.25,8.96"><forename type="first">Christine</forename><surname>Payne</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">OpenAI</orgName>
								<address>
									<region>San Francisco</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,314.33,142.20,68.85,8.96"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">OpenAI</orgName>
								<address>
									<region>San Francisco</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,394.97,142.20,57.03,8.96"><forename type="first">Alec</forename><surname>Radford</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">OpenAI</orgName>
								<address>
									<region>San Francisco</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,463.79,142.20,60.81,8.96"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">OpenAI</orgName>
								<address>
									<region>San Francisco</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="3,337.20,188.18,28.24,5.87;3,341.16,195.50,20.32,5.87"><forename type="first">Lookup</forename><forename type="middle">Codebook</forename><surname>Codebook</surname></persName>
						</author>
						<author>
							<persName coords="3,341.16,127.93,20.32,5.87"><surname>Lookup</surname></persName>
						</author>
						<title level="a" type="main" xml:id="_QUb7R4B">Jukebox: A Generative Model for Music</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">783CDCCC38136015C54128EABD7D612F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-05-12T20:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_B965zxP"><p xml:id="_RV2tZZ3">We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples, along with model weights and code.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1." xml:id="_gmqn5BV">Introduction</head><p xml:id="_gENu2ub">Music is an integral part of human culture, existing from the earliest periods of human civilization and evolving into a wide diversity of forms. It evokes a unique human spirit in its creation, and the question of whether computers can ever capture this creative process has fascinated computer scientists for decades. We have had algorithms generating piano sheet music <ref type="bibr" coords="1,108.22,459.72,118.25,12.00">(Hiller Jr &amp; Isaacson, 1957;</ref><ref type="bibr" coords="1,230.30,459.72,59.97,12.00" target="#b43">Moorer, 1972;</ref><ref type="bibr" coords="1,55.44,471.68,85.39,12.00" target="#b25">Hadjeres et al., 2017;</ref><ref type="bibr" coords="1,143.32,471.68,75.23,12.00" target="#b28">Huang et al., 2017)</ref>, digital vocoders generating a singer's voice <ref type="bibr" coords="1,166.55,483.63,97.18,12.00" target="#b8">(Bonada &amp; Serra, 2007;</ref><ref type="bibr" coords="1,266.29,483.63,23.15,12.00;1,55.44,495.59,46.27,12.00" target="#b60">Saino et al., 2006;</ref><ref type="bibr" coords="1,104.19,495.59,100.10,12.00" target="#b7">Blaauw &amp; Bonada, 2017)</ref> and also synthesizers producing timbres for various musical instruments <ref type="bibr" coords="1,262.38,507.54,27.06,12.00;1,55.44,519.50,46.20,12.00" target="#b19">(Engel et al., 2017;</ref><ref type="bibr" coords="1,104.13,519.50,21.02,12.00">2019)</ref>. Each captures a specific aspect of music generation: melody, composition, timbre, and the human voice singing. However, a single system to do it all remains elusive.</p><p xml:id="_N3aP3Rj">The field of generative models has made tremendous progress in the last few years. One of the aims of generative modeling is to capture the salient aspects of the data and to generate new instances indistinguishable from the true data The hypothesis is that by learning to produce the data we can learn the best features of the data<ref type="foot" coords="1,247.65,633.90,3.97,6.12" target="#foot_0">1</ref> . We are surrounded by highly complex distributions in the visual, audio, and text domain, and in recent years we have devel-oped advances in text generation <ref type="bibr" coords="1,444.93,174.45,62.61,12.00">(Radford et al.)</ref>, speech generation <ref type="bibr" coords="1,352.69,186.40,68.78,12.00" target="#b74">(Xie et al., 2017)</ref> and image generation <ref type="bibr" coords="1,513.55,186.40,28.14,12.00;1,307.44,198.36,49.00,12.00" target="#b9">(Brock et al., 2019;</ref><ref type="bibr" coords="1,359.49,198.36,78.79,12.00" target="#b56">Razavi et al., 2019)</ref>. The rate of progress in this field has been rapid, where only a few years ago we had algorithms producing blurry faces <ref type="bibr" coords="1,461.19,222.27,81.49,12.00;1,307.44,234.22,22.81,12.00" target="#b37">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" coords="1,332.74,234.22,93.21,12.00" target="#b23">Goodfellow et al., 2014</ref>) but now we now can generate high-resolution faces indistinguishable from real ones <ref type="bibr" coords="1,307.11,258.14,81.94,12.00" target="#b79">(Zhang et al., 2019b)</ref>.</p><p xml:id="_zV5C4NP">Generative models have been applied to the music generation task too. Earlier models generated music symbolically in the form of a pianoroll, which specifies the timing, pitch, velocity, and instrument of each note to be played. <ref type="bibr" coords="1,517.04,311.93,24.40,12.00;1,307.44,323.89,47.42,12.00" target="#b76">(Yang et al., 2017;</ref><ref type="bibr" coords="1,357.35,323.89,72.25,12.00" target="#b18">Dong et al., 2018;</ref><ref type="bibr" coords="1,432.09,323.89,81.17,12.00" target="#b29">Huang et al., 2019a;</ref><ref type="bibr" coords="1,515.75,323.89,26.93,12.00;1,307.44,335.84,22.88,12.00" target="#b50">Payne, 2019;</ref><ref type="bibr" coords="1,332.82,335.84,81.13,12.00" target="#b59">Roberts et al., 2018;</ref><ref type="bibr" coords="1,416.44,335.84,62.63,12.00" target="#b73">Wu et al., 2019)</ref>. The symbolic approach makes the modeling problem easier by working on the problem in the lower-dimensional space. However, it constrains the music that can be generated to being a specific sequence of notes and a fixed set of instruments to render with. In parallel, researchers have been pursuing the nonsymbolic approach, where they try to produce music directly as a piece of audio. This makes the problem more challenging, as the space of raw audio is extremely high dimensional with a high amount of information content to model. There has been some success, with models producing piano pieces either in the raw audio domain <ref type="bibr" coords="1,437.28,467.35,76.28,12.00" target="#b45">(Oord et al., 2016;</ref><ref type="bibr" coords="1,516.60,467.35,24.84,12.00;1,307.44,479.31,47.79,12.00" target="#b42">Mehri et al., 2017;</ref><ref type="bibr" coords="1,357.72,479.31,94.91,12.00" target="#b75">Yamamoto et al., 2020)</ref> or in the spectrogram domain <ref type="bibr" coords="1,340.92,491.26,103.18,12.00" target="#b70">(Vasquez &amp; Lewis, 2019)</ref>. The key bottleneck is that modeling the raw audio directly introduces extremely long-range dependencies, making it computationally challenging to learn the high-level semantics of music. A way to reduce the difficulty is to learn a lower-dimensional encoding of the audio with the goal of losing the less important information but retaining most of the musical information. This approach has demonstrated some success in generating short instrumental pieces restricted to a set of a few instruments <ref type="bibr" coords="1,356.43,598.86,73.32,12.00" target="#b46">(Oord et al., 2017;</ref><ref type="bibr" coords="1,432.24,598.86,87.02,12.00" target="#b14">Dieleman et al., 2018)</ref>.</p><p xml:id="_XAmPVXj">In this work, we show that we can use state-of-the-art deep generative models to produce a single system capable of generating diverse high-fidelity music in the raw audio domain, with long-range coherence spanning multiple minutes. Our approach uses a hierarchical VQ-VAE architecture <ref type="bibr" coords="1,510.75,664.61,30.69,12.00;2,55.44,67.82,48.77,12.00" target="#b56">(Razavi et al., 2019)</ref> to compress audio into a discrete space, with a loss function designed to retain the maximum amount of musical information, while doing so at increasing levels of compression. We use an autoregressive Sparse Transformer <ref type="bibr" coords="2,84.21,115.64,72.80,12.00" target="#b11">(Child et al., 2019;</ref><ref type="bibr" coords="2,159.21,115.64,81.34,12.00" target="#b71">Vaswani et al., 2017)</ref> trained with maximum-likelihood estimation over this compressed space, and also train autoregressive upsamplers to recreate the lost information at each level of compression.</p><p xml:id="_ar5fcj5">We show that our models can produce songs from highly diverse genres of music like rock, hip-hop, and jazz. They can capture melody, rhythm, long-range composition, and timbres for a wide variety of instruments, as well as the styles and voices of singers to be produced with the music. We can also generate novel completions of existing songs. Our approach allows the option to influence the generation process: by swapping the top prior with a conditional prior, we can condition on lyrics to tell the singer what to sing, or on midi to control the composition. We release our model weights and training and sampling code at https://github.com/openai/jukebox.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2." xml:id="_fKfFWG2">Background</head><p xml:id="_Cx2zsYx">We consider music in the raw audio domain represented as a continuous waveform x ∈ [−1, 1] T , where the number of samples T is the product of the audio duration and the sampling rate typically ranging from 16 kHz to 48 kHz. For music, CD quality audio, 44.1 kHz samples stored in 16 bit precision, is typically enough to capture the range of frequencies perceptible to humans. As an example, a fourminute-long audio segment will have an input length of ∼10 million, where each position can have 16 bits of information. In comparison, a high-resolution RGB image with 1024 × 1024 pixels has an input length of ∼3 million, and each position has 24 bits of information. This makes learning a generative model for music extremely computationally demanding with increasingly longer durations; we have to capture a wide range of musical structures from timbre to global coherence while simultaneously modeling a large amount of diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1." xml:id="_ceXCaUp">VQ-VAE</head><p xml:id="_GyypXyM">To make this task feasible, we use the VQ-VAE <ref type="bibr" coords="2,243.46,583.95,47.22,12.00;2,55.44,595.90,22.24,12.00" target="#b46">(Oord et al., 2017;</ref><ref type="bibr" coords="2,79.62,595.90,85.01,12.00" target="#b14">Dieleman et al., 2018;</ref><ref type="bibr" coords="2,166.58,595.90,74.50,12.00" target="#b56">Razavi et al., 2019)</ref> to compress raw audio to a lower-dimensional space. A one-dimensional VQ-VAE learns to encode an input sequence x = x t T t=1 using a sequence of discrete tokens z = z s ∈ [K] S s=1 , where K denotes the vocabulary size and we call the ratio T /S the hop length. It consists of an encoder E(x) which encodes x into a sequence of latent vectors h = h s S s=1 , a bottleneck that quantizes h s → e zs by mapping each h s to its nearest vector e zs from a codebook C = {e k } K k=1 , and a decoder D(e) that decodes the embedding vectors back to the input space. It is thus an auto-encoder with a discretization bottleneck. The VQ-VAE is trained using the following objective:</p><formula xml:id="formula_0" coords="2,347.35,114.92,194.09,39.19">L = L recons + L codebook + βL commit (1) L recons = 1 T t x t − D(e zt ) 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_YZxvs9J">2</head><p xml:id="_2MXvaBK">(2)</p><formula xml:id="formula_1" coords="2,338.44,157.47,140.62,18.70">L codebook = 1 S s sg [h s ] − e zs 2 2</formula><p xml:id="_k8DeVud">(3)</p><formula xml:id="formula_2" coords="2,344.24,179.20,197.20,19.03">L commit = 1 S s h s − sg [e zs ] 2 2 (4)</formula><p xml:id="_zrXKmyK">where sg denotes the stop-gradient operation, which passes zero gradient during backpropagation. The reconstruction loss L recons penalizes for the distance between the input x and the reconstructed output x = D(e z ), and L codebook penalizes the codebook for the distance between the encodings h and their nearest neighbors e z from the codebook. To stabilize the encoder, we also add L commit to prevent the encodings from fluctuating too much, where the weight β controls the amount of contribution of this loss. To speed up training, the codebook loss L codebook instead uses EMA updates over the codebook variables. <ref type="bibr" coords="2,458.63,327.21,83.48,12.00" target="#b56">Razavi et al. (2019)</ref> extends this to a hierarchical model where they train a single encoder and decoder but break up the latent sequence</p><formula xml:id="formula_3" coords="2,307.44,363.07,197.64,12.00">h into a multi-level representation [h (1) , • • • , h (L)</formula><p xml:id="_Y37aTH4">] with decreasing sequence lengths, each learning its own codebook C (l) . They use non-autoregressive encoder-decoders and jointly train all levels with a simple mean-squared loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3." xml:id="_9DRMMTe">Music VQ-VAE</head><p xml:id="_aFAM6pK">Inspired by the results from the hierarchical VQ-VAE model <ref type="bibr" coords="2,307.11,458.83,82.20,12.00" target="#b56">(Razavi et al., 2019)</ref> for images, we consider applying the same technique to model raw audio using three different levels of abstraction, as illustrated in Figure <ref type="figure" coords="2,479.22,482.74,3.66,12.00" target="#fig_0">1</ref>. At each level, we use residual networks consisting of WaveNet-style noncausal 1-D dilated convolutions, interleaved with downsampling and upsampling 1-D convolutions to match different hop lengths. A detailed description of the architecture is provided in Appendix B.1. We make a number of modifications to our VQ-VAE compared to the ones in <ref type="bibr" coords="2,494.08,554.47,48.61,12.00;2,307.44,566.43,23.15,12.00" target="#b46">(Oord et al., 2017;</ref><ref type="bibr" coords="2,333.88,566.43,79.52,12.00" target="#b56">Razavi et al., 2019)</ref>, as described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1." xml:id="_fWN5uPD">Random restarts for embeddings</head><p xml:id="_4Z98wxe">VQ-VAEs are known to suffer from codebook collapse, wherein all encodings get mapped to a single or few embedding vectors while the other embedding vectors in the codebook are not used, reducing the information capacity of the bottleneck. To prevent this, we use random restarts: when the mean usage of a codebook vector falls below a threshold, we randomly reset it to one of the encoder outputs from the current batch. This ensures all vectors in the Audio can be reconstructed using the codes at any one of the abstraction levels, where the least abstract bottom-level codes result in the highest-quality audio, as shown in Figure <ref type="figure" coords="3,205.90,300.31,3.36,10.80" target="#fig_4">4</ref>. For the detailed structure of each component, see Figure <ref type="figure" coords="3,419.46,300.31,3.36,10.80" target="#fig_7">7</ref>.</p><p xml:id="_XtntsTE">codebook are being used and thus have a gradient to learn from, mitigating codebook collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2." xml:id="_xv53zkn">Separated Autoencoders</head><p xml:id="_rsfa4Zd">When using the hierarchical VQ-VAE from <ref type="bibr" coords="3,234.39,386.96,56.30,12.00;3,55.44,398.92,23.15,12.00" target="#b56">(Razavi et al., 2019)</ref> for raw audio, we observed that the bottlenecked top level is utilized very little and sometimes experiences a complete collapse, as the model decides to pass all information through the less bottlenecked lower levels. To maximize the amount of information stored at each level, we simply train separate autoencoders with varying hop lengths. Discrete codes from each level can be treated as independent encodings of the input at different levels of compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3." xml:id="_mTawHAt">Spectral Loss</head><p xml:id="_JwmcF4w">When using only the sample-level reconstruction loss, the model learns to reconstruct low frequencies only. To capture mid-to-high frequencies, we add a spectral loss which is defined as</p><formula xml:id="formula_4" coords="3,100.03,582.63,144.32,11.18">L spec = |STFT(x)| − |STFT( x)| 2</formula><p xml:id="_NGNphSy">It encourages the model to match the spectral components without paying attention to phase which is more difficult to learn. This is similar to the use of power loss <ref type="bibr" coords="3,265.17,622.17,24.27,12.00;3,55.44,634.12,48.74,12.00" target="#b47">(Oord et al., 2018)</ref> and spectral convergence <ref type="bibr" coords="3,211.64,634.12,78.47,12.00" target="#b3">(Arık et al., 2018b)</ref> when training parallel decoders for raw audio. One difference between the latter approach and ours is that we are no longer optimizing the spectral signal-to-noise ratio; dividing by the magnitude of the signal results in numerical instability for mostly silent inputs. To prevent the model from overfitting to a particular choice of the STFT parameters, we use the sum of the spectral losses L spec calculated over multiple STFT parameters that trade-off time and frequency resolutions <ref type="bibr" coords="3,353.66,355.13,94.88,12.00" target="#b75">(Yamamoto et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4." xml:id="_tbyPGDW">Music Priors and Upsamplers</head><p xml:id="_AMUZ5d7">After training the VQ-VAE, we need to learn a prior p(z) over the compressed space to generate samples. We break up the prior model as</p><formula xml:id="formula_5" coords="3,319.55,452.25,221.89,29.77">p(z) = p(z top , z middle , z bottom ) (5) = p(z top )p(z middle |z top )p(z bottom |z middle , z top ) (6)</formula><p xml:id="_WuZQPWy">and train separate models for the top-level prior p(z top ), and upsamplers p(z middle |z top ) and p(z bottom |z middle , z top ). Each of these is an autoregressive modeling problem in the discrete token space produced by the VQ-VAE. We use Transformers with sparse attention <ref type="bibr" coords="3,428.14,544.46,88.19,12.00" target="#b71">(Vaswani et al., 2017;</ref><ref type="bibr" coords="3,518.85,544.46,22.59,12.00;3,307.44,556.42,45.91,12.00" target="#b11">Child et al., 2019)</ref> as they are currently the SOTA in autoregressive modeling. We propose a simplified version which we call the Scalable Transformer, that is easier to implement and scale (see Appendix A for details).</p><p xml:id="_aVaSEkH">For the upsamplers, we need to provide the autoregressive Transformers with conditioning information from the codes of the upper levels. To do so, we use a deep residual WaveNet <ref type="bibr" coords="3,361.45,646.08,68.24,12.00" target="#b74">(Xie et al., 2017)</ref> followed by an upsampling strided convolution and a layer norm <ref type="bibr" coords="3,457.12,658.03,63.03,12.00" target="#b4">(Ba et al., 2016)</ref>, and add the output as extra positional information to the embeddings of the current level. We condition the lower levels only on the chunk of upper level codes that correspond to the same segment of raw audio.</p><p xml:id="_5CsvpQq">At each level, we use Transformers over the same context length of discrete codes, which correspond to increasing the raw audio length with larger hop lengths, and modeling longer temporal dependencies at the higher levels while keeping the same computational footprint for training each level. As our VQ-VAE is convolutional, we can use the same VQ-VAE to produce codes for arbitrary lengths of audio.</p><p xml:id="_hfNQhYP">4.1. Artist, Genre, and Timing Conditioning</p><p xml:id="_TWfdu2X">Our generative model can be made more controllable by providing additional conditioning signals while training. For our first models, we provide artist and genre labels for the songs. This has two advantages: first, it reduces the entropy of the audio prediction, so the model is able to achieve better quality in any particular style. Second, at generation time, we are able to steer the model to generate in a style of our choosing. Additionally, we attach a timing signal for each segment at training time. This signal includes the total duration of the piece, the start time of that particular sample and how much fraction of the song that has elapsed. This allows the model to learn audio patterns that depend on the overall structure, such as spoken or instrumental introductions and applause at the end of a piece.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2." xml:id="_PcsVc3d">Lyrics Conditioning</head><p xml:id="_d8gAEDa">While the conditioned models above are able to generate songs of diverse genres and artistic styles, singing voices generated by those models, while often sung in a compelling melody, are mostly composed of babbling, rarely producing recognizable English words. In order to be able to control the generative model with lyrics, we provide more context at training time by conditioning the model on the lyrics corresponding to each audio segment, allowing the model to produce singing simultaneosly with the music.</p><p xml:id="_Z4NJbq2">Lyrics-to-singing (LTS) task: The conditioning signal only includes the text of the lyrics, without timing or vocalisation information. We thus have to model the temporal alignment of lyrics and singing, the artists voice and also the diversity of ways one can sing a phrase depending on the pitch, melody, rhythm and even genre of the song. The conditioning data isn't precise as the lyrics data often contains textual references to repeated sections like "chorus" or mismatching portions of lyrics with the corresponding music.</p><p xml:id="_Qsknz4d">There is also no separation between lead vocals, accompanying vocals and the background music in target audio. This makes the Lyrics-to-singing (LTS) task significantly more challenging than the corresponding Text-to-speech (TTS) task.</p><p xml:id="_jrgEP8V">Providing lyrics for chunks of audio: Our dataset includes song-level lyrics, but to make the task easier we train on shorter (24 sec) chunks of audio. To provide the lyrics cor-   responding to the audio during training, we began with a simple heuristics of aligning the characters of the lyrics to linearly span the duration of each song, and pass a fixed-side window of characters centered around the current segment during training. While this simple strategy of linear alignment worked surprisingly well, we found that it fails for certain genres such as hip-hop with fast lyrics. To address this, we use Spleeter <ref type="bibr" coords="5,139.44,151.50,96.65,12.00">(Hennequin et al., 2019)</ref> to extract vocals from each song and run NUS AutoLyricsAlign <ref type="bibr" coords="5,261.86,163.46,27.58,12.00;5,55.44,175.41,48.20,12.00" target="#b24">(Gupta et al., 2020)</ref> on the extracted vocals to obtain a word-level alignments of the lyrics, allowing us to more accurately provide the lyrics for a given chunk of audio. We choose a large enough window so that the actual lyrics have a high probability of being inside the window.</p><p xml:id="_q4jw3U7">Encoder-decoder model: We use an encoder-decoder style model to condition on the characters of the lyrics, with the encoder producing features from the lyrics which are attended to by the decoder which produces the top level music tokens. The lyrics encoder is a Transformer with an autoregressive modeling loss for lyrics, and its last level is used as features of the lyrics. In the music decoder, we interleave a few additional layers with encoder-decoder attention where the queries from the music tokens are only allowed to attend to keys and values from the lyrics tokens. These layers attend on the activation from the last layer of the lyrics encoder (see Figure <ref type="figure" coords="5,161.98,372.67,7.67,12.00">8c</ref>). In Figure <ref type="figure" coords="5,220.02,372.67,3.77,12.00" target="#fig_3">3</ref>, we see that the attention pattern learned by one of these layers corresponds to the alignment between the lyrics and the singing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3." xml:id="_7suF7ff">Decoder Pretraining</head><p xml:id="_9ezxJg8">To reduce computation required to train the lyrics conditional model, we use a pretrained unconditional top-level prior as our decoder and introduce the lyrics encoder using model surgery <ref type="bibr" coords="5,113.84,476.24,77.16,12.00" target="#b5">(Berner et al., 2019)</ref>. We initialize the output projection weights in the MLP and the attention layers of these residual blocks to zeros <ref type="bibr" coords="5,174.79,500.15,81.53,12.00" target="#b78">(Zhang et al., 2019a)</ref>, so that the added layers perform the identity function at initialization. Thus, at initialization the model behaves identically as the pretrained decoder, but there is still a gradient with respect to the encoder state and parameters<ref type="foot" coords="5,230.02,547.11,3.49,8.40" target="#foot_1">2</ref> , allowing the model to learn to use the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4." xml:id="_GDjgjxG">Sampling</head><p xml:id="_UtZbTas">After we have trained our VQ-VAE, upsamplers, and top level priors, we can then use them to sample novel songs.</p><p xml:id="_uUyUcAn">Ancestral sampling: We first generate the top level codes one token at a time by the usual ancestral sampling process (see Figure <ref type="figure" coords="5,102.65,657.51,7.84,12.00" target="#fig_2">2a</ref>): generating the first token, then passing all previously generated tokens into the model as inputs and outputting the next token conditioned on all previous tokens. We then run our conditioning wavenet on the top level codes to produce the conditioning information for the middle level and sample ancestrally from it too, and do the same for the bottom level.</p><p xml:id="_hBDx3bK">Windowed sampling: To sample segments longer than the context length, we use windowed sampling, where we move ahead our sampling window by half our context and continue sampling conditioned on this half context (See Figure <ref type="figure" coords="5,307.44,398.22,8.04,12.00" target="#fig_2">2b</ref>). We can trade off speed for quality by using a smaller hop length here.</p><p xml:id="_hhx2nda">Primed sampling: Instead of sampling the entire token sequence from the model, we can also run a forward pass of the VQ-VAE to obtain the top, middle, and bottom level codes corresponding to a segment from an actual song, as shown in Figure <ref type="figure" coords="5,372.80,475.93,7.77,12.00" target="#fig_2">2c</ref>. We can use these as the initial tokens in our ancestral sampling process and continue sampling from these to produce novel completions of the song.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5." xml:id="_DV2mw3t">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1." xml:id="_ynmxYwQ">Dataset</head><p xml:id="_qBEpJPg">We scraped a new dataset of 1.2 million songs (600k of which in English), paired with the lyrics and metadata from LyricWiki (LyricWiki). The metadata includes artist, album, genre, and year of the release, along with common moods or playlist keywords associated with each song. We train on 32 bit, 44.1 kHz raw audio and perform data augmentation by randomly downmixing the right and left channels to produce mono channel audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2." xml:id="_QTKaqeX">Training Details</head><p xml:id="_ZudZJpa">For the music VQ-VAE, we use 3 levels of bottlenecks compressing 44 kHz audio in dimensionality by 8x, 32x, and 128x respectively, with a codebook size of 2048 for each level. The VQ-VAE has 2 million parameters and is trained on 9-second audio clips on 256 V100 for 3 days. We used exponential moving average to update the codebook following <ref type="bibr" coords="6,86.28,115.64,79.21,12.00" target="#b56">Razavi et al. (2019)</ref>. For our prior and upsampler models, we use a context of 8192 tokens of VQ-VAE codes, which corresponds to approximately 24, 6, and 1.5 seconds of raw audio at the top, middle, and bottom level, respectively. The upsamplers have one billion parameters and are trained on 128 V100s for 2 weeks, and the top-level prior has 5 billion parameters and is trained on 512 V100s for 4 weeks. We use Adam with learning rate 0.00015 and weight decay of 0.002. For lyrics conditioning, we reuse the prior and add a small encoder, after which we train the model on 512 V100s for 2 weeks. The detailed hyperparameters for our models and training are provided in Appendix B.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3." xml:id="_4ftungq">Samples</head><p xml:id="_8qpCcKe">We trained a sequence of models with increasing sample quality. Our first model was trained on the MAESTRO dataset using 22 kHz VQ-VAE codes and relatively small prior models. We observed that this could generate high fidelity classical music samples with piano and occasional violin. We then collected a larger and more diverse dataset of songs with genre and artist labels. The same model when trained on this new dataset was able to produce diverse samples other than classical music, and demonstrated musicality and coherence over more than a minute.</p><p xml:id="_DHbSKNd">Despite the novelty of being able to generate generally high fidelity and coherent songs, sample quality was still limited by a number of factors. First, the use of 22 kHz sampling rate along with small upsamplers introduced noise both in the upsampling and decoding steps, which we hear as grainy texture. We improved fidelity by using 44 kHz VQ-VAE and 1B parameter upsamplers in all subsequent experiments at the expense of longer rendering time.</p><p xml:id="_ZCbBxfY">Second, the 1B top-level prior was not big enough to produce singing and diverse musical timbres. We first explored increasing the model size to 5 billion parameters. Larger capacity allowed better modeling of the broader distribution of songs, resulting in samples with better musicality, longer coherence and initial singing. While there is an overall qualitative improvement, the unconditional model still struggled to sing recognizable words. Training a seq2seq model with lyric conditioning and limiting the dataset only to songs primarily in English made singing both intelligible and controllable.</p><p xml:id="_aaN9ptM">The final model, which we call Jukebox, uses all these improvements. Because everyone experiences music differently, it is generally tricky and not very meaningful to evaluate samples by the mean opinion score or FID-like metrics. We manually evaluate coherence, musicality, diver-sity, and novelty of generated samples. The links to curated examples are embedded in text.</p><p xml:id="_TUd4T5y">Coherence: We find the samples stay very coherent musically through the context length of the top-level prior (approximately 24 seconds), and they maintain similar harmonies and textures as we slide the window to generate longer samples. However, because the top-level does not have the context of the entire song, we do not hear long term musical patterns, and we would never hear choruses or melodies that repeat.</p><p xml:id="_UHC6Ty6">The generations progress through beginnings of songs (for example applause or slow instrumental warm-ups), through sections that sound chorus-like, through instrumental interludes, and then fading or otherwise wrapping up at the end.</p><p xml:id="_NcMnyRu">The top-level prior always knows what fraction of the song is complete time-wise, so it is able to imitate appropriate beginnings, middles and ends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_UnkVTa7">Musicality:</head><p xml:id="_qe2F32T">The samples frequently imitate familiar musical harmonies and the lyrics are usually set in ways that are very natural. Frequently the highest or longest notes of the melody match words that a human singer would choose to emphasize, and the lyrics are almost always rendered in ways that capture the prosody of the phrases. This is noticeable in hip hop generations, where the model reliably captures the rhythm of spoken text. We do find that the generated melodies are usually less interesting than human composed melodies. In particular, we do not hear the antecedent and consequent pattern familiar to many human melodies, and we rarely hear choruses that are melodically memorable.</p><p xml:id="_HzHrjqV">Diversity: Likelihood training encourages covering of all modes, so we expect the model to produce diverse samples.</p><p xml:id="_Anc96u8">-Re-renditions: We generate multiple samples conditioned on artist and lyrics combinations that exist in our training data. While occasionally drum and bass lines or melodic intervals echo the original versions, we find that none of the generated samples is noticeably similar to the original songs.</p><p xml:id="_bpGk8au">We also generate multiple songs conditioned on the same artist and lyrics as Sample 1 to obtain Samples 9-12. All five sound interesting in their own ways with different moods and melodies with Sample 10 playing a harmonic at 00:14 as part of a blues riff, showing that the model has learned a wide range of singing and playing styles.</p><p xml:id="_Wwpqh9K">-Completions: We prime the model with 12 seconds of existing songs and ask it to complete them in the same styles. When the priming samples include singing, the continuations are more likely to imitate the original tunes and rhythms. Songs primed with more generic or common intros tend to be more diverse. Even generated samples that are close to the originals early on deviate completely into new musical material after about 30 seconds.</p><p xml:id="_KM6fkJd">Re-renditions and completions are interesting and diverse, but overall, there is still room for improvement in music quality compared to the original songs.</p><p xml:id="_9fVbKQz">-Full tree: To understand diversity in a more systematic way, we generate multiple continuations from the same segment. We start with a one-minute sample and independently sample four times per one-minute extension. By the three minute mark, there are 16 completions. We can think of this branching tree as exploring different possibilities obtained by ancestral sampling. In the generated songs in the link, we hear diversity in singing and development even when the same initial segment is used. We note that this particular sample follows the lyrics more successfully than many. For certain genres like hip hop and rap, where linearly moving the window does not yield good lyrics alignment, the chance of obtaining plausible singing is lower.</p><p xml:id="_SgKhp7E">Novelty: With the ability to condition on various styles, lyrics, and raw audio, we would like Jukebox to be a useful tool for both professional musicians and music enthusiasts alike. In this section, we are interested in exploring capabilities and applications of Jukebox.</p><p xml:id="_4VGyART">-Novel styles: We generate songs in an unusual genre typically not associated with an artist. In general, we find that it is fairly difficult to generalize to a novel style of singing while using the same voice as the artist embedding overpowers other information. In Joe Bonamassa and Frank Sinatra samples, we hear a modest variation in instrumentation, energy, and ambience depending on the genre embedding. However, our attempts to mix country singer Alan Jackson with unusual genres like hip hop and punk did not seem to move the samples away from a country style in meaningful ways.</p><p xml:id="_5GHNwTY">-Novel voices: We pick artists whose voices are reproduced reasonably well by the model, and interpolate their style embeddings to synthesize new voices. Some blending, for instance, between Frank Sinatra and Alan Jackson in Sample 4, still sounds similar to Frank Sinatra. In most cases, the model renders in a vaguely recognizable but distinct voice that preserves different vocal attributes. Samples 1 and 2 conditioned on the Céline Dion embeddings divided by two have slightly different timbre and tone but capture her unique vibrato.</p><p xml:id="_UZh675m">We also experiment with changing the style embedding in the middle of a song to create a duet (Sample 7). This is another way of guiding generation during sampling. Continuing in another voice works best when the segment ends in an interlude; otherwise, the model blends voices in the middle of a word or a sentence.</p><p xml:id="_gAQC6hp">-Novel lyrics: We ask Jukebox to sing poems and novel verses generated by GPT-2 <ref type="bibr" coords="7,417.49,79.77,61.54,12.00">(Radford et al.)</ref> to demonstrate that it can indeed sing new lyrics. While the training data consists of song lyrics with limited vocabulary and constrained structure, the model has learned to follow along most prompts and sing even new words that are reasonably pronounceable (including technical terms from the deep learning literature). To get the best results, however, we find that it is useful to spell out difficult words or acronyms as they are spoken. The generations are noticeably higher quality if the text matches the distribution of lyrics for the given artist, both in terms of length, and of rhyming or rhythmic qualities. For example, hip hop lyrics tend to be longer than most other genres, and the commonly emphasized syllables easily form clear rhythms.</p><p xml:id="_WmXPzmX">-Novel riffs: Another useful application of Jukebox is the ability to record an incomplete idea and explore various continuations without ever needing to tabulate in symbolic representations, which would lose details of timbre and mood. We curate recordings of novel riffs by our in-house musicians and prime the model during sampling. Sample 6 starts with a musical style not widely used in Elton John's songs. The model still carries out the tune and develops it further. Similarly, the beginning of Sample 1 is a progressive jazz piece with a 5/4 polymeter, which has never been used in hip hop. Despite this novelty, the rhythm persists throughout the song and is incorporated naturally with rapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4." xml:id="_AYSkmD5">VQ-VAE Ablations</head><p xml:id="_gAKteBy">Spectral  The first row is the ground-truth, and the second row shows the spectrograms of audio outputs from our VQ-VAE. In the third row, we remove the spectral loss, and see that the middle and top level lose high-frequency information. In the fourth row, we use a hierarchical VQ-VAE <ref type="bibr" coords="8,89.73,493.64,71.20,10.80" target="#b56">(Razavi et al., 2019)</ref> instead of separate auto-encoders (Figure <ref type="figure" coords="8,310.01,493.64,3.17,10.80" target="#fig_0">1</ref>), and we see the middle and top levels are not used for encoding pertinent information. Finally, the fifth row shows a baseline with the Opus codec that encodes audio at constant bitrates comparable to our VQ-VAE. It also fails to capture higher frequencies and adds noticeable artifacts at the highest level of compression. We compare raw audio VQ-VAEs when trained with varying compression ratios, objectives, and architectures. As we use nonautoregressive decoders with continuous representation for output, we report spectral convergence <ref type="bibr" coords="8,254.01,705.86,35.43,12.00;8,307.05,546.47,67.33,12.00" target="#b65">(Sturmel &amp; Daudet, 2011)</ref>, which measures the amount of spectral error relative to signal, as test error and proxy for reconstruction fidelity. We evaluate on 5000 held-out 3-second audio segments and report the average in decibels. All models in this section are trained with a batch size of 32, 3-second audio clips sampled at 44 kHz. As before, we use hop lengths of 8, 32, and 128 for the bottom, middle and top level respectively.</p><p xml:id="_n6nnssE">In Table <ref type="table" coords="8,344.72,648.09,3.81,12.00" target="#tab_1">1</ref>, we see that increasing the hop size results in higher reconstruction error. Figure <ref type="figure" coords="8,452.79,660.04,5.08,12.00" target="#fig_4">4</ref> indeed shows that a significant amount of information, especially higher frequencies, is missing at middle and top levels across all ablations we ran. This is expected as audio is compressed more with larger hop sizes. To mitigate codebook collapse, we restart dead codes near random encoder embeddings. In Figure <ref type="figure" coords="9,283.15,281.45,3.77,12.00" target="#fig_5">5</ref>, we see that this yields higher codebook usage even from early on in training. Models trained without random restarts can converge to the same test error and codebook usage but require more training steps. With poor initialization, these models sometimes end up with suboptimal codes hurting reconstruction fidelity.</p><p xml:id="_yQCECR6">Codebook size also matters, as it sets a limit on channel capacity through the bottleneck layers. In Table <ref type="table" coords="9,234.10,383.07,3.66,12.00" target="#tab_2">2</ref>, we find that reconstruction error increases considerably when the codebook size is reduced from 2048 to 256. We also compare with a model that uses continuous representations without vector quantization. We can think of this model as using a vastly large codebook with all encoder embeddings. This achieves almost perfect reconstruction with negligible spectral error.</p><p xml:id="_JrnjBVM">When the model is trained with L2 loss only, reconstructions tend to sound muddy from missing high frequencies, and this problem is exacerbated as hop size is increased. In Figure <ref type="figure" coords="9,84.40,520.55,3.80,12.00" target="#fig_4">4</ref>, we see that top-level codes trained without spectral loss do not capture much information beyond 2 kHz, and obtain worse reconstructions (Table <ref type="table" coords="9,221.20,544.46,3.67,12.00" target="#tab_3">3</ref>). However, we observe that while spectral loss helps encode more information, it also adds distortion artifacts which we hear as scratchy noise.</p><p xml:id="_aPkkCqh">Lastly, we train a raw audio hierarchical VQ-VAE <ref type="bibr" coords="9,258.43,598.26,31.01,12.00;9,55.44,610.21,49.93,12.00" target="#b56">(Razavi et al., 2019)</ref> and find that it is generally difficult to push information to higher levels. This model is trained twice as long as the previous models, but middle and top-level reconstructions as shown in Figure <ref type="figure" coords="9,171.75,646.08,4.88,12.00" target="#fig_4">4</ref> are not capturing much. It is possible that higher level codes may have collapsed before bottom level starts to reconstruct the audio well. Making the bottom layers explicitly model residuals pushed more information to the top. But, we found separate autoencoders to be cleaner and more effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6." xml:id="_Zgkf288">Related Work</head><p xml:id="_rqhPYNw">Generative modeling in deep learning: Generative models aim to learn the distribution of data by either explicitly by modeling the distribution or implicitly by constructing means to sample from it <ref type="bibr" coords="9,412.35,123.06,79.47,12.00" target="#b22">(Goodfellow, 2016)</ref>. Modeling the interdependency within high-dimensional data was traditionally considered extremely difficult, but starting with Deep Boltzmann Machines <ref type="bibr" coords="9,414.83,158.92,123.66,12.00" target="#b61">(Salakhutdinov &amp; Hinton, 2009)</ref>, various kinds of deep generative models have been introduced. Generative Adversarial Networks (GANs) <ref type="bibr" coords="9,513.75,182.83,24.46,12.00;9,307.44,194.79,80.39,12.00" target="#b23">(Goodfellow et al., 2014)</ref> use generator and discriminator networks that contest each other to make the generated samples as indistinguishable as possible from the data, and they are renowned for their ability to generate high-quality pictures <ref type="bibr" coords="9,329.35,242.61,83.49,12.00" target="#b79">(Zhang et al., 2019b;</ref><ref type="bibr" coords="9,415.33,242.61,72.80,12.00" target="#b9">Brock et al., 2019)</ref>. Autoregressive generative models such as NADE <ref type="bibr" coords="9,466.72,254.56,71.60,12.00" target="#b68">(Uria et al., 2016)</ref>, PixelCNN (Van den <ref type="bibr" coords="9,391.73,266.52,70.73,12.00" target="#b45">Oord et al., 2016)</ref>, and Transformers <ref type="bibr" coords="9,307.11,278.47,89.71,12.00" target="#b71">(Vaswani et al., 2017)</ref> use the chain rule of probability to factorize the joint distribution of data into a product of simpler distributions, and flow-based models <ref type="bibr" coords="9,493.60,302.38,49.09,12.00;9,307.44,314.34,22.38,12.00" target="#b15">(Dinh et al., 2015;</ref><ref type="bibr" coords="9,332.30,314.34,22.38,12.00">2017;</ref><ref type="bibr" coords="9,357.15,314.34,113.57,12.00" target="#b57">Rezende &amp; Mohamed, 2015;</ref><ref type="bibr" coords="9,473.21,314.34,65.62,12.00;9,307.08,326.29,38.53,12.00" target="#b36">Kingma &amp; Dhariwal, 2018</ref>) learn a series of invertible transformations that maps the data distribution with a simpler one such as a Gaussian distribution. Autoregressive flows <ref type="bibr" coords="9,482.80,350.20,58.64,12.00;9,307.44,362.16,47.10,12.00" target="#b49">(Papamakarios et al., 2017;</ref><ref type="bibr" coords="9,357.03,362.16,82.27,12.00" target="#b38">Kingma et al., 2016)</ref> combine the two ideas to achieve faster density estimation or data generation. Variational autoencoders (VAEs) <ref type="bibr" coords="9,419.75,386.07,87.13,12.00" target="#b58">(Rezende et al., 2014;</ref><ref type="bibr" coords="9,509.37,386.07,32.07,12.00;9,307.05,398.03,68.53,12.00" target="#b37">Kingma &amp; Welling, 2014</ref>) impose a Gaussian prior on the latent code in an encoder-decoder setup from which data can be sampled.</p><p xml:id="_9U2Gcuy">Generative models for music: Generative modeling of symbolic music dates back to more than half a century, when Hiller Jr &amp; Isaacson (1957) introduced the first computergenerated music based on Markov chains. There exists a variety of earlier approaches using rule-based systems <ref type="bibr" coords="9,307.11,499.64,59.86,12.00" target="#b43">(Moorer, 1972)</ref>, chaos and self-similarity <ref type="bibr" coords="9,474.62,499.64,63.76,12.00" target="#b54">(Pressing, 1988)</ref>, cellular automata <ref type="bibr" coords="9,382.40,511.60,54.41,12.00" target="#b6">(Beyls, 1989)</ref>, concatenative synthesis <ref type="bibr" coords="9,307.11,523.55,52.79,12.00" target="#b32">(Jehan, 2005)</ref>, and constraint programming <ref type="bibr" coords="9,482.92,523.55,55.16,12.00;9,307.44,535.51,49.20,12.00" target="#b1">(Anders &amp; Miranda, 2011)</ref>. More recent data-driven approaches include DeepBach <ref type="bibr" coords="9,350.32,547.46,86.37,12.00" target="#b25">(Hadjeres et al., 2017)</ref> and Coconet <ref type="bibr" coords="9,489.99,547.46,52.70,12.00;9,307.44,559.42,23.71,12.00" target="#b28">(Huang et al., 2017)</ref> which use Gibbs sampling to produce notes in the style of Bach chorals, MidiNet <ref type="bibr" coords="9,442.81,571.38,79.93,12.00" target="#b76">(Yang et al., 2017)</ref> and MuseGAN <ref type="bibr" coords="9,355.58,583.33,80.06,12.00" target="#b18">(Dong et al., 2018)</ref> which use generative adversarial networks, MusicVAE <ref type="bibr" coords="9,436.02,595.29,87.68,12.00" target="#b59">(Roberts et al., 2018)</ref> and HRNN <ref type="bibr" coords="9,338.87,607.24,68.97,12.00" target="#b73">(Wu et al., 2019)</ref> which use hierarchical recurrent networks, and Music Transformer <ref type="bibr" coords="9,452.64,619.20,89.47,12.00" target="#b29">(Huang et al., 2019a)</ref> and MuseNet <ref type="bibr" coords="9,363.37,631.15,55.79,12.00" target="#b50">(Payne, 2019)</ref> which use Transformers to autoregressively predict MIDI note events. There also have been a number of approaches for synthesizing music conditioned on symbolic music information, such as NSynth <ref type="bibr" coords="9,307.11,678.97,84.62,12.00" target="#b19">(Engel et al., 2017)</ref> which uses WaveNet-style autoencoder, Mel2Mel <ref type="bibr" coords="9,374.79,690.93,72.19,12.00" target="#b35">(Kim et al., 2019)</ref> and Wave2Midi2Wave (Hawthorne et al., 2019) which synthesize music using WaveNet conditioned on a piano roll representation, and GanSynth <ref type="bibr" coords="10,97.60,79.77,76.33,12.00" target="#b20">(Engel et al., 2019)</ref> which uses generative adversarial networks to produce magnitude spectrograms together with instananeous frequencies for easier spectrogram inversion. Generative models for music can also be used for music style transfer, as seen in Midi-VAE <ref type="bibr" coords="10,228.70,127.59,61.99,12.00;10,55.44,139.55,22.78,12.00" target="#b10">(Brunner et al., 2018)</ref> which uses a variational autoencoder to transfer styles between classical and jazz music, LakhNES <ref type="bibr" coords="10,228.70,151.50,61.99,12.00;10,55.44,163.46,23.71,12.00" target="#b17">(Donahue et al., 2019)</ref> which uses a Transformer architecture to generate chiptune music, and Universal Music Translator Network <ref type="bibr" coords="10,55.11,187.37,70.71,12.00" target="#b44">(Mor et al., 2019)</ref> which uses a denoising autoencoder that can disentangle musical style and content.</p><p xml:id="_bv4RKCG">Sample-level generation of audio: In recent years, a variety of generative models for raw audio have been introduced. WaveNet <ref type="bibr" coords="10,92.60,241.17,70.78,12.00" target="#b45">(Oord et al., 2016)</ref> performs autoregressive sampleby-sample probabilistic modeling of raw waveform using a series of dilated convolutions to exponentially increase the context length. It can produce realistic audio either unconditionally or by conditioning on acoustic features or spectrograms. The autoregressive nature of WaveNet makes the sampling notoriously slow, and it uses a categorical distribution for audio samples which introduces quantization noise. Parallel WaveNet <ref type="bibr" coords="10,129.97,336.81,77.33,12.00" target="#b47">(Oord et al., 2018)</ref> improves upon this by instead using a mixture of logistics distribution, a continuous probability distribution, and performing probability density distillation which learns a parallel feed-forward network from a pre-trained autoregressive model, allowing faster sampling of high fidelity audio. ClariNet <ref type="bibr" coords="10,267.42,396.58,22.02,12.00;10,55.44,408.54,48.63,12.00" target="#b52">(Ping et al., 2019)</ref> achieves similar audio quality using a simple Gaussian distribution instead and thus having a closed-form loss function, eliminating the need for Monte-Carlo sampling. SampleRNN <ref type="bibr" coords="10,135.69,444.41,78.48,12.00" target="#b42">(Mehri et al., 2017)</ref> uses a multi-scale, hierarchical recurrent neural network with convolutional upsampling to model long-range complex structures. Wa-veRNN <ref type="bibr" coords="10,89.39,480.27,111.31,12.00" target="#b34">(Kalchbrenner et al., 2018)</ref> uses recurrent neural networks that operate separately on the most significant and the least significant bytes, which can be efficiently deployed in mobile devices while having comparable audio quality to WaveNet. WaveGlow <ref type="bibr" coords="10,143.32,528.09,79.59,12.00" target="#b53">(Prenger et al., 2019</ref>) is a flow-based model for parallel sample-level audio synthesis, which can be trained with a straightforward maximum-likelihood estimation and thus is advantageous to the two-stage training process needed for distillation. Parallel WaveGAN <ref type="bibr" coords="10,273.47,575.91,13.22,12.00;10,55.44,587.87,86.75,12.00" target="#b75">(Yamamoto et al., 2020)</ref> and MelGAN <ref type="bibr" coords="10,205.22,587.87,84.89,12.00" target="#b40">(Kumar et al., 2019)</ref> are GAN-based approaches directly modeling audio waveforms, achieving similar quality as WaveNet and WaveGlow models with significantly fewer parameters. While the approaches above serve as sophisticated generative models for raw audio to be conditioned on a compact and controllable representation of audio such as Mel spectrograms, Mel-Net <ref type="bibr" coords="10,72.29,671.55,101.30,12.00" target="#b70">(Vasquez &amp; Lewis, 2019)</ref>   <ref type="bibr" coords="10,329.08,372.67,80.03,12.00" target="#b56">(Razavi et al., 2019)</ref> do not suffer from this issue, and thus we use their approach.</p><p xml:id="_G4u5xMX">Speech synthesis: Producing natural human voice entails an understanding of linguistic features, mapping of sounds, and steerability of expression. Many text-to-speech (TTS) systems rely on highly engineered features <ref type="bibr" coords="10,487.06,438.43,51.25,12.00" target="#b39">(Klatt, 1980)</ref>, carefully curated sound segments <ref type="bibr" coords="10,448.65,450.38,89.66,12.00" target="#b31">(Hunt &amp; Black, 1996)</ref>, statistical parametric modeling <ref type="bibr" coords="10,436.53,462.34,69.65,12.00" target="#b77">(Zen et al., 2009)</ref>, and often complex pipelines as described in <ref type="bibr" coords="10,465.68,474.29,73.13,12.00" target="#b21">(Arık et al., 2017)</ref>. These approaches are fairly involved and produce unnatural or inarticulate voices. More recent works like Deep Voice 3 <ref type="bibr" coords="10,315.72,510.16,73.24,12.00" target="#b51">(Ping et al., 2018)</ref>, Tacotron 2 <ref type="bibr" coords="10,444.09,510.16,74.93,12.00" target="#b62">(Shen et al., 2018)</ref>, and Char2Wav <ref type="bibr" coords="10,352.27,522.11,79.17,12.00" target="#b64">(Sotelo et al., 2017)</ref> learn speech synthesis endto-end using sequence-to-sequence architecture <ref type="bibr" coords="10,499.37,534.07,42.25,12.00;10,307.44,546.02,44.45,12.00" target="#b66">(Sutskever et al., 2014)</ref>. The design space is vast, but in general, typical approaches comprise of a bidirectional encoder, a decoder, and a vocoder to build text representations, audio features, and the final raw waveforms. To generate multiple voices, text-to-speech models can also condition on the speaker identity <ref type="bibr" coords="10,340.12,605.80,72.78,12.00" target="#b45">(Oord et al., 2016;</ref><ref type="bibr" coords="10,415.39,605.80,89.65,12.00" target="#b21">Gibiansky et al., 2017;</ref><ref type="bibr" coords="10,507.53,605.80,35.15,12.00;10,307.44,617.76,23.71,12.00" target="#b33">Jia et al., 2018)</ref> as well as text prompt. By learning and manipulating auxiliary embeddings, models can mimic a new voice <ref type="bibr" coords="10,307.11,641.67,74.25,12.00" target="#b2">(Arık et al., 2018a;</ref><ref type="bibr" coords="10,383.85,641.67,83.23,12.00" target="#b67">Taigman et al., 2018)</ref> at test time. These methods, however, require labeled data. Ideas like clustering <ref type="bibr" coords="10,334.52,665.58,81.66,12.00" target="#b13">(Dehak et al., 2011)</ref>, priming <ref type="bibr" coords="10,459.71,665.58,78.61,12.00" target="#b72">(Wang et al., 2018)</ref>, and variational autoencoders <ref type="bibr" coords="10,428.24,677.53,71.97,12.00" target="#b27">(Hsu et al., 2019;</ref><ref type="bibr" coords="10,503.32,677.53,38.12,12.00;10,307.44,689.49,46.11,12.00" target="#b0">Akuzawa et al., 2018)</ref> have been used to learn broader styles of speech and control expressivity in an unsupervised way. There are also works on synthesizing singing by additionally controlling pitch and timbre. Similar to TTS literature, early works use concatenative methods <ref type="bibr" coords="11,192.87,91.73,97.24,12.00" target="#b8">(Bonada &amp; Serra, 2007)</ref> that join short segments of curated singing, and statistical parametric methods <ref type="bibr" coords="11,138.56,115.64,77.29,12.00" target="#b60">(Saino et al., 2006;</ref><ref type="bibr" coords="11,218.48,115.64,71.63,12.00" target="#b48">Oura et al., 2010)</ref> which allow modeling of timbre from training data. Both approaches impose fairly strong assumptions resulting in noticeable artifacts. <ref type="bibr" coords="11,134.43,151.50,102.06,12.00" target="#b7">(Blaauw &amp; Bonada, 2017)</ref> train a neural TTS model with a parametric vocoder to separate pitch and timbre which can be controlled at generation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7." xml:id="_bmmgwsz">Future work</head><p xml:id="_y22XZVX">While our approach represents a step forward in the ability to generate coherent long raw audio music samples, we recognize several directions for future work. Great music generation should be high quality over all time scales: it should have a developing musical and emotional structure across the entire piece, local notes and harmonies that always make sense, nuanced and appropriate small timbral and textural details, and audio recording quality that balances and blends the multiple voices well, and without unwanted noise. We view our current model as stronger on the mid-range time scales: often the model generates samples that locally sound very good, with interesting and diverse harmonies, rhythms, instruments, and voices. We have frequently been very impressed how the melody and rhythm generated suits a particular lyric extremely well. However, while the samples stay consistent over longer time scales, we notice they don't have traditional larger music structures (such as choruses that repeat, or melodies that have a question and answer form). Additionally, on the smallest scale, we sometimes hear audio noise or scratchiness.</p><p xml:id="_z9QyPaT">Beyond the quality of the samples, we also would look to diversify the languages and styles the model is able to generate. Our current model has been trained only on songs whose primary language as detected by <ref type="bibr" coords="11,225.55,504.30,53.26,12.00" target="#b63">(Sites, 2013)</ref> is English. In the future, we would look to include other languages and artists. We believe this will be of interest both for generating strictly in those styles, and because historically we have seen much creativity and development coming from unusual blends of existing musical styles.</p><p xml:id="_7DJCpdC">Finally, we consider it very important that computer music generation also serves as a tool for human musicians, and increasingly those interested in music but without formal training.  <ref type="bibr" coords="11,373.36,259.10,83.71,12.00" target="#b38">(Kingma et al., 2016)</ref>, and distill the information from our current model into it <ref type="bibr" coords="11,464.42,271.06,74.39,12.00" target="#b47">(Oord et al., 2018)</ref>.</p><p xml:id="_52dfHap">The distillation works by generating samples from the parallel sampler and evaluating it likelihood and entropy using the parallel likelihood evaluator, and then optimising the sampler by minimising the KL divergence of it from our current model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8." xml:id="_vJVCSM6">Conclusion</head><p xml:id="_er8KdmP">We have introduced Jukebox, a model that generates raw audio music imitating many different styles and artists. We can condition this music on specific artists and genres, and can optionally specify the lyrics for the sample. We laid out the details necessary to train a Hierarchical VQ-VAE to compress the music effectively into tokens. While previous work has generated raw audio music in the 20-30 second range, our model is capable of generating pieces that are multiple minutes long, and with recognizable singing in natural-sounding voices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9." xml:id="_825HUB7">Acknowledgement</head><p xml:id="_69nk9KQ">We would like to thank John Schulman and Will Guss for producing and performing novel riffs for our sampling experiments, and Rewon Child, Aditya Ramesh, Ryan Lowe and Jack Clark for providing feedback for initial drafts of this paper. We have three separate raw audio VQ-VAEs to produce discrete codes at varying hop sizes for the bottom, middle, and top priors. All autoencoders comprise non-causal, dilated 1-D convolutions, and are trained independently using nonautoregressive reconstruction losses. Basic building blocks in these networks share the same architecture, as shown in Figure <ref type="figure" coords="17,84.41,177.57,3.80,12.00" target="#fig_7">7</ref>. Each encoder block consists of a downsampling convolution, a residual network, and a 1D convolution with a kernel size of 3. Dilation is grown by a factor of 3 in these residual networks to increase the receptive field. The decoder block mirrors this exactly with a 1D convolution with the kernel size of 3, a residual network with dilation contracting across depth, and an upsampling transposed convolution. Here, all resampling convolutions use a kernel size of 4 and stride 2 so that each building block changes the hop length by a factor of 2. To get higher compression in time, we simply stack more of these blocks. For example, using seven blocks yields a hop length of 128 for the top level autoencoder.</p><p xml:id="_WFXmEG3">Each residual network has four residual blocks in the middle and top VQ-VAEs resulting in a receptive field of 120 ms and 480 ms for the respective discrete tokens. Because increasing the residual depth helped improve reconstruction quality slightly, we doubled the number of residual blocks for the bottom level. This dramatically increases the receptive field to about 2 seconds per code but the actual receptive field is mostly local.</p><p xml:id="_q5rVamN">We also experimented with having a single decoder and modeling the residuals to separate out learned representations as in <ref type="bibr" coords="17,98.81,464.50,80.05,12.00" target="#b56">(Razavi et al., 2019)</ref>, hoping upsampling priors would simply fill in local musical structure. However, pushing information to the top level was quite challenging as the bottommost level reconstructs almost perfectly early on in training. When we add auxiliary objectives to encourage the top to be used more, the top-level codes add serious distortions to the final output. A similar challenge is shown in <ref type="bibr" coords="17,65.68,548.18,90.33,12.00" target="#b14">(Dieleman et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_2zDRr7Q">B.2. Music Priors and Upsamplers</head><p xml:id="_EcPbDUb">Architectural details of our music prior and upsampler models are depicted in Figure <ref type="figure" coords="17,161.30,603.93,3.81,12.00">8</ref>. They perform autoregressive modeling of tokens at each level, conditioned on information such as artist and genre, as well as the tokens from the upper level in the case of the upsamplers (Figure <ref type="figure" coords="17,251.30,639.79,7.58,12.00">8a</ref>). Each artist and genre are learned as embedding vectors, whose sum is provided as the very first token in each sequence. In addition, positional embedding is learned as a function of each position's absolute and relative timing in the duration of the song. In upsampler models, upper-level tokens (b) The bottleneck takes the sequence of embeddings from the encoder and maps it into a sequence of code vectors from the codebook. This sequence of code indices is used as a discrete representation to be modeled by the priors. Larger codebooks improve fidelity but may be more difficult to compress.  are upsampled by the conditioner network, using WaveNetstyle dilated convolutions followed by a transposed 1-D convolutional layer (Figure <ref type="figure" coords="17,418.35,496.09,7.89,12.00">8b</ref>).</p><p xml:id="_rCThnJ2">When the model is trained on lyrics, the top-level prior takes lyrics data corresponding to each audio segment and uses them to train an encoder-decoder Transformer as shown in Figure <ref type="figure" coords="17,336.41,549.89,8.07,12.00">8c</ref>. All transformer stacks use sparse self-attention layers with the three factorized attention types (row, column, and previous-row) repeating, and encoder-decoder attention layers, when present, are interleaved with the other attention types. Each layer consists of residual connections of an attention and an MLP feedforward network, each prepended by layer normalization (see Figure <ref type="figure" coords="17,447.17,621.62,7.89,12.00">8d</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,54.94,245.52,486.50,10.80;3,55.44,256.48,338.22,10.80;3,393.66,263.36,2.81,4.37;3,397.46,256.48,143.98,10.80;3,55.44,267.44,486.00,10.80;3,55.44,278.40,487.57,10.80;3,55.12,289.35,486.32,10.80;3,55.44,300.31,370.74,10.80"><head>Figure 1 .</head><label>1</label><figDesc xml:id="_qRvsc9v">Figure1. We first train three separate VQ-VAE models with different temporal resolutions. At each level, the input audio is segmented and encoded into latent vectors ht, which are then quantized to the closest codebook vectors ez t . The code zt is a discrete representation of the audio that we later train our prior on. The decoder takes the sequence of codebook vectors and reconstructs the audio. The top level learns the highest degree of abstraction, since it is encoding longer audio per token while keeping the codebook size the same. Audio can be reconstructed using the codes at any one of the abstraction levels, where the least abstract bottom-level codes result in the highest-quality audio, as shown in Figure4. For the detailed structure of each component, see Figure7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,307.14,278.50,234.29,10.80;4,307.44,288.47,234.00,10.80;4,307.44,298.43,234.00,10.80;4,307.44,308.39,234.00,10.80;4,307.44,318.35,234.00,10.80;4,307.44,328.32,234.00,10.80;4,307.44,338.28,234.00,10.80;4,307.44,348.24,101.75,10.80;4,507.93,387.52,14.06,7.05;4,495.57,461.97,43.45,7.05;4,307.14,475.43,234.30,10.80;4,307.44,485.39,234.00,10.80;4,307.44,495.36,234.00,10.80;4,307.44,505.32,234.00,10.80;4,307.44,515.28,226.49,10.80;4,343.19,644.04,41.39,6.53;4,441.25,644.04,52.13,6.53;4,311.54,588.60,6.53,23.39;4,392.61,556.56,28.75,6.53;4,530.37,588.41,6.53,23.77;4,307.14,657.93,234.30,10.80;4,307.44,667.89,234.00,10.80;4,307.44,677.85,175.45,10.80"><head></head><label></label><figDesc xml:id="_Gsz8JZJ">(a) Ancestral sampling: Priors for the VQ-VAE codes are trained using a cascade of Transformer models, shown in blue. Each model takes conditioning information such as genre, artist, timing, and lyrics, and the upsampler models are also conditioned on the codes from the upper levels. To generate music, the VQ-VAE codes are sampled from top to bottom using the conditioning information for control, after which the VQ-VAE decoder can convert the bottom-level codes to audio. time new samples (b) Windowed sampling: To generate music longer than the model's context length (12 in this figure), we repeatedly sample continuations at each level, using overlapping windows of previous codes as the context. The overlap amount is a hyperparameter, and the figure shows an example of 75% overlap with hop length 3. Primed sampling: The model can generate continuations of an existing audio signal by converting it into the VQ-VAE codes and sampling the subsequent codes in each level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,334.98,697.88,178.92,10.80"><head>Figure 2 .</head><label>2</label><figDesc xml:id="_GUYb6ER">Figure 2. Sampling methods for generating music</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,306.94,197.47,235.99,10.80;5,307.44,208.43,234.00,10.80;5,307.44,219.39,234.00,10.80;5,307.44,230.35,235.57,10.80;5,328.09,71.45,192.99,96.70"><head>Figure 3 .</head><label>3</label><figDesc xml:id="_wpyxKvh">Figure 3. Lyrics-singing alignment learned by one of the encoderdecoder attention layers. The x-axis is the position of music queries, and the y-axis is the position of lyric keys. The positions attended to by the decoder correspond to the characters being sung.</figDesc><graphic coords="5,328.09,71.45,192.99,96.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,54.94,449.81,486.50,10.80;8,55.44,460.76,487.57,10.80;8,55.16,471.72,486.28,10.80;8,55.44,482.68,486.00,10.80;8,55.12,493.64,486.32,10.80;8,55.44,504.60,486.00,10.80;8,55.44,515.56,431.03,10.80;8,71.57,364.66,154.55,71.79"><head>Figure 4 .</head><label>4</label><figDesc xml:id="_cNHYyMY">Figure 4. Comparison of reconstructions from different VQ-VAEs, x-axis is time and y-axis is frequency. The columns from left to right are bottom-, middle-, and top-level reconstructions at hop lengths 8, 32, and 128 respectively, visualized as Mel spectrograms.The first row is the ground-truth, and the second row shows the spectrograms of audio outputs from our VQ-VAE. In the third row, we remove the spectral loss, and see that the middle and top level lose high-frequency information. In the fourth row, we use a hierarchical VQ-VAE<ref type="bibr" coords="8,89.73,493.64,71.20,10.80" target="#b56">(Razavi et al., 2019)</ref> instead of separate auto-encoders (Figure1), and we see the middle and top levels are not used for encoding pertinent information. Finally, the fifth row shows a baseline with the Opus codec that encodes audio at constant bitrates comparable to our VQ-VAE. It also fails to capture higher frequencies and adds noticeable artifacts at the highest level of compression.</figDesc><graphic coords="8,71.57,364.66,154.55,71.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,54.94,192.86,234.65,10.80;9,55.44,203.82,235.49,10.80;9,55.44,214.78,206.95,10.80"><head>Figure 5 .</head><label>5</label><figDesc xml:id="_ZvYnj34">Figure 5. Entropy of codebook with 2048 codes, i.e 11 bits, over training. Reviving dead codes near random encoder outputs ensures good codebook utilization from the start of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="17,495.01,343.75,19.39,5.08;17,472.43,314.66,6.41,5.10;17,427.39,323.60,7.01,5.10;17,355.44,327.19,16.71,5.08;17,354.10,333.46,19.39,5.08;17,394.36,330.33,19.39,5.08;17,427.09,343.73,3.13,5.10;17,448.14,340.61,28.15,5.08;17,452.52,346.88,19.39,5.08;17,311.00,342.41,7.20,8.08;17,531.17,342.86,5.24,6.29;17,307.14,371.43,235.79,10.80;17,307.44,381.39,234.00,10.80;17,307.44,391.36,234.00,10.80;17,307.44,401.32,234.00,10.80;17,307.44,411.28,234.00,10.80"><head></head><label></label><figDesc xml:id="_eRgabgY">The decoder reconstructs the raw audio from latent representations. It is a mirror of the encoder where dilations constracts by a factor of 3 down to 1 at the last block. The final Conv1D projects to the desired number of audio channels and also acts as a smoothing operation after a sequence of transposed convolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="17,342.56,431.31,163.76,10.80"><head>Figure 7 .</head><label>7</label><figDesc xml:id="_Qmv2R3u">Figure 7. Components of the VQ-VAE model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,306.89,451.62,236.12,189.66"><head>Table 1 .</head><label>1</label><figDesc xml:id="_NHAp6NN">Reconstruction fidelity degrades with higher compression.</figDesc><table coords="7,307.44,451.62,235.48,189.66"><row><cell>Level</cell><cell cols="3">convergence (dB) Hop length Without restart With restart</cell></row><row><cell>Bottom Middle Top</cell><cell>8 32 128</cell><cell>−21.1 −12.4 −8.3</cell><cell>−23.0 −12.4 −8.3</cell></row><row><cell cols="4">Restarting dead codes near random encoder outputs mitigates learn-</cell></row><row><cell cols="2">ing suboptimal codes.</cell><cell></cell><cell></cell></row><row><cell cols="4">Codebook size Spectral convergence (dB)</cell></row><row><cell cols="2">256 2048 No quantization</cell><cell>−15.9 −23.0 −40.5</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,306.89,653.33,236.04,54.64"><head>Table 2 .</head><label>2</label><figDesc xml:id="_sqrAcvF">Bottom-level VQ-VAE reconstruction results with differ-</figDesc><table /><note xml:id="_bCQE6DM">ent codebook sizes. Using larger codebooks helps reconstruction because it allows more information to be encoded at the bottleneck layers. Removing the bottleneck entirely yields almost perfect reconstruction.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,54.89,548.46,234.55,97.65"><head>Table 3 .</head><label>3</label><figDesc xml:id="_SYDa9fN">Top-level codes are generally difficult to train well without spectral loss or with a single hierarchical autoencoder. Resulting reconstructions may lose some to most of information.</figDesc><table coords="8,65.21,548.46,214.45,52.87"><row><cell>Ablation</cell><cell>Spectral convergence (dB)</cell></row><row><cell>None Without spectral loss With single autoencoder</cell><cell>−8.3 −6.3 2.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,55.44,67.82,487.74,627.70"><head></head><label></label><figDesc xml:id="_erdku4F">takes a different approach of hierarchically generating accurate high-resolution Mel spec-trograms, after which a simple gradient-based optimization can produce high-fidelity audio.VQ-VAE:<ref type="bibr" coords="10,351.27,97.71,73.96,12.00" target="#b46">Oord et al. (2017)</ref> introduced VQ-VAE, an approach of downsampling extremely long context inputs to a shorter-length discrete latent encoding using a vector quantization, and they showed that it can generate both highquality images and audio, as well as learn unsupervized representations of phonemes.<ref type="bibr" coords="10,425.87,157.48,77.63,12.00" target="#b56">Razavi et al. (2019)</ref> extended the above model by introducing a hierarchy of discrete representations for images and showed that the resulting model can learn to separate high-level semantics into the highest level of discrete codes which have the largest receptive field, while capturing local features like textures in the lower levels with smaller receptive fields. They used the hierarchical model to generate high-diversity and high-fidelity images for the conditional ImageNet and FFHQ datasets.<ref type="bibr" coords="10,503.48,253.12,37.96,12.00;10,307.44,265.08,49.89,12.00" target="#b14">Dieleman et al. (2018)</ref> tried variants of this approach where instead of a single encoder there are successive encoders that each further compress the lossy discrete encodings from the previous levels. A downside of this approach is that information is lost at each step and requires separate training for each VQ-VAE level, and it leads to a hierarchy collapse problem. De Fauw et al. (2019) used AR decoders which are known to cause the problem of ignoring the latent variables, and they suggested ways to mitigate it. The feed-forward decoders from</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,55.08,67.82,488.10,621.84"><head></head><label></label><figDesc xml:id="_XjETbHR">While we are able to steer our current model somewhat through lyric and midi conditioning, we can imagine many other possible ways for humans to influence the generations, including indicating the mood or dynamic at various sections, or controlling when drums, singers, or other instruments should play.The current model takes around an hour to generate 1 minute of top level tokens. The upsampling process is very slow, as it proceeds sequentially through the sample. Currently it takes around 8 hours to upsample one minute of top level tokens. We can create a human-in-the-loop co-composition process at the top level only, using the VQ-VAE decoders to get a fast upsampling of the top level tokens to hear a very rough sense of what the model generates.</figDesc><table coords="11,307.08,151.50,236.10,119.60"><row><cell>The top-level model generates multiple samples, the person picks a fa-vorite (listening to the rough VQ-VAE decoding), and then the model continues generating multiple samples continuing the favorite. This process would be significantly improved with faster generation and Transformer upsampling steps. Our models have fast parallel evaluation of likelihood but slow autoregressive sampling. We can instead use a model with fast parallel sampling but slow autoregressive likeli-hood evaluation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="13,55.44,67.82,235.75,158.74"><head></head><label></label><figDesc xml:id="_4EKbfJz">Hawthorne, C., Stasyuk, A., Roberts, A., Simon, I., Huang, C.-Z. A., Dieleman, S., Elsen, E., Engel, J., and Eck, D. Enabling factorized piano music modeling and generation with the MAESTRO dataset. In International Conference on Learning Representations, 2019.</figDesc><table coords="13,55.44,135.21,235.75,91.35"><row><cell>B. Experimental details</cell></row><row><cell>B.1. Music VQ-VAE</cell></row><row><cell>Hennequin, R., Khlif, A., Voituret, F., and Moussallam, M. Spleeter: A fast and state-of-the art music source separa-tion tool with pre-trained models. Late-Breaking/Demo ISMIR 2019, November 2019. Deezer Research.</cell></row><row><cell>Hiller Jr, L. A. and Isaacson, L. M. Musical composition with a high speed digital computer. In Audio Engineering Society Convention 9. Audio Engineering Society, 1957.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="17,307.14,70.35,235.41,167.06"><head></head><label></label><figDesc xml:id="_QYXFcqU">The encoder compresses the raw audio input into a sequence of embeddings. The length of this latent representation relative to the raw audio duration determines the amount of compression, and is an important factor for the trade-off between fidelity and coherence.</figDesc><table coords="17,307.14,70.35,230.24,167.06"><row><cell></cell><cell></cell><cell></cell><cell>×L</cell></row><row><cell></cell><cell>Dilated Conv1D</cell><cell>Conv1D</cell><cell>×D</cell></row><row><cell>x t</cell><cell>Conv1D</cell><cell></cell><cell>+</cell><cell>h t</cell></row><row><cell cols="2">Codebook (a) Gradient Passthrough Nearest-Neighbor Search z t h t</cell><cell>Codebook Lookup</cell><cell></cell><cell>e zt</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Richard Feynmann famously said, "What I cannot create, I do not understand"</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The gradient also needs to break symmetry with the encoder output features, which is the case here since the weights of the input projections in the attention are not zero.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_rycNYBp">A. Scalable Transformer</head><p xml:id="_hbQhKs4">We make the Sparse Transformer <ref type="bibr" coords="16,190.60,87.19,75.80,12.00" target="#b11">(Child et al., 2019)</ref> more scalable and easier to implement by a few small changes. We implement a simpler attention pattern that has the same performance without needing custom kernels to implement. We simplify the initialization by using the same initalization scale in the whole model without rescaling the weights based on fan-in and depth, and we optimize the memory footprint with fully half-precision training, i.e. storing the model weights, gradients and the optimizer states in half precision and performing computations in half precision as well. To cope with the narrower dynamic range of the fp16 format, we use dynamic scaling of the gradient and Adam optimizer states.</p><p xml:id="_YNhnhZA">Axis-aligned attention patterns: The Sparse Transformer <ref type="bibr" coords="16,55.11,260.54,84.83,12.00" target="#b11">(Child et al., 2019)</ref> sparsifies the attention pattern by reshaping the input sequence into a 2-D sequence of shape (blocks, block length) to use factorized attention. They observe that the strided attention pattern works best for images and audio because it does not have the state bottleneck of the fixed attention. However, their implementation require specialized CUDA kernels. We can obtain a similar pattern by doing masked row, masked column, and unmasked previous-row attention. While the masked row captures the local context, the masked column and unmasked previous-row attention captures the context of all previous rows. We observe the same computational speed as well as training loss with this pattern. Each of these can be implemented directly as a dense attention by transposing or slicing the input sequence along appropriate axes, and thus do not require special CUDA kernels to implement. This can be easily extended to video too. Complementary to our work, a similar pattern was introduced in <ref type="bibr" coords="16,158.91,475.73,65.62,12.00" target="#b26">(Ho et al., 2019)</ref> where they also used axis-aligned attention but instead used a two-stream architecture.</p><p xml:id="_8bRkgh8">Half-precision parameters and optimizer state with dynamic scaling: To allow training large models, <ref type="bibr" coords="16,242.31,541.49,48.38,12.00;16,55.44,553.44,23.71,12.00" target="#b11">(Child et al., 2019)</ref> uses recompute with gradient checkpointing, performs computations using half precision activations and gradients, and uses dynamic loss scaling. While this speeds up training on Volta cores, one still has a high memory usage from storing the parameters and Adam state in full float precision. To scale our models further, we store our matmul parameters and their Adam state in half precision, thus halving our memory usage. We use a single parameter s to set the scale of all weights and initialize all matmul and input/output embeddings 3 to N (0, s), and position embeddings to N (0, 2s). The initialization ensures all parameters are in a similar dynamic range, and allows us to train in half preci- 3 We share the input and output embedding (b) Combining two of the attention patterns, each position can attend to any of the previous positions, while not causing a state bottleneck as in fixed sparse attention <ref type="bibr" coords="16,444.28,313.81,66.37,10.80" target="#b11">(Child et al., 2019)</ref>.</p><p xml:id="_Sqx3NvE">Figure <ref type="figure" coords="16,377.07,336.29,3.36,7.77">6</ref>. Axis-aligned attention patterns sion completely without loss in training performance. For the Adam state tensors (m_t, v_t) we do dynamic scaling. For each iteration and for every parameter, we rescale its state tensors before casting so that their maximum corresponds to the maximum value of the float16 range, thus maximizing the use of the float16 range. Thus, we store the state m_t as the tuple (scale, (m_t/scale).half()), where scale = m_t.max()/float16.max(), and similarly for v_t. The above lets us fit models of size 1B parameters into memory for our large context of 8192 tokens. To train even larger models, we use GPipe <ref type="bibr" coords="16,510.96,494.26,30.48,12.00;16,307.44,506.22,50.67,12.00" target="#b30">(Huang et al., 2019b)</ref>. (a) The structure of our prior models, performing next-token prediction at each level. The Transformer takes the embeddings of the tokens z1:T −1 prepended by the sum of the artist and genre embeddings, in addition to the time embedding that encodes relative and absolute timing of the segments in the duration of the song. The upsampler priors additionally take the tokens from the upper level, which are fed to the conditioner network and added to the input sequence. The top-level prior takes lyrics as conditioning information as well (see Figure <ref type="figure" coords="18,323.20,302.60,6.85,10.80">8c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_UcfC7gj">×D</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_rvThGws">Dilated Conv1D</head><p xml:id="_jEveAj3">Conv1D</p><p xml:id="_gbAXF77">Token Embedding (b) The conditioner network takes the tokens from the upper level, and their embedding vectors go through non-causal WaveNet-like layers with increasingly dilated convolutions. The transposed 1-D convolution upsamples the sequence to the higher temporal resolution of the current level. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,307.44,628.62,234.00,12.00;11,317.40,640.58,225.69,12.00;11,317.40,652.53,182.34,12.00" xml:id="b0">
	<monogr>
		<title level="m" type="main" xml:id="_MPKFxQn">Expressive speech synthesis via modeling expressions with variational autoencoder</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Akuzawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Iwasawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,307.44,681.94,234.00,12.00;11,317.40,693.90,225.79,12.00;11,316.79,705.86,211.37,12.00;12,221.05,47.92,154.56,8.06;12,55.44,67.82,235.66,12.00;12,65.40,79.77,225.29,12.00;12,65.21,91.73,224.23,12.00;12,65.40,103.68,224.04,12.00;12,64.98,115.64,155.92,12.00" xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_sytZQMq">Constraint programming systems for modeling music theories and composition</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Anders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">R</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Ö</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_UY7mRvb">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2011">2011. 2017</date>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="195" to="204" />
		</imprint>
	</monogr>
	<note>Deep Voice: Real-time neural text-to-speech</note>
</biblStruct>

<biblStruct coords="12,55.44,136.93,235.74,12.00;12,65.40,148.88,224.04,12.00;12,65.40,160.84,225.53,12.00;12,64.65,172.79,56.73,12.00" xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_eaerNgf">Neural voice cloning with a few samples</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Ö</forename><surname>Arık</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_FQnBWeF">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
			<biblScope unit="page" from="10019" to="10029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,55.44,194.08,234.00,12.00;12,65.40,206.04,225.79,12.00;12,65.23,217.99,212.53,12.00" xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_HTgrxxk">Fast spectrogram inversion using multi-head convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Ö</forename><surname>Arık</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UtktFqv">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="98" />
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,55.44,239.28,235.75,12.00;12,65.40,251.24,159.58,12.00" xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">E. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,55.44,272.53,235.25,12.00;12,65.40,284.48,225.28,12.00;12,65.40,296.44,224.04,12.00;12,65.40,308.39,198.30,12.00" xml:id="b5">
	<monogr>
		<title level="m" type="main" xml:id="_rau7cbZ">Dota 2 with large scale deep reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dębiak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dennison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Farhi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hashme</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06680</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,55.44,329.68,234.00,12.00;12,65.23,341.64,225.45,12.00;12,64.65,353.59,22.42,12.00" xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_u6SxNUE">The musical universe of cellular automata</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Beyls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_SXeK2RQ">International Computer Music Conference</title>
				<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="34" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,55.44,374.88,234.00,12.00;12,65.40,386.84,153.29,12.00" xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_SwR8kr6">A neural parametric singing synthesizer</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blaauw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bonada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_d64qXVz">INTERSPEECH</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,55.44,408.13,234.35,12.00;12,65.40,420.08,224.04,12.00;12,65.40,432.04,164.93,12.00" xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_tTBcccB">Synthesis of the singing voice by performance sampling and spectral models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bonada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5Dg65dK">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="67" to="79" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,55.44,453.33,234.00,12.00;12,65.40,465.28,224.04,12.00;12,65.23,479.78,225.45,8.59;12,65.40,489.19,22.42,12.00" xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_MdA3uFF">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_5eVWNkS">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,55.44,510.48,235.74,12.00;12,65.40,522.44,224.03,12.00;12,65.40,534.39,224.04,12.00;12,65.15,546.35,226.03,12.00;12,65.15,558.30,62.27,12.00" xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_53sA4YX">MIDI-VAE: modeling dynamics and instrumentation of music with applications to style transfer</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wattenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_SysZkgZ">International Society for Music Information Retrieval Conference</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="747" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,55.44,579.59,235.65,12.00;12,65.40,591.55,224.04,12.00;12,65.40,603.50,134.95,12.00" xml:id="b11">
	<monogr>
		<title level="m" type="main" xml:id="_C5aMDJS">Generating long sequences with sparse transformers</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,55.44,624.79,235.65,12.00;12,65.40,636.75,225.78,12.00;12,65.40,648.70,159.58,12.00" xml:id="b12">
	<monogr>
		<title level="m" type="main" xml:id="_yzX9SzB">Hierarchical autoregressive image models with auxiliary decoders</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.04933</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,55.44,669.99,234.00,12.00;12,65.40,681.94,225.69,12.00;12,65.40,693.90,224.03,12.00;12,65.09,705.86,135.42,12.00" xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_st9BCag">Front-end factor analysis for speaker verification</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_StEAJU4">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,307.44,67.82,235.66,12.00;12,317.40,79.77,224.04,12.00;12,317.40,91.73,224.04,12.00;12,317.15,103.68,123.70,12.00" xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_8CGrmqp">The challenge of realistic music generation: modelling raw audio at scale</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_g6NJn24">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7989" to="7999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,307.44,122.56,235.65,12.00;12,317.40,134.52,225.69,12.00;12,317.40,146.47,217.62,12.00" xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_y8RQUpT">NICE: Non-linear independent components estimation</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_xSDqdDF">International Conference in Learning Representations, Workshop</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,307.44,165.35,235.65,12.00;12,317.40,177.30,224.04,12.00;12,317.12,189.26,130.25,12.00" xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_r3Cu9fn">Density estimation using Real NVP</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_rm3br7d">International Conference in Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,307.44,208.14,234.00,12.00;12,317.40,220.09,224.04,12.00;12,317.40,232.05,225.69,12.00;12,317.40,246.54,225.69,8.59;12,317.40,255.96,111.14,12.00" xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_CaFwTGD">Improving multi-instrumental music generation with cross-domain pre-training</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lakhnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" xml:id="_gAMkyas">In International Society for Music Information Retrieval Conference</title>
		<imprint>
			<biblScope unit="page" from="685" to="692" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,307.44,274.83,235.74,12.00;12,317.40,286.79,224.04,12.00;12,317.40,298.74,225.69,12.00;12,317.40,310.70,224.04,12.00;12,317.23,322.65,74.33,12.00" xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_fRmGQTa">Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment</title>
		<author>
			<persName coords=""><forename type="first">H.-W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-Y</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Musegan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wan7gwb">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,307.44,341.53,235.24,12.00;12,317.40,353.49,224.04,12.00;12,317.40,365.44,225.69,12.00;12,317.40,377.40,225.29,12.00;12,317.40,389.35,22.42,12.00" xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_CkJXfFw">Neural audio synthesis of musical notes with wavenet autoencoders</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_MVAYwtR">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1068" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,307.44,408.23,235.24,12.00;12,317.40,420.19,225.69,12.00;12,317.40,432.14,224.04,12.00;12,317.09,444.10,91.23,12.00" xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_RZeHbJK">Adversarial neural audio synthesis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">K</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gansynth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_qNMcWzU">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,307.44,462.97,235.24,12.00;12,317.40,474.93,225.70,12.00;12,317.40,486.88,224.04,12.00;12,317.40,498.84,217.34,12.00" xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_FDW2TbX">Deep Voice 2: Multispeaker neural text-to-speech</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Ö</forename><surname>Arık</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_QWU5PJv">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2962" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,307.44,517.72,234.00,12.00;12,317.40,529.67,225.28,12.00;12,317.09,541.63,59.05,12.00" xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_VFBnVCC">NIPS 2016 tutorial: Generative adversarial networks</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_pwg8xSq">Neural Information Processing Systems</title>
				<imprint>
			<publisher>Tutorial</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,307.44,560.50,235.25,12.00;12,316.93,572.46,225.76,12.00;12,316.82,584.41,224.61,12.00;12,317.40,596.37,217.34,12.00" xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_PbrSB9F">Generative adversarial nets</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_9YfB5MU">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,307.44,615.25,235.65,12.00;12,317.40,627.20,224.04,12.00;12,317.40,639.16,225.78,12.00;12,317.40,651.11,116.61,12.00" xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_FDzUMp5">Automatic lyrics transcription in polyphonic music: Does background music help</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yılmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_7wECSKr">International Conference on Acoustics, Speech, and Signal Processing</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,307.44,669.99,235.65,12.00;12,317.40,681.94,224.04,12.00;12,317.07,693.90,226.11,12.00;12,317.40,705.86,40.50,12.00" xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_FvtPU6u">Deepbach: a steerable model for bach chorales generation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hadjeres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pachet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_hzfyqHc">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1362" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,55.44,234.12,235.74,12.00;13,65.04,246.08,224.40,12.00;13,65.40,258.03,134.95,12.00" xml:id="b26">
	<monogr>
		<title level="m" type="main" xml:id="_D3jGTD4">Axial attention in multidimensional transformers</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,55.44,277.60,235.24,12.00;13,64.82,289.56,224.62,12.00;13,65.40,301.51,225.69,12.00;13,65.40,313.47,224.04,12.00;13,65.12,325.42,130.25,12.00" xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_dw5xHV2">Hierarchical generative modeling for controllable speech synthesis</title>
		<author>
			<persName coords=""><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_DXSJRaV">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,55.44,344.99,235.25,12.00;13,65.40,356.95,225.69,12.00;13,65.40,371.44,225.69,8.59;13,65.40,380.86,100.44,12.00" xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_9C6tkQv">Counterpoint by convolution</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_yHEnyeg">International Society for Music Information Retrieval Conference</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="211" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,55.44,400.43,235.25,12.00;13,65.40,412.38,225.29,12.00;13,65.40,424.34,225.69,12.00;13,65.40,436.29,224.04,12.00;13,65.07,448.25,195.18,12.00" xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_TYE6rBt">Music Transformer: Generating music with long-term structure</title>
		<author>
			<persName coords=""><forename type="first">C.-Z</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Fp3FCaG">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,55.44,467.82,235.24,12.00;13,65.40,479.77,225.42,12.00;13,65.40,491.73,224.03,12.00;13,65.40,503.68,225.69,12.00;13,65.40,515.64,133.94,12.00" xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_sZqZRfu">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_J3pqK5w">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,55.44,535.21,235.65,12.00;13,65.40,547.16,224.03,12.00;13,65.40,559.12,225.78,12.00;13,65.15,571.07,225.54,12.00;13,64.65,583.03,22.42,12.00" xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_P7RBHZq">Unit selection in a concatenative speech synthesis system using a large speech database</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_53fmBcB">IEEE International Conference on Acoustics, Speech, and Signal Processing Conference</title>
				<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="373" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,55.44,602.60,235.65,12.00;13,65.40,614.55,224.04,12.00;13,65.40,626.51,225.79,12.00" xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_hpCbNFw">Massachusetts Institute of Technology, School of Architecture and Planning</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Jehan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Sn2GPvR">Program in Media Arts and Sciences</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>Creating music by listening</note>
</biblStruct>

<biblStruct coords="13,55.44,646.08,235.25,12.00;13,65.40,658.03,224.04,12.00;13,64.93,669.99,224.50,12.00;13,65.40,681.94,224.04,12.00;13,65.07,693.90,226.11,12.00;13,65.40,705.86,22.42,12.00" xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_Vhszzsc">Transfer learning from speaker verification to multispeaker text-to-speech synthesis</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Lopez Moreno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_zTW9TSX">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4480" to="4490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,307.44,67.82,235.25,12.00;13,317.40,79.77,225.29,12.00;13,317.40,91.73,225.69,12.00;13,317.40,103.68,224.04,12.00;13,317.12,115.64,128.69,12.00" xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_5FBy65r">Efficient neural audio synthesis</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_vwfXsSN">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2410" to="2419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,307.44,137.78,234.00,12.00;13,317.40,149.73,225.69,12.00;13,317.40,164.23,224.04,8.59;13,317.09,173.64,126.02,12.00" xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_C4zHmqf">Neural music synthesis for flexible timbre control</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_jdfcQwK">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="176" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,307.44,195.78,234.36,12.00;13,317.04,207.73,224.40,12.00;13,317.23,219.69,225.93,12.00" xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_wHRjFa6">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_FMqzEZD">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,307.44,241.83,234.00,12.00;13,317.40,253.78,225.69,12.00;13,317.40,265.74,67.80,12.00" xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_9anPjX2">Auto-encoding variational bayes</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_2CdugXS">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,307.44,287.88,235.25,12.00;13,317.40,299.83,225.69,12.00;13,317.40,311.79,224.04,12.00;13,317.40,323.74,225.29,12.00;13,317.40,335.70,22.42,12.00" xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_hRmmwcg">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_UyXTquD">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,307.44,357.83,235.65,12.00;13,317.40,369.79,224.54,12.00;13,317.07,381.75,76.65,12.00" xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_J458prs">Software for a cascade/parallel formant synthesizer</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Klatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_gCA5Xb8">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="971" to="995" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,307.44,403.88,235.25,12.00;13,316.93,415.84,224.51,12.00;13,317.40,427.79,225.69,12.00;13,317.04,439.75,224.40,12.00;13,317.40,451.70,225.53,12.00;13,316.65,463.66,52.30,12.00" xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_bMyfrY2">MelGAN: Generative adversarial networks for conditional waveform synthesis</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>De Boissiere</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gestin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Z</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Brébisson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_MDtKYHa">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14881" to="14892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,307.44,485.80,235.20,12.00;13,317.10,501.41,86.18,7.01" xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Lyricwiki</surname></persName>
		</author>
		<ptr target="https://lyrics.fandom.com/wiki/LyricWiki" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,307.44,519.89,235.25,12.00;13,317.40,531.85,224.04,12.00;13,317.40,543.80,225.78,12.00;13,317.40,555.76,225.28,12.00;13,317.40,567.71,22.42,12.00" xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_Y6r98xG">SampleRNN: An unconditional end-to-end neural audio generation model</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_9vEgtWD">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,307.44,589.85,235.65,12.00;13,317.40,601.80,168.51,12.00" xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_5DQEmMK">Music and computer composition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Moorer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_p2E9tE3">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="104" to="113" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,307.44,623.94,235.66,12.00;13,317.40,635.90,224.04,12.00;13,317.12,647.85,130.25,12.00" xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_AqC7HW3">Autoencoderbased music translation</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_sEVPAB2">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,307.44,669.99,235.25,12.00;13,317.04,681.94,225.64,12.00;13,317.40,693.90,224.21,12.00;13,317.40,705.86,205.07,12.00" xml:id="b45">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Wavenet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m" xml:id="_33DZXwS">A generative model for raw audio</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,55.44,67.82,234.00,12.00;14,65.40,79.77,224.04,12.00;14,65.09,91.73,105.25,12.00" xml:id="b46">
	<analytic>
		<title level="a" type="main" xml:id="_NuPqfXC">Neural discrete representation learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ZtbVnbz">Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,55.44,111.03,235.25,12.00;14,65.40,122.98,225.29,12.00;14,65.40,134.94,225.29,12.00;14,65.40,146.89,225.29,12.00;14,65.40,158.85,225.29,12.00;14,65.40,170.80,225.79,12.00;14,65.40,182.76,225.78,12.00;14,65.40,194.71,72.23,12.00" xml:id="b47">
	<analytic>
		<title level="a" type="main" xml:id="_dcm7y26">Fast high-fidelity speech synthesis</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Parallel</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Wavenet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_pR57bD6">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3918" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,55.44,214.01,235.25,12.00;14,65.40,225.96,224.04,12.00;14,65.40,237.92,182.35,12.00" xml:id="b48">
	<monogr>
		<title level="m" type="main" xml:id="_r4nxKrx">Recent development of the HMM-based singing voice synthesis system -Sinsy</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Oura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Muto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Nankaku</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,55.44,257.22,234.00,12.00;14,65.40,269.17,224.04,12.00;14,65.07,281.13,225.61,12.00;14,65.40,293.08,22.42,12.00" xml:id="b49">
	<analytic>
		<title level="a" type="main" xml:id="_BqsTyr5">Masked autoregressive flow for density estimation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_p8CgPbB">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2338" to="2347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,55.44,312.38,236.99,12.00;14,64.80,327.99,151.93,7.01" xml:id="b50">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Musenet</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/musenet" />
		<title level="m" xml:id="_6cktTqa">OpenAI blog</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,55.44,343.63,235.25,12.00;14,65.04,355.58,224.39,12.00;14,65.40,367.54,224.04,12.00;14,65.07,379.49,190.75,12.00" xml:id="b51">
	<analytic>
		<title level="a" type="main" xml:id="_XW9EGpm">Deep Voice 3: 2000-speaker neural text-to-speech</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_9vdQaFq">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,55.44,398.79,234.00,12.00;14,65.40,410.75,224.04,12.00;14,65.07,422.70,190.75,12.00" xml:id="b52">
	<analytic>
		<title level="a" type="main" xml:id="_SUQJdEz">Clarinet: Parallel wave generation in end-to-end text-to-speech</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_RMcbCtc">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,55.44,442.00,234.36,12.00;14,65.40,453.95,224.04,12.00;14,65.23,468.45,224.21,8.59;14,65.15,477.86,163.93,12.00" xml:id="b53">
	<analytic>
		<title level="a" type="main" xml:id="_UfxE7sN">WaveGlow: A flow-based generative network for speech synthesis</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_fXZgr38">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3617" to="3621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,55.44,497.16,235.74,12.00;14,65.07,509.12,181.56,12.00" xml:id="b54">
	<analytic>
		<title level="a" type="main" xml:id="_AH2k2bb">Nonlinear maps as generators of musical design</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pressing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UbsdV9G">Computer Music Journal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="35" to="46" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,55.44,528.41,234.00,12.00;14,65.40,540.37,224.28,12.00;14,65.40,552.32,34.02,12.00" xml:id="b55">
	<monogr>
		<title level="m" type="main" xml:id="_eR4yxM4">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,55.44,571.62,234.00,12.00;14,65.40,583.58,224.04,12.00;14,65.40,595.53,225.53,12.00;14,64.65,607.49,52.30,12.00" xml:id="b56">
	<analytic>
		<title level="a" type="main" xml:id="_uakGe8U">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_BPnh48R">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14837" to="14847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,55.44,626.78,234.00,12.00;14,65.40,638.74,225.69,12.00;14,65.40,650.69,152.61,12.00" xml:id="b57">
	<analytic>
		<title level="a" type="main" xml:id="_r6cZCGc">Variational inference with normalizing flows</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Dqh2VJd">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,55.44,669.99,234.00,12.00;14,65.40,681.94,225.69,12.00;14,65.40,693.90,224.04,12.00;14,65.12,705.86,128.69,12.00" xml:id="b58">
	<analytic>
		<title level="a" type="main" xml:id="_BpEBubN">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_tW8fPCJ">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,307.44,67.82,235.25,12.00;14,317.40,79.77,225.69,12.00;14,317.40,91.73,224.04,12.00;14,316.98,103.68,165.89,12.00" xml:id="b59">
	<analytic>
		<title level="a" type="main" xml:id="_yGqgCW9">A hierarchical latent vector model for learning longterm structure in music</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_rVjKsSe">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4364" to="4373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,307.44,121.98,235.25,12.00;14,317.40,133.94,224.04,12.00;14,317.23,145.89,92.15,12.00" xml:id="b60">
	<monogr>
		<title level="m" type="main" xml:id="_gTbwGy9">An HMM-based singing voice synthesis system</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Saino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Nankaku</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,307.44,164.19,235.74,12.00;14,317.40,176.15,225.78,12.00" xml:id="b61">
	<analytic>
		<title level="a" type="main" xml:id="_5bDgEdp">Deep boltzmann machines</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Y3A3SHP">Artificial intelligence and statistics</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,307.44,194.45,235.25,12.00;14,316.82,206.41,225.86,12.00;14,317.40,218.36,225.79,12.00;14,317.40,230.32,224.04,12.00;14,317.40,242.27,225.70,12.00;14,317.40,254.23,225.78,12.00;14,317.05,266.18,72.23,12.00" xml:id="b62">
	<analytic>
		<title level="a" type="main" xml:id="_9FhGKbT">Natural TTS synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Skerrv-Ryan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Agiomvrgiannakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_AcftmbF">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4779" to="4783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,307.44,284.48,236.99,12.00;14,316.80,300.09,169.86,7.01" xml:id="b63">
	<monogr>
		<title level="m" type="main" xml:id="_dfVRJDp">Compact language detector 2</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sites</surname></persName>
		</author>
		<ptr target="https://github.com/CLD2Owners/cld2" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,307.44,314.74,235.25,12.00;14,317.40,326.69,225.69,12.00;14,317.40,338.65,224.04,12.00;14,317.12,350.60,130.25,12.00" xml:id="b64">
	<analytic>
		<title level="a" type="main" xml:id="_5329RJH">Char2Wav: End-toend speech synthesis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_UemkMfM">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,307.44,368.90,234.00,12.00;14,317.40,380.86,224.03,12.00;14,317.40,392.81,152.03,12.00" xml:id="b65">
	<analytic>
		<title level="a" type="main" xml:id="_k2gHKBD">Signal reconstruction from stft magnitude: A state of the art</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sturmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Daudet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_NFJueXt">International Conference on Digital Audio Effects</title>
				<meeting><address><addrLine>DAFx</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,307.44,411.11,235.65,12.00;14,317.40,423.07,224.04,12.00;14,317.40,435.02,225.29,12.00;14,317.40,446.98,22.42,12.00" xml:id="b66">
	<analytic>
		<title level="a" type="main" xml:id="_gufC9qw">Sequence to sequence learning with neural networks</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_zDuCBCd">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,307.44,465.28,235.74,12.00;14,317.04,477.24,224.40,12.00;14,317.40,489.19,225.69,12.00;14,317.40,501.15,54.52,12.00" xml:id="b67">
	<analytic>
		<title level="a" type="main" xml:id="_GKDJ8vS">VoiceLoop: Voice fitting and synthesis via a phonological loop</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_J44dNWZ">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,307.44,519.45,234.00,12.00;14,317.40,531.40,225.69,12.00;14,317.40,543.36,224.53,12.00;14,317.07,555.31,86.61,12.00" xml:id="b68">
	<analytic>
		<title level="a" type="main" xml:id="_pXNAuu4">Neural autoregressive distribution estimation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-A</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_HJTJChs">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7184" to="7220" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,307.44,573.61,235.24,12.00;14,317.40,585.57,224.04,12.00;14,317.40,597.52,224.04,12.00;14,317.40,609.48,167.80,12.00" xml:id="b69">
	<analytic>
		<title level="a" type="main" xml:id="_33B3emg">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_amPSSBH">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,307.44,627.78,234.00,12.00;14,317.40,639.73,224.04,12.00;14,317.40,651.69,100.17,12.00" xml:id="b70">
	<monogr>
		<title level="m" type="main" xml:id="_3AwthkK">A generative model for audio in the frequency domain</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vasquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Melnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01083</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,307.44,669.99,235.24,12.00;14,317.40,681.94,225.69,12.00;14,317.40,693.90,224.04,12.00;14,317.09,705.86,170.01,12.00" xml:id="b71">
	<analytic>
		<title level="a" type="main" xml:id="_Hh4n7qV">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_xcqRzJC">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,55.44,67.82,235.65,12.00;15,65.40,79.77,225.29,12.00;15,65.40,91.73,224.04,12.00;15,65.40,103.68,225.69,12.00;15,65.40,115.64,187.37,12.00" xml:id="b72">
	<analytic>
		<title level="a" type="main" xml:id="_hvrma2q">Style Tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_uUw9d6K">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,55.44,135.56,234.00,12.00;15,65.40,147.52,225.79,12.00;15,65.23,159.47,165.34,12.00" xml:id="b73">
	<analytic>
		<title level="a" type="main" xml:id="_GFTyAbP">A hierarchical recurrent neural network for symbolic melody generation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_u8ZGmRa">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,55.44,179.40,235.66,12.00;15,65.40,191.35,225.78,12.00;15,65.40,203.31,224.04,12.00;15,65.09,215.26,140.20,12.00" xml:id="b74">
	<analytic>
		<title level="a" type="main" xml:id="_uFKrT8B">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_PP5QQ6a">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,55.44,235.19,235.65,12.00;15,65.40,247.15,225.69,12.00;15,65.40,259.10,225.69,12.00;15,65.40,271.06,225.78,12.00;15,65.40,283.01,116.61,12.00" xml:id="b75">
	<analytic>
		<title level="a" type="main" xml:id="_mfJTgjr">Parallel Wave-GAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_swV8Tbx">International Conference on Acoustics, Speech, and Signal Processing</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,55.44,302.94,234.00,12.00;15,65.40,314.89,225.69,12.00;15,65.40,326.85,225.69,12.00;15,65.40,338.80,196.78,12.00" xml:id="b76">
	<analytic>
		<title level="a" type="main" xml:id="_pqHNNy8">Midinet: A convolutional generative adversarial network for symbolic-domain music generation</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_aUWBNWp">International Society for Music Information Retrieval Conference</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="324" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,55.44,358.73,234.00,12.00;15,65.40,370.68,224.79,12.00;15,65.07,382.64,91.60,12.00" xml:id="b77">
	<analytic>
		<title level="a" type="main" xml:id="_hMKek8P">Review: Statistical parametric speech synthesis</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_AnhnvkN">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1039" to="1064" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,55.44,402.56,235.39,12.00;15,65.40,414.52,224.04,12.00;15,65.07,426.47,166.06,12.00" xml:id="b78">
	<analytic>
		<title level="a" type="main" xml:id="_nf3be7F">Fixup initialization: Residual learning without normalization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_H7MXFnm">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,55.44,446.40,235.74,12.00;15,65.40,458.35,225.69,12.00;15,65.40,470.31,202.32,12.00" xml:id="b79">
	<analytic>
		<title level="a" type="main" xml:id="_YVsTDtT">Self-attention generative adversarial networks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_6UDK2CP">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
