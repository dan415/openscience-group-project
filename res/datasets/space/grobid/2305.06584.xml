<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_6As2DWT">Active Learning in the Predict-then-Optimize Framework: A Margin-Based Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-11">11 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,288.42,186.54,35.16,9.61"><forename type="first">Mo</forename><surname>Liu</surname></persName>
							<email>liu@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Industrial Engineering and Operations Research</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley, Berkeley</settlement>
									<region>CA, mo</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,277.68,226.11,56.63,9.61"><forename type="first">Paul</forename><surname>Grigas</surname></persName>
							<email>pgrigas@berkeley.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Industrial Engineering and Operations Research</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley, Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,277.92,265.67,56.16,9.61"><forename type="first">Heyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Industrial Engineering and Operations Research</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley, Berkeley</settlement>
									<region>CA</region>
									<country>heyuan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,260.26,305.23,91.47,9.61"><forename type="first">Zuo-Jun</forename><forename type="middle">Max</forename><surname>Shen</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Industrial Engineering and Operations Research</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley, Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_E2zB22A">Active Learning in the Predict-then-Optimize Framework: A Margin-Based Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-11">11 May 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">9D6F6F02C859E6FAF4263AC69BD09E52</idno>
					<idno type="arXiv">arXiv:2305.06584v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-05-12T20:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_P9m3Tqn">active learning</term>
					<term xml:id="_vcSYVAv">predict-then-optimize</term>
					<term xml:id="_wPzhhrS">prescriptive analytics</term>
					<term xml:id="_vmcAHfh">data-driven optimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_F9VF7Kg"><p xml:id="_SxSKNAH">We develop the first active learning method in the predict-then-optimize framework. Specifically, we develop a learning method that sequentially decides whether to request the "labels" of feature samples from an unlabeled data stream, where the labels correspond to the parameters of an optimization model for decision-making.</p><p xml:id="_NvBfRdD">Our active learning method is the first to be directly informed by the decision error induced by the predicted parameters, which is referred to as the Smart Predict-then-Optimize (SPO) loss. Motivated by the structure of the SPO loss, our algorithm adopts a margin-based criterion utilizing the concept of distance to degeneracy and minimizes a tractable surrogate of the SPO loss on the collected data. In particular, we develop an efficient active learning algorithm with both hard and soft rejection variants, each with theoretical excess risk (i.e., generalization) guarantees. We further derive bounds on the label complexity, which refers to the number of samples whose labels are acquired to achieve a desired small level of SPO risk. Under some natural low-noise conditions, we show that these bounds can be better than the naive supervised learning approach that labels all samples. Furthermore, when using the SPO+ loss function, a specialized surrogate of the SPO loss, we derive a significantly smaller label complexity under separability conditions. We also present numerical evidence showing the practical value of our proposed algorithms in the settings of personalized pricing and the shortest path problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="37" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="38" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="39" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="40" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="41" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="42" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="43" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="44" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="45" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="46" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="47" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="48" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="49" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="50" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="51" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="52" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="53" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="54" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="55" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1." xml:id="_nEktyMZ">Introduction</head><p xml:id="_pwshbxx">In many applications of operations research, decisions are made by solving optimization problems that involve some unknown parameters. Typically, machine learning tools are used to predict these unknown parameters, and then an optimization model is used to generate the decisions based on the predictions. For example, in the shortest path problem, we need to predict the cost of each edge in the network and then find the optimal path to route users. Another example is the personalized pricing problem, where we need to predict the purchase probability of a given customer at each possible price and then decide the optimal price. In this predict-then-optimize paradigm, when generating the prediction models, it is natural to consider the final decision error as a loss function to measure the quality of a model instead of standard notions of prediction error. The loss function that directly considers the cost of the decisions induced by the predicted parameters, in contrast to the prediction error of the parameters, is called the Smart Predict-then-Optimize (SPO) loss as proposed by <ref type="bibr" coords="2,134.69,229.98,147.91,9.61" target="#b15">Elmachtoub and Grigas (2022)</ref>. Naturally, prediction models designed based on the SPO loss have the potential to achieve a lower cost with respect to the ultimate decision error.</p><p xml:id="_NpnzryG">In general, for a given feature vector x, calculating the SPO loss requires knowing the correct (in hindsight) optimal decision associated with the unknown parameters. However, a full observation of these parameters, also known as a label associated with x, is not always available. For example, we may not observe the cost of all edges in the graph in the shortest path problem. In practice, acquiring the label of one feature vector instance could be costly, and thus acquiring the labels of all feature vectors in a given dataset would be prohibitively expensive and time-consuming. In such settings, it is essential to actively select the samples for which label acquisition is worthwhile.</p><p xml:id="_kaHzYUB">Algorithms that make decisions about label acquisition lie in the area of active learning. The goal of active learning is to learn a good predictor while requesting a small number of labels of the samples, whereby the labels are requested actively and sequentially from unlabeled samples.</p><p xml:id="_Gu48sSR">Intuitively, if we are very confident about the label of an unlabeled sample based on our current predictor, then we do not have to request the label of it. Active learning is most applicable when the cost of acquiring labels is very expensive. Traditionally in active learning, the selection rules for deciding which samples to acquire labels for are based on measures of prediction error that ignore the cost of the decisions in the downstream optimization problem. Considering the SPO loss in active learning can hopefully reduce the number of labels required while achieving the same cost of decisions, compared to standard active learning methods that only consider measures of prediction error.</p><p xml:id="_5Qj8kcZ">Considering active learning in the predict-then-optimize framework can bridge the gap between active learning and operational decisions, but there are two major challenges when designing algorithms to select samples. One is the computational issue due to the non-convexity and non-Lipschitzness of the SPO loss. When one is concerned with minimizing the SPO loss, existing active learning algorithms are computationally intractable. For example, the general importance weighted active learning (IWAL) algorithm proposed by <ref type="bibr" coords="2,291.34,697.35,116.28,9.61" target="#b5">Beygelzimer et al. (2009)</ref> is impractical to implement, since calculating the "weights" of samples requires a large enumeration of all pairs of predictors.</p><p xml:id="_RJfEkqt">Other active learning algorithms that are designed for the classification problem cannot be extended to minimize the SPO loss directly. Another challenge is to derive bounds for the label complexity of the algorithms and to demonstrate the advantages over supervised learning. Label complexity refers to the number of labels that must be acquired to ensure that the risk of predictor is not greater than a desired threshold. To demonstrate the savings from active learning, label complexity should be smaller than the sample complexity of supervised learning, when achieving the same risk level with respect to the loss function of interest (in our case SPO). <ref type="bibr" coords="3,421.79,192.59,86.68,9.61" target="#b26">Kääriäinen (2006)</ref> shows that, without additional assumptions on the distributions of features and noise, active learning algorithms have the same label complexity as supervised learning. Thus, deriving smaller label complexity for an active learning algorithm under some natural conditions on the noise and feature distributions is a critical but nontrivial challenge.</p><p xml:id="_PhNMUEb">In this paper, we develop the first active learning method in the predict-then-optimize framework. We consider the standard setting of a downstream linear optimization problem where the parameters/label correspond to an unknown cost vector that is potentially related to some feature information. Our proposed algorithm, inspired by margin-based algorithms in active learning, uses a measure of "confidence" associated with the cost vector prediction of the current model to decide whether or not to acquire a label for a given feature. Specifically, the label acquisition decision is based on the notion of distance to degeneracy introduced by El Balghiti et al. <ref type="bibr" coords="3,433.96,398.23,28.13,9.61">(2022)</ref>, which precisely measures the distance from the prediction of the current model to the set of cost vectors that have multiple optimal solutions. Intuitively, the further away the prediction is from degeneracy, the more confident we are that the associated decision is actually optimal. Our proposed margin-based active learning (MBAL-SPO) algorithm has two versions depending on the precise rejection criterion: soft rejection and hard rejection. Hard rejection generally has a smaller label complexity, whereas soft rejection is computationally easier. In any case, when building prediction models based on the actively selected training set, our algorithm will minimize a generic surrogate of the SPO loss over a given hypothesis class. For each version, we demonstrate theoretical guarantees by providing non-asymptotic excess surrogate risk bounds, as well as excess SPO risk bounds, that hold under a natural consistency assumption.</p><p xml:id="_fXWf8Mp">To analyze the label complexity of our proposed algorithm, we define the near-degeneracy function, which characterizes the distribution of optimal predictions near the regions of degeneracy. Based on this definition, we derive upper bounds on the label complexity. We consider a natural low-noise condition, which intuitively says that the distribution of features for a given problem is far enough from degeneracy. Indeed, for most practical problems, the data are expected to be somewhat bounded away from degeneracy. Under these conditions, we show that the label complexity bounds are smaller than those of the standard supervised learning approach. In addition to the results for a general surrogate loss, we also demonstrate improved label complexity results for the SPO+ surrogate loss, proposed by <ref type="bibr" coords="4,203.59,99.12,144.93,9.61" target="#b15">Elmachtoub and Grigas (2022)</ref> to account for the downstream problem, when the distribution satisfies a separability condition. We also conduct some numerical experiments on instances of shortest path problems and personalized pricing problems, demonstrating the practical value of our proposed algorithm above the standard supervised learning approach. Our contributions are summarized below.</p><p xml:id="_TtQAtpf">• We are the first work to consider active learning algorithms in the predict-then-optimize framework. To efficiently acquire labels to train a machine learning model to minimize the decision cost (SPO loss), we propose a margin-based active learning algorithm that utilizes a surrogate loss function.</p><p xml:id="_vVghbCn">• We analyze the label complexity and derive non-asymptotic surrogate and SPO risk bounds for our algorithm, under both soft-rejection and hard-rejection settings. Our analysis applies even when the hypothesis class is misspecified, and we demonstrate that our algorithms can still achieve a smaller label complexity than supervised learning. In particular, under some natural consistency assumptions, we develop the following guarantees.</p><p xml:id="_FT4SQDk">-In the hard rejection case with general surrogate loss functions, we provide generic bounds on the label complexity and the non-asymptotic surrogate and SPO risks in Theorem 1.</p><p xml:id="_GrnBvDx">-In the hard rejection case with the SPO+ surrogate loss, we provide a much smaller non-asymptotic surrogate (and, correspondingly, SPO) risk bound in Theorem 2 under a separability condition. This demonstrates the advantage of the SPO+ surrogate loss over general surrogate losses.</p><p xml:id="_4E8Atwf">-In the soft rejection case with a general surrogate loss, which is computationally easier, we provide generic bounds on the label complexity and the non-asymptotic surrogate and SPO risks in Theorem 3.</p><p xml:id="_ZHvtGvp">-For each case above, we characterize sufficient conditions for which we can specialize the above generic guarantees and demonstrate that the margin-based algorithm achieves sublinear or even finite label complexity. We provide concrete examples of these conditions, and we provide different non-asymptotic bounds in cases where the feasible region of the downstream optimization problem is either a polyhedron or a strongly convex region.</p><p xml:id="_8tjSaXR">In these situations, and under natural low-noise conditions, we demonstrate that our algorithm can achieve much smaller label complexity than the sample complexity of supervised learning.</p><p xml:id="_Qrchw7A">• We demonstrate the practical value of our algorithm by conducting comprehensive numerical experiments in two settings. One is the personalized pricing problem, and the other is the shortest path problem. Both sets of experiments show that our algorithm achieves a smaller SPO risk than the standard supervised learning algorithm given the same number of acquired labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1." xml:id="_SDUpJEM">Example: Personalized Pricing Problem</head><p xml:id="_zmxnDB3">To further illustrate and motivate the integration of active learning into the predict-then-optimize setting, we present the following personalized pricing problem as an example.</p><p xml:id="_2cr7Sw9">Example 1 (Personalized pricing via customer surveys). Suppose that a retailer needs to decide the prices of J items for each customer, after observing the features (personalized information) of the customers. The feature vector of a generic customer is x, and the purchase probability of that customer for item j is d j (p j ), which is a function of the price p j . This purchase probability d j (p j ) is unknown and corrupted with some noise for each customer. Suppose the price for each item is selected from a candidate list {p 1 , p 2 , ..., p I }, which is sorted in ascending order.</p><p xml:id="_85najnd">Then, the pricing problem can be formulated as</p><formula xml:id="formula_0" coords="5,193.78,320.34,346.22,58.54">max w E[ J j=1 I i=1 d j (p i )p i w i,j |x]<label>(1) s.t.</label></formula><p xml:id="_QBmVBUe">I i=1 w i,j = 1, j = 1, 2, ..., J,</p><formula xml:id="formula_1" coords="5,225.14,369.27,314.86,35.44">Aw ≤ b,<label>(1a)</label></formula><p xml:id="_saGFvs6">w i,j ∈ {0, 1}, i = 1, 2, ..., I, j = 1, 2, ..., J.</p><p xml:id="_sbXqxnd">Here, w encodes the decision variables with indices in the set I × J, where w i,j is a binary variable indicating which price for item j is selected. Namely, w i,j = 1 if item j is priced at p i , and otherwise w i,j = 0. The objective (1) is to maximize the expected total revenue of J items by offering price p i for item j. Constraints (1a) require each item to have one price selected. In constraint (1b), A is a matrix with K rows, and b is a vector with K dimensions. Each row of constraints (1b) characterizes one rule for setting prices. For example, if the first row of Aw is w i,j − I i =i w i ,j+1 and the first entry in b is zero, then this constraint further requires that if item j is priced at p i , then the price for the item j + 1 must be no smaller than p i . For another example, if the second row of Aw is i−1 i =1 J j=1 w i ,j , and the second entry of b is 1, then it means that at most one item can be priced below the price p i . Thus, constraints (1b) can characterize different rules for setting prices for J items.</p><p xml:id="_TEPnsDa">Traditionally, the conditional expectation of revenue E[d j (p i )p i |x] must be estimated from the purchasing behavior of the customers. In this example, we consider the possibility that the retailer can give the customers surveys to investigate their purchase probabilities. By analyzing the results of the surveys, the retailer can infer the purchase probability d j (p i )|x for each price point p i and each item j for this customer. Therefore, whenever a survey is conducted, the retailer acquires a noisy estimate of the revenue, denoted by d j (p i )p i |x, at each price point p i and item j.</p><p xml:id="_SjRMYUb">In personalized pricing, first, the retailer would like to build a prediction model to predict</p><formula xml:id="formula_4" coords="6,72.00,137.57,58.85,10.47">E[d j (p i )p i |x]</formula><p xml:id="_fxh642d">given the customer's feature vector x. Then, given the prediction model, the retailer solves the problem (1) to obtain the optimal prices. In practice, when evaluating the quality of the prediction results of d j (p i )p i |x, the retailer cares more about the expected revenue from the optimal prices based on this prediction, rather than the direct prediction error. Therefore, when building the prediction model for d j (p i )p i |x, retailers are expected to be concerned with minimizing SPO loss, rather than minimizing prediction error.</p><p xml:id="_2rjEVY2">One property of ( <ref type="formula" coords="6,184.05,251.85,4.67,9.61" target="#formula_0">1</ref>) is that the objective is linear and can be further written as</p><formula xml:id="formula_5" coords="6,72.00,267.82,157.69,15.17">max w J j=1 K i=1 E[d j (p i )p i |x]w i,j</formula><p xml:id="_dy6QHXa">. By the linearity of the objective, the revenue loss induced by the prediction errors can be written in the form of the SPO loss considered in <ref type="bibr" coords="6,427.42,289.95,112.58,9.61;6,70.72,308.99,29.25,9.61" target="#b15">Elmachtoub and Grigas (2022)</ref>. In general, considering the prediction errors when selecting customers may be inefficient, since smaller prediction errors do not always necessarily lead to smaller revenue losses, because of the properties of the SPO loss examined by <ref type="bibr" coords="6,284.57,347.09,145.93,9.61" target="#b15">Elmachtoub and Grigas (2022)</ref>.</p><p xml:id="_723Kdxc">In Example 1, in practice, there exists a considerable cost to investigate all customers, for example, the labor cost to collect the answers and incentives given to customers to fill out the surveys.</p><p xml:id="_fRzxbPt">Therefore, the retailer would rather intelligently select a limited subset of customers to investigate. This subset of customers should be ideally selected so that the retailer can build a prediction model with small SPO loss, using a small number of surveys.</p><p xml:id="_wSyaJ4K">Active learning is essential to help retailers select representative customers and reduce the number of surveys. Traditional active learning algorithms would select customers to survey based on model prediction errors, which are different from the final revenue of the retailer. On the contrary, when considering the SPO loss, the final revenue is integrated into the learning and survey distribution processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2." xml:id="_Gm6f48g">Literature Review</head><p xml:id="_mCcAkAF">In this section, we review existing work in active learning and the predict-then-optimize framework.</p><p xml:id="_A6PAQ4A">To the best of our knowledge, our work is the first work to bridge these two streams.</p><p xml:id="_8yDKpRX">Active learning. There has been substantial prior work in the area of active learning, focusing essentially exclusively on measures of prediction error. Please refer to <ref type="bibr" coords="6,408.11,639.85,65.80,9.61" target="#b37">Settles (2009)</ref> for a comprehensive review of many active learning algorithms. <ref type="bibr" coords="6,319.93,658.90,88.29,9.61" target="#b9">Cohn et al. (1994)</ref> shows that in the noiseless binary classification problem, active learning can achieve a large improvement in label complexity, compared to supervised learning. It is worth noting that in the general case, <ref type="bibr" coords="6,452.50,696.99,88.78,9.61" target="#b26">Kääriäinen (2006)</ref> provides a lower bound of the label complexity which matches supervised learning. Therefore, to demonstrate the advantages of active learning, some further assumptions on the noise and distribution of samples are required. For the agnostic case where the noise is not zero, many algorithms have also been proposed in the past few decades, for example, <ref type="bibr" coords="7,415.83,117.81,66.91,9.61" target="#b21">Hanneke (2007</ref><ref type="bibr" coords="7,482.75,117.81,57.26,9.61;7,72.00,136.50,59.18,9.61" target="#b10">), Dasgupta et al. (2007)</ref>, <ref type="bibr" coords="7,136.43,136.50,68.66,9.61" target="#b22">Hanneke (2011)</ref>, <ref type="bibr" coords="7,213.45,136.50,117.63,9.61" target="#b1">Balcan et al. (2009), and</ref><ref type="bibr" coords="7,334.74,136.50,91.86,9.61" target="#b2">Balcan et al. (2007)</ref>. These papers focus on binary or multiclass classification problems. <ref type="bibr" coords="7,287.27,155.20,95.87,9.61" target="#b2">Balcan et al. (2007)</ref> proposed a margin-based active learning algorithm, which is used in the noiseless binary classification problem with a perfect linear separator. <ref type="bibr" coords="7,153.68,192.58,94.76,9.61" target="#b2">Balcan et al. (2007)</ref> achieves the label complexity O( −2α ln(1/ )) under uniform distribution, where α ∈ (0, 1) is a parameter defined for the low noise condition and is the desired error rate. <ref type="bibr" coords="7,122.95,229.96,131.55,9.61" target="#b29">Krishnamurthy et al. (2017)</ref> and Gao and Saar-Tsechansky (2020) consider cost-sensitive classification problems in active learning, where the misclassification cost depends on the true labels of the sample.</p><p xml:id="_UBGjzqR">The above active learning algorithms in the classification problem do not extend naturally to real-valued prediction problems. However, the SPO loss is a real-valued function. When considering real-valued loss functions, <ref type="bibr" coords="7,199.88,323.43,94.58,9.61" target="#b7">Castro et al. (2005)</ref> prove convergence rates in the regression problem, and <ref type="bibr" coords="7,93.96,342.12,154.99,9.61" target="#b38">Sugiyama and Nakajima (2009)</ref> and <ref type="bibr" coords="7,274.85,342.12,80.78,9.61" target="#b6">Cai et al. (2016)</ref> also consider squared loss as the loss function. <ref type="bibr" coords="7,116.83,360.81,117.07,9.61" target="#b5">Beygelzimer et al. (2009)</ref> propose an importance-weighted algorithm (IWAL) that extends disagreement-based methods to real-valued loss functions. However, it is intractable to directly use the IWAL algorithm in the SPO framework. Specifically, it requires solving a non-convex problem at each iteration, which may have to enumerate all pairs of predictor candidates even when the hypothesis set is finite.</p><p xml:id="_6bkxTHw">Predict-then-optimize framework. In recent years, there has been a growing interest in developing machine learning models that incorporate the downstream optimization problem. For example, <ref type="bibr" coords="7,72.00,491.73,68.72,9.61" target="#b4">Bertsimas and</ref><ref type="bibr" coords="7,144.37,491.73,150.95,9.61">Kallus (2020), Kao et al. (2009)</ref>, <ref type="bibr" coords="7,303.74,491.73,145.75,9.61" target="#b15">Elmachtoub and Grigas (2022)</ref>, <ref type="bibr" coords="7,457.91,491.73,78.83,9.61" target="#b44">Zhu et al. (2022)</ref>, <ref type="bibr" coords="7,72.00,510.43,89.35,9.61" target="#b13">Donti et al. (2017)</ref> and <ref type="bibr" coords="7,186.39,510.43,134.26,9.61" target="#b23">Ho and Hanasusanto (2019)</ref> propose frameworks that somehow relate the learning problem to the downstream optimization problem. In our work, we consider the Smart Predict-then-Optimize (SPO) framework proposed by <ref type="bibr" coords="7,330.54,547.81,144.51,9.61" target="#b15">Elmachtoub and Grigas (2022)</ref>. Because the SPO loss function is nonconvex and non-Lipschitz, the computational and statistical properties of the SPO loss in the fully supervised learning setting have been studied in several recent works. <ref type="bibr" coords="7,72.00,603.89,154.95,9.61" target="#b15">Elmachtoub and Grigas (2022)</ref> provide a surrogate loss function called SPO+ and show the consistency of this loss function. <ref type="bibr" coords="7,236.29,622.58,121.82,9.61" target="#b17">Elmachtoub et al. (2020)</ref>, <ref type="bibr" coords="7,367.28,622.58,86.41,9.61" target="#b32">Loke et al. (2022)</ref>, <ref type="bibr" coords="7,462.86,622.58,79.26,9.61;7,70.72,641.27,28.30,9.61" target="#b12">Demirovic et al. (2020)</ref>, <ref type="bibr" coords="7,107.39,641.27,107.49,9.61" target="#b11">Demirović et al. (2019)</ref>, <ref type="bibr" coords="7,223.25,641.27,111.73,9.61">Mandi and Guns (2020)</ref>, <ref type="bibr" coords="7,343.35,641.27,89.99,9.61">Mandi et al. (2020)</ref>, and <ref type="bibr" coords="7,462.77,641.27,77.23,9.61;7,70.72,659.97,31.05,9.61" target="#b39">Tang and Khalil (2022)</ref> all develop new applications and computational frameworks for minimizing the SPO loss in various settings. El <ref type="bibr" coords="7,180.80,678.66,101.22,9.61" target="#b14">Balghiti et al. (2022)</ref> consider generalization error bounds of the SPO loss function. <ref type="bibr" coords="7,116.91,697.35,173.82,9.61" target="#b24">Ho-Nguyen and Kılınç-Karzan (2022)</ref>, <ref type="bibr" coords="7,298.86,697.35,36.40,9.61" target="#b31">Liu and</ref><ref type="bibr" coords="7,338.70,697.35,87.60,9.61">Grigas (2021), and</ref><ref type="bibr" coords="7,429.74,697.35,74.46,9.61" target="#b25">Hu et al. (2022)</ref> further consider risk bounds of different surrogate loss functions in the SPO setting. There is also a large body of work more broadly in the area of decision-focused learning, which is largely concerened with differentiating through the parameters of the optimization problem, as well as other techniques, for training. See, for example, <ref type="bibr" coords="8,217.49,118.56,114.10,9.61" target="#b0">Amos and Kolter (2017)</ref>, <ref type="bibr" coords="8,339.99,118.56,92.31,9.61" target="#b42">Wilder et al. (2019)</ref>, <ref type="bibr" coords="8,440.70,118.56,96.08,9.61" target="#b3">Berthet et al. (2020)</ref>, <ref type="bibr" coords="8,72.00,137.62,93.56,9.61" target="#b8">Chung et al. (2022)</ref>, the survey paper <ref type="bibr" coords="8,259.99,137.62,96.07,9.61" target="#b28">Kotary et al. (2021)</ref>, and the references therein. Recently there has been growing attention on problems with nonlinear objectives, where estimating the conditional distribution of parameters is often needed; see, for example, <ref type="bibr" coords="8,425.81,175.76,110.84,9.61" target="#b27">Kallus and Mao (2023)</ref>, <ref type="bibr" coords="8,72.00,194.82,93.32,9.61" target="#b20">Grigas et al. (2021)</ref> and <ref type="bibr" coords="8,190.28,194.82,117.74,9.61" target="#b16">Elmachtoub et al. (2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3." xml:id="_rtqAxr6">Organization</head><p xml:id="_BdD98b3">The remainder of the paper is organized as follows. In Section 2, we introduce preliminary knowledge on the predict-then-optimize framework and active learning, including the SPO loss function, label complexity, and the SPO+ surrogate loss function. Then, we present our active learning algorithm, margin-based active learning (MBAL-SPO), in Section 3. We first present an illustration to motivate the incorporation of the distance to degeneracy in the active learning algorithm in 3.1. Next, we analyze the risk bounds and label complexities for both hard and soft rejection in Section 4. To demonstrate the strength of our algorithm over supervised learning, we consider natural low-noise conditions and derive sublinear label complexity in Section 5. We demonstrate the advantage of using SPO+ as the surrogate loss in some cases by providing a smaller label complexity. We further provide concrete examples of these low-noise conditions. In Section 6, we test our algorithm using synthetic data in two problem settings: the shortest path problem and the personalized pricing problem. Lastly, we point out some future research directions in Section 7. The omitted proofs, sensitivity analysis of the numerical experiments, and additional numerical results are provided in the Appendices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2." xml:id="_j4epWEW">Preliminaries</head><p xml:id="_MApPgat">We first introduce some preliminaries about active learning and the predict-then-optimize framework.</p><p xml:id="_7XxWPst">In particular, we introduce the SPO loss function, we discuss the goals of active learning in the predict-then-optimize framework, and we review the SPO+ surrogate loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1." xml:id="_d8sACHY">Predict-then-Optimize Framework and Active Learning</head><p xml:id="_EWySNgT">Let us begin by formally describing the "predict-then-optimize" framework and the "Smart Predictthen-Optimize (SPO)" loss function. We assume that the downstream optimization problem has a linear objective, but the cost vector of the objective, c ∈ C ⊆ R d , is unknown when the problem is solved to make a decision. Instead, we observe a feature vector, x ∈ X ⊆ R p , which provides auxiliary information that can be used to predict the cost vector. The feature space X and cost vector space C are assumed to be bounded. We assume there is a fixed but unknown distribution D over pairs (x, c) living in X × C. The marginal distribution of x is denoted by D X . Let w ∈ S denote the decision variable of the downstream optimization problem, where the feasible region S ⊆ R d is a convex and compact set that is assumed to be fully known to the decision-maker. To avoid trivialities, we also assume throughout that the set S is not a singleton. Given an observed feature vector x, the ultimate goal is to solve the contextual stochastic optimization problem:</p><formula xml:id="formula_6" coords="9,230.79,181.47,151.94,17.21">min w∈S E c [c T w|x] = min w∈S E[c|x] T w.</formula><p xml:id="_KjgAg7t">(2)</p><p xml:id="_q6gSZAv">From the equivalence in (2), observe that the downstream optimization problem in the predict-thenoptimize framework relies on a prediction (otherwise referred to as estimation) of the conditional</p><formula xml:id="formula_7" coords="9,72.00,248.75,92.43,10.47">expectation E c [c|x].</formula><p xml:id="_VAMnVJc">Given such a prediction ĉ, a decision is made by then solving the deterministic version of the downstream optimization problem:</p><formula xml:id="formula_8" coords="9,264.97,295.58,275.03,15.43">P (ĉ) : min w∈S ĉT w.<label>(3)</label></formula><p xml:id="_bgYXCxF">For simplicity, we assume w * : R d → S is an oracle for solving (3), whereby w * (ĉ) is an optimal solution of P (ĉ).</p><p xml:id="_nbGCm4F">Our goal is to learn a cost vector predictor function h : X → R d , so that for any newly observed feature vector x, we first make prediction h(x) and then solve the optimization problem P (h(x))</p><p xml:id="_ttcVN8A">in order to make a decision. This predict-then-optimize paradigm is prevalent in applications of machine learning to problems in operations research. We assume the predictor function h is within a compact hypothesis class H of functions on X → R d . We say the hypothesis class is well-specified if E[c|x] ∈ H. In our analysis, the well-specification is not required. The active learning methods we consider herein rely on using a variant of empirical risk minimization to select h ∈ H by minimizing an appropriately defined loss function. Our primary loss function of interest in the predict-thenoptimize setting is the SPO loss, introduced by Elmachtoub and Grigas (2022), which characterizes the regret in decision error due to an incorrect prediction and is formally defined as</p><formula xml:id="formula_9" coords="9,236.32,555.44,143.92,12.26">SPO (ĉ, c) := c T w * (ĉ) − c T w * (c),</formula><p xml:id="_vMbUDfu">for any cost vector prediction ĉ and realized cost vector c. We further define the SPO risk of</p><formula xml:id="formula_10" coords="9,72.00,604.06,324.87,10.47">a prediction function h ∈ H as R SPO (h) := E (x,c)∼D [ SPO (h(x), c)],</formula><p xml:id="_cVyGG4g">and the excess risk of h as</p><formula xml:id="formula_11" coords="9,72.00,622.72,133.44,10.47">R SPO (h) − inf h ∈H R SPO (h ).</formula><p xml:id="_QGKRbG6">(Throughout, we typically remove the subscript notation from the expectation operator when it is clear from the context.) Notice that a guarantee on the excess SPO risk implies a guarantee that holds "on average" with respect to x for the contextual stochastic optimization problem (2).</p><p xml:id="_6kbxt3m">As previously described, in many situations acquiring cost vector data may be costly and timeconsuming. The aim of active learning is to choose which feature samples x to label sequentially and interactively, in contrast to standard supervised learning which acquires the labels of all the samples before training the model. In the predict-then-optimize setting, acquiring a "label" corresponds to collecting the cost vector data c that corresponds to a given feature vector x. An active learner aims to use a small number of labeled samples to achieve a small prediction error. In the agnostic case, the noise is nonzero and the smallest prediction error is the Bayes risk, which is R * SPO = inf h∈H R SPO (h) &gt; 0. The goal of an active learning method is to then find a predictor ĥ trained on the data with the minimal number of labeled samples, such that R SPO ( ĥ) ≤ R * SPO + , with high probability and where &gt; 0 is a given risk error level. The number of labels acquired to achieve this goal is referred to as the label complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2." xml:id="_QReX7Hx">Surrogate Loss Functions and SPO+</head><p xml:id="_X6ckTS9">Due to the potential non-convexity and even non-continuity of the SPO loss, a common approach is to consider surrogate loss functions that have better computational properties and are still (ideally) aligned with the original SPO loss. In our work, the surrogate loss function : R d × R d → R + is assumed to be continuous. The surrogate risk of a predictor h ∈ H is denote by R (h), and the corresponding minimum risk is denoted by R * := min h∈H R (h).</p><p xml:id="_6EkYtVw">As a special case of the surrogate loss function , <ref type="bibr" coords="10,330.45,371.55,151.84,9.61" target="#b15">Elmachtoub and Grigas (2022)</ref> proposed a convex surrogate loss function, called the SPO+ loss, which is defined by</p><formula xml:id="formula_12" coords="10,182.08,418.76,252.40,17.21">SPO+ (ĉ, c) := max w∈S (c − 2ĉ) T w + 2ĉ T w * (c) − c T w * (c),</formula><p xml:id="_mZBEMCV">and is an upper bound on the SPO loss, i.e., SPO (ĉ, c) ≤ SPO+ (ĉ, c) for any ĉ ∈ Ĉ and c ∈ C. <ref type="bibr" coords="10,72.00,469.53,146.12,9.61" target="#b15">Elmachtoub and Grigas (2022)</ref> demonstrate the computational tractability of the SPO+ surrogate loss, conditions for Fisher consistency of the SPO+ risk with respect to the true SPO risk, as well as strong numerical evidence of its good performance with respect to the downstream optimization task. <ref type="bibr" coords="10,99.25,526.42,108.89,9.61" target="#b31">Liu and Grigas (2021)</ref> further demonstrate sufficient conditions that imply that when the excess surrogate SPO+ risk of a prediction function h is small, the excess true SPO risk of a prediction function h is also small. This property not only holds for the SPO+ loss, but also for other surrogate loss functions, such as the squared 2 loss (see, for details, Ho-Nguyen and Kılınç-Karzan (2022)). Importantly, the SPO+ loss still accounts for the downstream optimization problem and the structure of the feasible region S, in contrast to losses like the 2 loss that focus only on prediction error. As will be shown in Theorem 2, compared to the general surrogate loss functions that satisfy Assumption 1 in our analysis, the SPO+ loss function achieves a smaller label complexity by utilizing the structure of the downstream optimization problem.</p><p xml:id="_R3CpMjJ">Notations. Let • on w ∈ R d be a generic norm. Its dual norm is denoted by • * , which is defined by c * = max w: w ≤1 c T w. We denote the set of extreme points in the feasible region S by S, and the diameter of the set S ⊂ R d by D S := sup w,w ∈S { w − w }. The "linear optimization gap" of S with respect to cost vector c is defined as ω S (c) := max w∈S {c T w} − min w∈S {c T w}. We further define ω S (C) := sup c∈C {ω S (c)} and ρ(C) := max c∈C { c }, where again C is the domain of possible realizations of cost vectors under the distribution D. We denote the cost vector space of the prediction range by Ĉ, i.e., Ĉ := {c ∈ R d : c = h(x), h ∈ H, x ∈ X }. For the surrogate loss function , we define ω ( Ĉ, C) := sup ĉ∈ Ĉ,c∈C { (ĉ, c)}. We also denote ρ(C, Ĉ) := max{ρ(C), ρ( Ĉ)} for the general norm. We use N (µ, σ 2 ) to denote the multivariate normal distribution with center µ and covariance matrix σ 2 . We use R + to denote [0, +∞). When conducting the asymptotic analysis, we adopt the standard notations O(•) and Ω(•). We further use Õ(•) to suppress the logarithmic dependence. We use I to refer to the indicator function, which outputs 1 if the argument is true and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3." xml:id="_u8XPpRA">Margin-Based Algorithm</head><p xml:id="_sREJg8K">In this section, we develop and present the margin-based algorithm in the predict-then-optimize framework (MBAL-SPO). We first illustrate and motivate the algorithm in the polyhedral case.</p><p xml:id="_fX9KzkZ">Then, we provide some conditions for the noise distribution and surrogate loss functions for our MBAL-SPO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1." xml:id="_QuDRVcM">Illustration and Algorithm</head><p xml:id="_h8hpn8f">Let us introduce the idea of the margin-based algorithm with the following two examples, which illustrate the value of integrating the SPO loss into active learning. Particularly, given the current training set and predictor, it is very likely that some features will be more informative and thus more valuable to label than others. In general, the "value" of labeling a feature depends on the associated prediction error (Figure <ref type="figure" coords="11,239.03,472.28,4.79,9.61" target="#fig_24">1</ref>) and the location of the prediction relative to the structure of the feasible region S (Figure <ref type="figure" coords="11,207.83,491.03,4.18,9.61" target="#fig_2">2</ref>). In Figure <ref type="figure" coords="11,270.14,491.03,4.18,9.61" target="#fig_24">1</ref>, the feasible region S is polyhedral and the yellow arrow represents − ĥ(x). Within this example, for the purpose of illustration, let us assume the hypothesis class is well-specified. Our goal then is to find a good predictor h from the hypothesis class H, such that h(x) is close to E[c|x]. However, because c|x is random, the empirical best predictor ĥ in the training set may not exactly equal the true predictor h * , where h * (x) = E[c|x]. Given one feature x, the prediction is ĉ = ĥ(x), the negative of which is shown in Figures <ref type="figure" coords="11,404.00,584.79,46.12,9.61" target="#fig_24">1a and 1b</ref>. Intuitively, when the training set gets larger, the empirical best predictor ĥ should get closer to h * , and ĥ(x) should get closer to E[c|x]. Thus, we can construct a confidence region around ĥ(x), such that E[c|x] is within this confidence region with some high probability. Examples of confidence regions for the estimation of E[c|x] given the current training set are shown in the green circles in Figure <ref type="figure" coords="11,508.75,659.79,4.29,9.61" target="#fig_24">1</ref>. The optimal solution w * (ĉ) is the extreme point indicated in Figure <ref type="figure" coords="11,384.49,678.54,4.35,9.61" target="#fig_24">1</ref>, and the normal cone at w * (ĉ) illustrates the set of all cost vectors whose optimal solution is also w * (ĉ). In addition, those cost vectors that lie on the boundary of the normal cone are the cost vectors that can lead to multiple optimal decisions (they will be defined as degenerate cost vectors in Definition 1 later). In cases when the confidence region is large (e.g., because the training set is small), as indicated in Figure <ref type="figure" coords="12,71.45,117.79,9.52,9.61" target="#fig_24">1a</ref>, the green circle intersects with the degenerate cost vectors, which means that some vectors within the confidence region for estimating E[c|x] could lead to multiple optimal decisions. When the confidence region is smaller (e.g., because the training set is larger), as indicated in Figure <ref type="figure" coords="12,71.45,173.84,9.82,9.61" target="#fig_24">1b</ref>, the green circle does not intersect with the degenerate cost vectors, which means the optimal decision of E[c|x] is the same as the optimal decision of ĉ = ĥ(x), w * (ĉ), with high probability. Thus, when the confidence region of E[c|x] does not intersect with the degenerate cost vectors, the optimal decision based on the current estimated cost vector will lead to the correct optimal decision with high probability, and the SPO loss will be zero. This in turn suggests that the label corresponding to x is not informative (and we do not have to acquire it), when the confidence region centered at the prediction ĥ(x) is small enough to not intersect those degenerate cost vectors. Figure <ref type="figure" coords="12,498.32,285.94,5.41,9.61" target="#fig_2">2</ref>  shows that considering the SPO loss function reduces the label complexity when the confidence regions of the cost vector are the same size. In Figure <ref type="figure" coords="12,325.18,529.21,4.18,9.61" target="#fig_2">2</ref>, both green circles have the same radius but their locations are different. In Figure <ref type="figure" coords="12,253.79,547.90,9.15,9.61" target="#fig_2">2a</ref>, the confidence region for E[c|x] is close to the degenerate cost vectors, and thus the cost vectors within the confidence region will lead to multiple optimal decisions. In Figure <ref type="figure" coords="12,167.39,585.26,9.55,9.61" target="#fig_2">2b</ref>, the confidence region for E[c|x] is far from the degenerate cost vectors, and therefore acquiring a label for x is less informative, as we are more confident that ĥ(x) leads to the correct optimal decision due to the more central location of the confidence region.</p><p xml:id="_Zd7yjFD">The above two examples highlight that the confidence associated with a prediction ĥ(x) is crucial to determine whether it is valuable to acquire a true label c associated with x.  The SPO loss function reduces the label complexity, given the same size confidence region.</p><p xml:id="_VqETeT6">precisely measures the distance of a prediction ĥ(x) to those degenerate cost vectors with multiple optimal solutions and thus provides the correct way to measure confidence about the location of a </p><formula xml:id="formula_13" coords="13,72.00,539.03,469.52,30.86">o := {ĉ ∈ R d : P (ĉ) has multiple optimal solutions}. Given a norm • on R d , the distance to degeneracy of the prediction ĉ is ν S (ĉ) := inf c∈C o { c − ĉ }.</formula><p xml:id="_xYNYzWM">The distance to degeneracy can be easily computed in some special cases, for example, when the feasible region S is strongly convex or in the case of a polyhedral feasible region with known extreme point representations. El Balghiti et al. ( <ref type="formula" coords="13,309.86,616.87,20.23,9.61">2022</ref>) provide the exact formulas of the distance to degeneracy function in these two special cases. In particular, in the case of a polyhedral feasible region with extreme points {v j : j = 1, ..., K}, that is, S = conv(v 1 , . . . , v K ), Theorem 8 of El Balghiti et al. ( <ref type="formula" coords="13,103.19,674.33,19.89,9.61">2022</ref>) says that the distance to degeneracy of any vector c ∈ R d satisfies the following equation:</p><formula xml:id="formula_14" coords="13,221.35,696.55,313.98,26.57">ν S (c) = min j:v j =w * (c) c T (v j − w * (c)) v j − w * (c) * . (<label>4</label></formula><formula xml:id="formula_15" coords="13,535.33,705.17,4.67,9.61">)</formula><p xml:id="_GA7am9v">Theorem 7 of El Balghiti et al. ( <ref type="formula" coords="14,239.16,80.43,19.52,9.61">2022</ref>), on the other hand, says that ν S (c) = c whenever S is a strongly convex set. As mentioned, the distance to degeneracy ν S (ĉ) provides a measure of "confidence" regarding the cost vector prediction ĉ and its implied decision w * (ĉ). This observation motivates us to design a margin-based active learning algorithm, whereby if the distance to degeneracy ν S (ĉ) is greater than some threshold (depending on the number of iterations and samples acquired so far), then we are confident enough to label it using our current model without asking for the true label.</p><p xml:id="_83Z82nH">Our margin-based method is proposed in Algorithm 1. The idea of the margin-based algorithm can be explained as follows. At iteration t, we first observe an unlabeled feature vector x t , which follows distribution D X . Given the current predictor h t−1 , we calculate the distance to the degeneracy ν S (h t−1 (x t )) of this unlabeled sample x t . If the distance to degeneracy ν S (h t−1 (x t )) is greater than the threshold b t−1 , then we reject x t with some probability 1 − p. If p = 0, this rejection is referred to as a hard rejection; when p &gt; 0, this rejection is referred to as a soft rejection. If a soft-rejected sample is not ultimately rejected, we acquire a label (cost vector) c t associated with x t and add the sample (x t , c t ) to the set Wt . On the other hand, if ν S (h t−1 (x t )) &lt; b t−1 , then we acquire a label (cost vector) c t associated with x t and add the sample (x t , c t ) to the working training set W t . At each iteration, we update the predictor h t by computing the best predictor within a subset of the hypothesis class H t ⊆ H that minimizes the empirical surrogate risk measured on the labeled samples. Note that Algorithm 1 maintains two working sets, Wt and W t , due to the two different types of labeling criteria. To ensure that the expectation of empirical loss is equal to the expectation of the true loss, we need to assign weight 1 p to the soft-rejection samples in the set Wt . It is assumed throughout that the sequence (x 1 , c 1 ), (x 2 , c 2 ), . . . is an i.i.d. sequence from the distribution D.</p><p xml:id="_rq7ZSwM">Two versions of the MBAL-SPO have their own advantages. When using hard rejection, we update the set of predictors H t according to Line 20 in Algorithm 1, and the value of p is set to zero. In contrast, in the soft rejection case, we keep H t as the entire hypothesis class H for all iterations, and the value of p is non-zero. In comparison, hard rejection can result in a smaller label complexity because p = 0, while soft rejection can reduce computational complexity by keeping H t as the whole hypothesis class H. Please see the discussion in Sections 4.2 and 4.4 for further details.</p><p xml:id="_AXhCeF5">In Algorithm 1, the case where ν S (h t−1 (x t )) ≥ b t−1 intuitively corresponds to the case where the confidence region of h t−1 (x t ) does not intersect with the degenerate cost vectors. Hence, we are sufficiently confident that the optimal decision w * (h t (x t )) is equal to w * (h * (x t )), where h * is a model that minimizes the SPO risk. Thus, we do not have to ask for the label of x t . Lemma 1 further characterizes the conditions when two predictions lead to the same decision when the feasible region S is polyhedral.</p><p xml:id="_TCEvKMt">Lemma 1 (Conditions for identical decisions in polyhedral feasible regions.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_WRFte7d">Suppose that the feasible region</head><formula xml:id="formula_16" coords="15,72.00,98.59,468.00,31.11">S is polyhedral. Given two cost vectors c 1 , c 2 ∈ R d , if c 1 − c 2 &lt; max{ν S (c 1 ), ν S (c 2 )}, then it holds that w * (c 1 ) = w * (c 2 ).</formula><p xml:id="_fNKkXDN">In other words, the optimal decisions for c 1 and c 2 are the same.</p><p xml:id="_UZkDFkn">Lemma 1 implies that given one prediction of the cost vector, when its distance to degeneracy is larger than the radius of its confidence region, then all the predictions within this confidence region will lead to the same decision. Moreover, if the optimal prediction is also within this confidence region, the SPO loss of this prediction is zero.</p><p xml:id="_jwCUqjv">The computational complexity of Algorithm 1 depends on the choice of the surrogate loss we use.</p><p xml:id="_gzUx6Zc">As discussed earlier, calculating the distance to degeneracy ν S (h(x)) is efficient in some special cases.</p><p xml:id="_WeXtq5y">In general, in the polyhedral case when a convex hull representation is not available, a reasonable heuristic is to only compute the minimum in (4) with respect to the neighboring extreme points of w * (c). Alternatively, we observe that the objective inside the minimum in ( <ref type="formula" coords="15,452.20,320.63,4.67,9.61" target="#formula_14">4</ref>) is quasiconcave.</p><p xml:id="_zN8MNc7">Therefore, we can relax the condition that v j be an extreme point and still recover an extreme point solution. One can solve the resulting problem with a Frank-Wolfe type method, for example, see <ref type="bibr" coords="15,90.08,378.83,121.37,9.61" target="#b43">Yurtsever and Sra (2022)</ref>. The computational complexity of updating h t in Line 19 depends on the choice of hypothesis class H. In the case of soft rejection, we maintain H t = H for all t and the update is the same as performing empirical risk minimization in H, which can be efficiently computed exactly or approximately for most common choices of H, including linear and nonlinear models. In the case of hard rejections, H t is now the intersection of t different level sets. Thus, min h∈H t ˆ t (h) is a minimization problem with t level set constraints. The complexity of solving this problem again depends on the choice of H and can often be solved efficiently. For example, in the case of linear models or nonlinear models such as neural networks, a viable approach would be to apply stochastic gradient descent to a penalized version of the problem or to apply a Lagrangian dual-type algorithm. In practice, since the constraints may be somewhat loose, we may simply ignore them and still obtain good results. Finally, we note that in both cases of hard rejection and soft rejection, although we have to solve a different optimization problem at every iteration, these optimization problems do not change much from one iteration to the next, and therefore using a warm-start strategy that uses h t−1 as the initialization for calculating h t will be very effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1." xml:id="_EW8EnqC">Surrogate Loss Function and Noise Distribution</head><p xml:id="_vKBBun4">Without further assumptions on the distribution of noise and features, the label complexity of an active learning algorithm can be the same as the sample complexity of supervised learning, as shown in <ref type="bibr" coords="15,118.39,716.04,85.71,9.61" target="#b26">Kääriäinen (2006)</ref>. Therefore, we make several natural assumptions in order to analyze Algorithm 1 Margin-Based Active Learning for SPO (MBAL-SPO)</p><p xml:id="_hNQAAtf">1: Input: Exploration probability p, a sequence of cut-off values {b t }, a sequence {r t }, and a constant ϑ.</p><p xml:id="_t9B8v5g">2: Initialize the working sets W 0 ← ∅, W0 ← ∅ and H 0 ← H.</p><p xml:id="_2HYD9zZ">3: Arbitrarily pick one h 0 ∈ H, n 0 ← 0.</p><p xml:id="_X9KmnnT">4: for t from 1, 2, ..., T do 5:</p><p xml:id="_v9sK33z">Draw one sample x t from D X .</p><p xml:id="_zqK3A9k">6:</p><formula xml:id="formula_17" coords="16,77.98,212.51,153.30,28.19">if ν S (h t−1 (x t )) ≥ b t−1 then 7:</formula><p xml:id="_Y6QpCqb">Flip a coin with heads-up probability p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_x8eMvWw">8:</head><p xml:id="_zys7B8g">if the coin gets heads-up then 9:</p><p xml:id="_ZAG9jke">Acquire a "true" label c t of x t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Y77sEBB">10:</head><p xml:id="_bxTeKkQ">Update working set Wt ← Wt−1 ∪ {(x t , c t )}. Set n t ← n t−1 + 1. Acquire a "true" label c t of x t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_WcKB7Vw">16:</head><p xml:id="_UFWmRbz">Update working set W t ← W t−1 ∪ {(x t , c t )}. Set n t ← n t−1 + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_4JjVfsS">17:</head><p xml:id="_rKSc2Pu">end if 18:</p><formula xml:id="formula_18" coords="16,73.38,436.80,311.76,33.55">Let ˆ t (h) ← 1 t (x,c)∈W t (h(x), c) + 1 p (x,c)∈ Wt (h(x), c) . 19: Update h t ← arg min h∈H t−1 ˆ t (h) and ˆ t, * ← min h∈H t−1 ˆ t (h).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_hu8f9Zr">20:</head><p xml:id="_jNpBc7W">Optionally update the confidence set of the predictor</p><formula xml:id="formula_19" coords="16,73.38,474.66,468.75,70.15">H t by H t ← {h ∈ H t−1 : ˆ t (h) ≤ ˆ t, * + r t + ϑ t t−1 i=0 b 2 i }. 21: end for 22: Return h T .</formula><p xml:id="_8SnFF7N">the convergence and label complexity of our algorithm. Recall that the optimal SPO and surrogate risk values are defined as:</p><formula xml:id="formula_20" coords="16,192.90,612.24,226.21,17.29">R * SPO := min h∈H R SPO (h), and R * := min h∈H R (h).</formula><p xml:id="_zPbSBWG">We define H * as the set of all optimal predictors for the SPO risk, i.e., H * = {h ∈ H : R SPO (h) ≤ R SPO (h ), for all h ∈ H} and H * as the set of all optimal predictors for the risk of the surrogate loss, i.e., H * = {h ∈ H : R (h) ≤ R (h ), for all h ∈ H}. We also use the notation R * SPO+ and H * SPO+ when the surrogate loss is SPO+. We define the essential sup norm of a function h : X → R d as h ∞ := inf{α ≥ 0 : h(x) ≤ α for almost every x ∈ X }, with respect to the marginal distribution of x and where • is the norm defining the distance to degeneracy (Definition 1). Given a set</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_4VP58kZ">H ⊆ H, we further define the distance between a fixed predictor function h and H as Dist</head><formula xml:id="formula_21" coords="17,72.00,99.34,468.00,29.38">H (h) := inf h ∈H { h − h ∞ }.</formula><p xml:id="_57aYZhT">Assumption 1 states our main assumptions on the surrogate loss function that we work with.</p><p xml:id="_8u2q9Yv">Assumption 1 (Consistency and error bound condition). The hypothesis class H is a nonempty compact set w.r.t. to the sup norm, and the surrogate loss function :</p><formula xml:id="formula_22" coords="17,461.22,179.68,78.78,11.71">R d × R d → R + is</formula><p xml:id="_j5J8qHK">continuous and satisfies:</p><p xml:id="_3h8DaQB">(1) H * ⊆ H * , i.e., the minimizers of the surrogate risk are also minimizers of the SPO risk.</p><p xml:id="_cPW8qwF">(2) There exists a non-decreasing function φ : R + → R + with φ(0) = 0 such that for any h ∈ H, for any &gt; 0,</p><formula xml:id="formula_23" coords="17,230.56,284.55,174.02,12.26">R (h) − R * ≤ ⇒ Dist H * (h) ≤ φ( ).</formula><p xml:id="_h7vMFmY">Assumption 1.( <ref type="formula" coords="17,158.71,316.11,4.60,9.61" target="#formula_0">1</ref>) states the consistency of the surrogate loss function. Note that since H is a nonempty compact set and is a continuous function, H * is also a nonempty compact set. On the other hand, the SPO loss is generally discontinuous so H * is not necessarily compact, although the consistency assumption H * ⊆ H * ensures that H * is nonempty. Assumption 1.( <ref type="formula" coords="17,473.01,372.84,4.56,9.61">2</ref>) is a type of error bound condition on the risk of the surrogate loss, wherein the function φ provides an upper bound of the sup norm between the predictor h and the set of optimal predictors H * whenever the surrogate risk of h is close to the minimum surrogate risk value. By Assumption 1.(2), when the excess surrogate risk of h becomes smaller, h becomes closer to the set H * , which implies that the prediction h(x) also gets closer to an optimal prediction h * (x) for any given x. As a consequence, the distance to degeneracy ν S (h(x)) also converges to ν S (h * (x)) for almost all x ∈ X . with respect to the distribution of x ∼ D X is defined as:</p><formula xml:id="formula_24" coords="17,223.16,646.67,165.69,17.29">Ψ(b) := P inf h * ∈H * {ν S (h * (x))} ≤ b .</formula><p xml:id="_x8jQBSN">The near-degeneracy function Ψ measures the probability that the distance to degeneracy of h * (x) is smaller than b, when x follows the marginal distribution of x in D X . If H * contains more than one optimal predictor, the near-degeneracy function Ψ considers the distribution of the smallest distance to degeneracy of all optimal predictors h * . Intuitively, when Ψ(b) is smaller, the density allocated near the points of degeneracy becomes smaller, which means Algorithm 1 has a larger probability to reject samples, and achieves smaller label complexity. This intuition is characterized in Lemma 2.</p><p xml:id="_wcnjT8p">Lemma 2 (Upper bound on the expected number of acquired labels). Suppose that Assumption 1 holds. In Algorithm 1, if h t satisfies Dist H * (h t ) ≤ b t for all iterations t ≥ 0, then the expected number of acquired labels after T total iterations is at most pT + T t=1 Ψ(2b t−1 ).</p><p xml:id="_H5m6WSx">Lemma 2 provides an upper bound for the expected number of acquired labels up to time t, by utilizing the near-degeneracy function Ψ. Note that in the soft rejection case, if p &gt; 0 and p is independent of T , Lemma 2 implies that this upper bound grows linearly in T . However, if we know the value of T before running the algorithm, then this upper bound can be reduced to a sublinear order by setting p as a function of T . On the other hand, if we can set p = 0, i.e., in the hard rejection case, the upper bound in Lemma 2 is sublinear if</p><formula xml:id="formula_25" coords="18,395.39,335.60,107.49,15.11">T t=1 Ψ(2b t ) is sublinear.</formula><p xml:id="_9QJ7gJR">As will be shown later in Proposition 4, in the hard rejection case, we achieve a sublinear and sometimes even finite label complexity when the near-degeneracy function Ψ satisfies certain conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4." xml:id="_MgmVsmr">Guarantees and Analysis for the Margin-Based Algorithm</head><p xml:id="_EeSJhvr">In this section, we analyze the convergence and label complexity of MBAL-SPO in various settings.</p><p xml:id="_nSB9DyG">We first review some preliminary information about sequential complexity and covering numbers in Section 4.1. Next, we analyze the label complexity under hard rejections and soft rejections in Sections 4.2 and 4.4, respectively. In both sections, we develop non-asymptotic surrogate and SPO risk error bounds. We also develop bounds for the label complexity that, under certain conditions, can be much smaller than supervised learning. In Section 4.3, we further provide tighter SPO+ and SPO risk bounds when using the SPO+ surrogate under a separability condition. At the end of this section, we discuss how to set the values of the parameters in practice in MBAL-SPO. All missing proofs are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1." xml:id="_3KPfXnq">Preliminaries About Sequential Complexity</head><p xml:id="_2BU5k47">In Algorithm 1, the samples in the training set are not i.i.d., instead, whether to acquire the label at iteration t depends on the historical label results. One of the challenges in analyzing the convergence and label complexity of the margin-based algorithm stems from the non i.i.d. samples. In this section, we review some techniques that characterize the convergence of non i.i.d. random sequences.</p><p xml:id="_FpNdNqJ">In Algorithm 1, the random variables in one iteration can be written as (x t , c t , d M t , q t ), where d M t ∈ {0, 1} represents whether the sample is near degeneracy or not, i.e. if ν S (h t−1 (x t )) &lt; b t−1 then</p><formula xml:id="formula_26" coords="19,72.00,79.19,124.66,12.77">d M t = 1, otherwise d M t = 0.</formula><p xml:id="_FGHvC4X">The random variable q t ∈ {0, 1} represents the outcome of the coin flip that determines if we acquire the label of this sample or not, in the case when d M t = 0. For simplicity, we use random variable z t ∈ Z := X × C × {0, 1} × {0, 1} to denote the tuple of random variables z t := (x t , c t , d M t , q t ). Thus, z t depends on z 1 , ..., z t−1 and the classical convergence results for i.i.d. samples do not apply in the margin-based algorithm. We define F t−1 as the σ-field of all random variables until the end of iteration t − 1 (i.e., {z 1 , ..., z t−1 }). In Algorithm 1, the re-weighted loss</p><formula xml:id="formula_27" coords="19,72.00,195.06,468.00,54.02">function at iteration t is rew (h; z t ) := d M t (h(x t ), c t ) + (1 − d M t )q t I{ p&gt;0} p (h(x t ), c t ). It is easy to see that rew (h; z) is upper bounded by ω ( Ĉ,C) pI{p&gt;0} &lt; ∞. Then, to analyze the convergence of 1 T T t=1 rew (h; z t ) to 1 T T t=1 E[ rew (h; z t )|F t−1 ],</formula><p xml:id="_hn8GKEq">we adopt the sequential covering number defined in <ref type="bibr" coords="19,255.83,255.98,104.00,9.61" target="#b36">Rakhlin et al. (2015b)</ref>. This notion generalizes the classical Rademacher complexity by defining a stochastic process on a binary tree. Let us briefly review the relevant results from <ref type="bibr" coords="19,176.68,295.00,106.39,9.61" target="#b36">Rakhlin et al. (2015b)</ref>. A Z-valued tree z of depth T is a rooted complete binary tree with nodes labeled by elements of Z. A path in the tree z is denoted by σ σ σ = (σ 1 , ..., σ T ), where σ t ∈ {±1}, for all t ∈ {1, . . . , T }, with σ t = −1 representing the left child node, and σ t = +1 representing the right child node. The tree z is identified with the sequence z = (z 1 , ..., z T ) of labeling functions z i : {±1} i−1 → Z which provide the labels for each node. Therefore, z 1 ∈ Z is the label for the root of the tree, while z i for i &gt; 1 is the label of the node obtained by following the path of length i − 1 from the root. In a slight abuse of notation, we use z i (σ σ σ) to refer to the label of the i th node along the path defined by σ σ σ. Similar to a Z-valued tree z, a real-valued tree v v v = (v v v 1 , ...,v v v T ) of depth T is a tree identified by the real-valued labeling functions v v v i : {±1} i−1 → R. Thus, given any loss function (h; •) : Z → R, the composition (h; •) • z is a real-valued tree given by the labeling functions ( (h;</p><formula xml:id="formula_28" coords="19,143.21,489.76,245.38,10.78">•) • z z z 1 , (h; •) • z z z 2 , ..., (h; •) • z z z T ) for any fixed h ∈ H.</formula><p xml:id="_5VYVXXf">Definition 3 (Sequential covering number). <ref type="bibr" coords="19,333.86,509.56,118.96,9.61" target="#b36">(Rakhlin et al. (2015b)</ref>, <ref type="bibr" coords="19,465.07,509.56,74.62,9.61;19,71.34,529.07,65.77,9.61" target="#b30">Kuznetsov and Mohri (2015)</ref>) Let (h; z) denote the loss of predictor h given the random variable z. Given a Z-valued tree z of depth T , a set V of real-valued trees of depth T is a sequential α-cover, with respect to the 1 norm, of a function class H with respect to the loss if for all h ∈ H and for all paths σ σ σ ∈ {±1} T , there exists a real-valued tree</p><formula xml:id="formula_29" coords="19,237.92,587.29,139.62,56.89">v v v ∈ V such that T t=1 |v v v t (σ σ σ) − (h;z z z t (σ σ σ))| ≤ T α.</formula><p xml:id="_tZ6JByW">The sequential covering number N 1 (α, • H,z z z) of a function class H with respect to the loss is defined to be the cardinality of the minimal sequential cover. The maximal covering number is then taken to be N 1 (α, • H, T ) := sup z z z {N 1 (α, • H,z z z)}, where the supremum is over all Z-valued trees z of depth T .</p><p xml:id="_wxVfp4T">In Definition 3, the loss function can be either the reweighted loss rew or the surrogate loss. In comparison, the standard covering number for i.i.d. observations N1 (α, • H) is defined as</p><formula xml:id="formula_30" coords="20,112.56,121.86,390.78,20.33">N1 (α, • H) := inf{|V | : V ⊆ R Z s.t. ∀h ∈ H, ∃v ∈ V with sup z∈Z {|v(z) − (h; z)|} ≤ α}.</formula><p xml:id="_ZTMrQcd">Utilizing the sequential covering number, <ref type="bibr" coords="20,272.39,150.52,137.75,9.61" target="#b30">Kuznetsov and Mohri (2015)</ref> provides a data-dependent generalization error bound for non i.i.d. sequences. As a relaxed version of their results, the dataindependent error bound is stated in Proposition 1. This proposition is from the last line of proof of Theorem 1 in <ref type="bibr" coords="20,152.70,205.46,136.25,9.61" target="#b30">Kuznetsov and Mohri (2015)</ref>, and we apply it to the reweighted loss.</p><p xml:id="_pSejUbM">Proposition 1 (Non i.i.d. generalization error bound). (Theorem 1 in <ref type="bibr" coords="20,483.06,228.48,56.81,9.64;20,71.03,246.80,96.75,9.64" target="#b30">Kuznetsov and Mohri (2015)</ref>) Let {z 1 , z 2 , ...z T } be a (non i.i.d.) sequence of random variables. Fix &gt; 2α &gt; 0.</p><p xml:id="_aSvdZyH">Then, the following holds:</p><formula xml:id="formula_31" coords="20,72.00,283.43,498.96,32.42">P sup h∈H 1 T T t=1 (E[ rew (h; z t )|F t−1 ] − rew (h; z t )) ≥ ≤ 2N 1 (α, rew • H, T ) exp − p2I{p&gt;0} T ( − 2α) 2 2ω ( Ĉ, C) 2 .</formula><p xml:id="_tbnKHC4">Proposition 2 further provides an upper bound for N 1 (α, rew • H, T ) when H is a smoothlyparameterized class.</p><p xml:id="_K5wA6Vw">Proposition 2 (Bound for the sequential covering number). Suppose H is a class of functions smoothly-parameterized by θ ∈ Θ ⊆ R d θ with respect to the ∞ norm, i.e., there exists</p><formula xml:id="formula_32" coords="20,71.16,401.90,468.84,28.79">L 1 &gt; 0 such that for any θ 1 , θ 2 ∈ Θ and any x ∈ X , h θ 1 (x) − h θ 2 (x) ∞ ≤ L 1 θ 1 − θ 2 ∞ . Let ρ(Θ) be the diameter of Θ in the ∞ norm. Suppose the surrogate loss function (•, c) is L 2 -Lipschitz with</formula><p xml:id="_m2ezNjH">respect to the ∞ norm for any fixed c. Then, given p, for any α &gt; 0, for any T ≥ 1, we have that</p><formula xml:id="formula_33" coords="20,139.60,457.04,310.40,24.48">ln(N 1 (α, rew • H, T )) ≤ d θ ln 1 + 2ρ(Θ)L 1 L 2 αp I{ p&gt;0} ≤ O ln 1 αp I{ p&gt;0}</formula><p xml:id="_TjRMz2Y">.</p><p xml:id="_rA92m8B">The smoothly-parameterized hypothesis class H is a common assumption when analyzing the covering number for parameterized class, e.g., see Assumption 3 in <ref type="bibr" coords="20,392.11,508.62,53.11,9.61" target="#b18">Gao (2022)</ref> and their examples.</p><p xml:id="_BuyuH2p">When hypothesis class H is smoothly parameterized, for example, a bounded class of linear functions, Proposition 2 implies that</p><formula xml:id="formula_34" coords="20,71.61,543.68,469.92,50.80">N 1 (α, rew • H, T ) is upper bounded by O ln 1 α pI{p&gt;0} , which is independent of T . Together with Proposition 1, it shows that 1 T T t=1 rew (h; z t ) converges to 1 T T t=1 E[ rew (h; z t )|F t−1 ] at rate Õ(1/ √ T ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2." xml:id="_sqvQC5X">MBAL-SPO with Hard Rejections</head><p xml:id="_TWqASrd">In this section, we develop excess risk bounds for the surrogate risk and the SPO risk, and present label complexity results, for MBAL-SPO with hard rejections. Our excess risk bounds for the surrogate risk hold for general feasible regions S. To develop risk bounds for the SPO risk, we consider two additional assumptions on S: (i) the case where S satisfies the strength property, and</p><p xml:id="_UGuesTY">(ii) the case where S is polyhedral. </p><formula xml:id="formula_35" coords="21,225.37,123.10,314.63,24.47">(w − w * (ĉ)) ≥ µ • ν S (ĉ) 2 w − w * (ĉ) 2 ,<label>(5)</label></formula><p xml:id="_rnNEz9x">where ν S is the distance to degeneracy function. We refer to µ as the strength parameter.</p><p xml:id="_ehWExk8">The strength property can be interpreted as a variant of strong convexity that bounds the distance to the optimal solution based on the parameter µ as well as the distance to degeneracy ν S (ĉ).</p><p xml:id="_Grcdjru">El Balghiti et al. ( <ref type="formula" coords="21,161.81,218.85,20.50,9.61">2022</ref>) demonstrate that the strength property holds when S is polyhedral or a strongly convex set. In addition, for some of the results herein, we make the following assumption concerning the surrogate loss function, which states the uniqueness of the surrogate risk minimizer and a relaxation of Hölder continuity.</p><p xml:id="_DpadQWX">Assumption 2 (Unique minimizer and Hölder-like property). There is a unique minimizer h * of the surrogate risk, i.e., the set H * is a singleton, and there exists a constant η &gt; 0 such that the surrogate loss function satisfies</p><formula xml:id="formula_36" coords="21,145.19,368.65,321.63,12.35">|E[ (ĉ, c) − (h * (x), c)|x]| ≤ η ĉ − h * (x) 2 for all x ∈ X , and ĉ ∈ Ĉ.</formula><p xml:id="_TxbPCRp">It is easy to verify that the common squared loss satisfies Assumption 2 with η = 1 when the hypothesis class is well-specified. In Lemma 7 in Appendix A, we further show that the SPO+ loss satisfies Assumption 2 under some noise conditions.</p><p xml:id="_auAzarD">Theorem 1 is our main theorem concerning MBAL-SPO with hard rejections and with general surrogate losses satisfying Assumption 2. Theorem 1 presents bounds on the excess surrogate and SPO risks as well as the expected label complexity after T iterations.</p><p xml:id="_uSfxB44">Theorem 1 (General surrogate loss, hard rejection). Suppose that Assumptions 1 and 2 hold, and that Algorithm 1 sets p ← 0 and updates the set of predictors according to the optional update rule in Line 20 with ϑ ← η. Furthermore in Algorithm 1, for a given δ</p><formula xml:id="formula_37" coords="21,72.00,562.24,468.00,31.76">∈ (0, 1], let r 0 ≥ ω ( Ĉ, C), r t ← 2ω ( Ĉ, C) 4 ln(2tN 1 (ω ( Ĉ,C)/t, rew •H,t)/δ) t + 2 t for t ≥ 1, b 0 ← max{φ(r 0 ), r 0 /η},<label>and</label></formula><formula xml:id="formula_38" coords="21,72.00,599.05,155.95,15.62">b t ← φ(2r t + 2η t t−1 i=0 b 2 i ) for t ≥ 1.</formula><p xml:id="_u7CU8qQ">Then, the following guarantees hold simultaneously with probability at least 1 − δ for all T ≥ 1:</p><formula xml:id="formula_39" coords="21,83.97,637.37,340.77,32.40">• (a) The excess surrogate risk satisfies R (h T ) − R * ≤ r T + η T T −1 t=0 b 2 t , •<label>(</label></formula><p xml:id="_6RnmuyQ">b) If the feasible region S satisfies the strength property with parameter µ &gt; 0, then the excess SPO risk satisfies</p><formula xml:id="formula_40" coords="21,189.82,702.22,255.49,25.34">R SPO (h T ) − R * SPO ≤ inf γ T ≥2b T 2ρ(C)b T µγ T + Ψ(γ T )ω S (C) , • (c) If the feasible region S is polyhedral, then the excess SPO risk satisfies R SPO (h T ) − R * SPO ≤ Ψ(2b T )ω S (C), • (d) The expectation of the number of labels acquired, E[n T ], deterministically satisfies E[n T ] ≤ T t=1 Ψ(2b t−1 ) + δT .</formula><p xml:id="_zD63A5x">In the polyhedral case, Theorem 1 indicates that the excess SPO risk of Algorithm 1 converges to zero at rate O(Ψ(2b T )), and the expectation of the number of acquired labels grows at rate O T t=1 Ψ(2b t ) for small δ. (Usually, δ O(1/T ).) Note that Theorem 1 is generic in that the excess risk and label complexity bounds depend on the functions φ and Ψ. In Appendix A, we give the explicit forms of these functions in some special cases of interest.</p><p xml:id="_WCuuHKh">Remark 1 (Updates of H t ). In Theorem 1, the set of predictors is updated according to Line 20 in Algorithm 1. This is a technical requirement for the convergence when setting p = 0. This update process means that h t ∈ H t−1 ⊆ H t−2 ... ⊆ H 0 = H. By constructing these shrinking sets H t of predictors, we are able to utilize the information from previous iterations. Particularly, Lemma 4 below shows that these shrinking sets H t−1 always contain the true optimal predictor h * under certain conditions.</p><p xml:id="_FxW5pdh">Remark 2 (Value of γ T ). In Theorem 1, to find the best value of the parameter γ T in part (b) that minimizes the excess SPO risk for sets satisfying the strength property, we observe that the choice of γ T depends on Ψ, φ and T . If γ T satisfies that (1) b T γ T → 0, when r T → 0, and (2) γ T → 0, when T → ∞, then the excess SPO risk will converge to zero. For example, we can set γ T = (b T )</p><p xml:id="_6kXPk84">κ , where κ ∈ (0, 1).</p><p xml:id="_S4MsFZT">Auxiliary Results for the Proof of Theorem 1. To achieve the risk bound in part (a) of Theorem 1, we decompose the excess surrogate risk into three parts. First, we denote the re-weighted surrogate risk for the features that are far away from degeneracy by f t (h), defined by:</p><formula xml:id="formula_41" coords="22,140.25,546.09,336.06,13.32">f t (h) := E[ (h; z t )I{ν S (h t−1 (x t )) ≥ b t−1 }|F t−1 ] = E[ (h; z t )(1 − d M t )|F t−1 ],</formula><p xml:id="_kXwRZqb">where we use (h; z t ) to denote (h(x t ), c t ) and the expectation above is with respect to z t . Since</p><p xml:id="_5X5GV4Y">x t and c t are i.i.d. random variables, and only d M t depends on F t−1 , f t (h) can further be written</p><formula xml:id="formula_42" coords="22,72.00,617.60,220.82,12.77">as f t (h) = E[ (h(x t ), c t )|d M t = 0]P(d M t = 0|F t−1</formula><p xml:id="_K3KK3xp">). Note also that, since p = 0, the re-weighted loss function can be written as rew (h;</p><formula xml:id="formula_43" coords="22,72.00,637.04,468.00,30.29">z t ) = (h(x t ), c t )d M t = (h(x t ), c t )I{ν S (h t−1 (x t )) &lt; b t−1 }, for a given h ∈ H.</formula><p xml:id="_y4nbPAH">Next, for given h ∈ H and h * ∈ H * , we denote the discrepancy between the conditional expectation and the realized excess re-weighted loss of predictor h at time t by Z t h , i.e., Z t</p><formula xml:id="formula_44" coords="22,72.00,677.16,468.00,29.91">h := E[ rew (h; z t ) − rew (h * ; z t )|F t−1 ] − ( rew (h; z t ) − rew (h * ; z t ))</formula><p xml:id="_JGYNS7D">. Lemma 3 shows that the excess surrogate risk can be decomposed into three parts.</p><p xml:id="_ypSCHUc">Lemma 3 (Decomposition of the excess surrogate risk). In the case of hard rejections, i.e., p ← 0 in Algorithm 1, for any given h * ∈ H * and T ≥ 1, the excess surrogate risk of any predictor h ∈ H can be decomposed as follows:</p><formula xml:id="formula_45" coords="23,102.73,138.62,406.54,32.42">R (h) − R (h * ) = 1 T T t=1 f t (h) − f t (h * ) + 1 T T t=1 Z t h + 1 T T t=1 ( rew (h; z t ) − rew (h * ; z t )) .</formula><p xml:id="_wqH5aM9">The first part in Lemma 3 is the averaged excess surrogate risk for the hard rejected features at each iteration. Lemma 4 below further shows that f t (h)</p><formula xml:id="formula_46" coords="23,343.98,198.40,193.99,12.77">− f t (h * ) is close to zero when h ∈ H T −1 .</formula><p xml:id="_Ye7kyTh">Lemma 4. Suppose that Assumptions 1 and 2 hold where h * denotes the unique minimizer of the surrogate risk, and that Algorithm 1 sets p ← 0 and updates the set of predictors according to the optional update rule in Line 20 with ϑ ← η. Furthermore, suppose that that r 0 ≥ ω ( Ĉ, C),</p><formula xml:id="formula_47" coords="23,72.00,277.23,467.82,34.50">r t ≥ sup h∈H 1 t t i=1 Z i h for t ≥ 1, b 0 ← max{φ(r 0 ), r 0 /η}, and b t ← φ(2r t + 2η t t−1 i=0 b 2 i ) for t ≥ 1. Then, for all t ≥ 1, it holds that (a) h * ∈ H t−1 , and (b) sup h∈H t−1 f t (h) − f t (h * ) ≤ ηb 2 t−1 .</formula><p xml:id="_QfRZqvs">With Lemma 4, we can appropriately bound the first average of terms in Lemma 3, involving the expected surrogate risk when far from degeneracy. Thus, Lemmas 3 and 4 enable us to prove the excess surrogate risk bound in part (a). The proofs of the remaining parts follow by translating the excess surrogate risk bound to guarantees on the excess SPO risk and the label complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3." xml:id="_7aGB4xY">Refined Bounds for SPO+ Under Separability</head><p xml:id="_mJ56KKa">Next, we provide a smaller excess risk bound when using SPO+ as the surrogate loss, again in the case of hard rejections. The SPO+ loss function incorporates the structure of the downstream optimization problem and, intuitively, the excess SPO+ risk when far away from degeneracy will be close to zero when the distance Dist H * (h) is small and the distribution satisfies a separability condition, which we define below.</p><p xml:id="_KMvWCd3">Assumption 3 (Strong separability condition). There exist constants ∈ [0, 1) and τ ∈ (0, 1] such that, for all h * ∈ H * SPO+ , with probability one over (x, c) ∼ D, it holds that:</p><formula xml:id="formula_48" coords="23,83.97,559.18,171.54,11.71">• (1) h * (x) − c ≤ ν S (h * (x)),<label>and</label></formula><formula xml:id="formula_49" coords="23,83.97,577.93,223.43,15.55">• (2) ν S (h * (x)) ≥ τ sup h ∈H * SPO+ {ν S (h (x))} .</formula><p xml:id="_hgvCpQd">The following proposition shows that the separability condition leads to zero SPO+ and SPO risk in the polyhedral case. Indeed, the SPO+ loss is a generalization of the hinge loss and the structured hinge loss in binary and multi-class classification problems and is expected to achieve zero loss when there is a predictor function h that strictly separates the cost vectors into different classes corresponding to the extreme points of S <ref type="bibr" coords="23,303.09,678.54,142.86,9.61" target="#b15">(Elmachtoub and Grigas 2022)</ref>. Assumption 3 and Proposition 3 formally define the notion of separability, wherein the distance between the prediction h(x) and the realized cost vector c, relative to the distance to degeneracy of h(x), is controlled.</p><p xml:id="_T96NPJC">Proposition 3 (Zero SPO+ risk in the polyhedral and separable case). Assume that there exists h ∈ H and a constant ∈ [0, 1) such that h(x) − c ≤ ν S ( h(x)) with probability one over (x, c) ∼ D. When the feasible region S is polyhedral, it holds that R * SPO+ = R * SPO = 0 and h is a minimizer for both R SPO+ and R SPO .</p><p xml:id="_DVPhvyd">As compared to Theorem 1, Theorem 2 below presents improved surrogate risk convergence guarantees for SPO+ under separability in the polyhedral case.</p><p xml:id="_YqVp4SQ">Theorem 2 (SPO+ surrogate loss, hard rejection, polyhedral and separable case).</p><p xml:id="_U66KDjY">Suppose that the feasible region S is polyhedral, Assumptions 1 and 3 hold, and the surrogate loss function is SPO+. Suppose that Algorithm 1 sets p ← 0 and H t ← H for all t. Furthermore in Algorithm 1, for a given δ</p><formula xml:id="formula_50" coords="24,72.00,254.79,461.02,38.31">∈ (0, 1], let r 0 ≥ ω ( Ĉ, C), r t ← ω ( Ĉ, C) 4 ln(2t N1 (ω ( Ĉ,C)/t, SPO+ •H)/δ) t + 2 t for t ≥ 1, b 0 ← max{φ(r 0 ), ρ( Ĉ)}, and b t ← (1 + 2 τ (1−ρ) )φ(r t ) for t ≥ 1.</formula><p xml:id="_hzP3xRV">Then, the following guarantees hold simultaneously with probability at least 1 − δ for all T ≥ 1:</p><formula xml:id="formula_51" coords="24,83.97,316.65,456.03,69.42">• (a) The excess SPO+ risk satisfies R SPO+ (h T ) − R * SPO+ = R SPO+ (h T ) ≤ r T , • (b) The excess SPO risk satisfies R SPO (h T ) − R * SPO = R SPO (h T ) ≤ Ψ(2b T )ω S (C), • (c) The expectation of the number of labels acquired, E[n T ], deterministically satisfies E[n T ] ≤ T t=1 Ψ(2b t−1 ) + δT .</formula><p xml:id="_ZFmAeab">Remark 3 (Benefits of SPO+ Under Separability). When using SPO+ in the separable case, the bound in part (a) of Theorem 2 is substantially improved as compared to Theorem 1.</p><p xml:id="_35kVcVt">Intuitively, when an optimal predictor h * (x) is far away from degeneracy and h t (x) and h * (x) are close, then the excess SPO+ risk of h t (x) can be shown to be zero. As a result, the rejection criterion -which compares ν S (h t (x)) to a quantity b t that is related to the distance between h t and h * -is "safe" in the sense that whenever h t (x) ≥ b t we can demonstrate that h t (x) leads to a correct optimal decision with high probability. Thus, when using the SPO+ loss function, we can obtain a smaller excess SPO+ risk bound. Indeed, in Theorem 2, the value of r t is determined by the i.i.d. covering number, which implies that this risk bound is the same as the risk bound of supervised learning that labels all the samples. Furthermore, another benefit of SPO+ under the separability assumption is that we do not need to update H t at each iteration, which simplifies the computation substantially. Finally, Assumption 2 which assumes the minimizer h * is unique is not needed.</p><p xml:id="_fhNYnxj">Theorem 2 shows that the excess SPO+ risk converges to zero at rate Õ(1/ √ T ), which equals the typical learning rate for the excess SPO+ risk in supervised learning. As Algorithm 1 requires much fewer labels, this demonstrates the advantage of active learning. In fact, the main idea of the proof of Theorem 2 is to show that h T actually, with high probability, achieves zero empirical SPO+ risk over all samples (x 1 , c 1 ), . . . , (x T , c T ) -including the cases where the label is not acquired.</p><p xml:id="_9WTRPek">Indeed, in the separable case, the rejection criterion is "safe" and we are able to demonstrate that</p><formula xml:id="formula_52" coords="25,76.57,80.43,216.35,10.47">SPO+ (h t (x t ), c t ) = 0 when ν S (h t−1 (x t )) ≥ b t−1 .</formula><p xml:id="_uH5qTKv">This of course implies that h T is an empirical risk minimizer for SPO+ across T i.i.d. samples (x 1 , c 1 ), . . . , (x T , c T ) and we are able to conclude part (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4." xml:id="_VHcWmzp">MBAL-SPO with Soft Rejections</head><p xml:id="_XNzXuuP">In this section, we analyze the convergence and label complexity of MBAL-SPO with soft rejection.</p><p xml:id="_KB82j9t">We return to the setting of a generic surrogate loss function . Compared to the hard rejection case in Theorem 1, this positive p will lead to a larger label complexity than Theorem 1. On the other hand, when p is positive, we do not have to construct the confidence set H t of the predictors at each iteration. In other words, H t can be set as H, for all t as in Theorem 2. Thus, we do not have to consider t additional constraints when minimizing the empirical re-weighted risk, which will reduce the computational complexity significantly. Theorem 3 is our main theorem for the MBAL-SPO under a general surrogate loss, which again provides upper bounds for the excess surrogate and SPO risk and label complexity of the algorithm.</p><p xml:id="_7ZsFdvC">Theorem 3 (General surrogate loss, soft rejection). Suppose that Assumption 1 holds, and let δ ∈ (0, 1] and p ∈ (0, 1] be given. In Algorithm 1, set H t ← H for all t ≥ 0, r 0 ≥ ω ( Ĉ, C),</p><formula xml:id="formula_53" coords="25,72.00,373.49,372.95,20.88">r t ← 2ω ( Ĉ, C) 1 p 2 ln(2N 1 ( ω ( Ĉ,C) t , rew •H,t)/δ) t + 2 t for t ≥ 1, b t ← 2φ(r t ) for t ≥ 0.</formula><p xml:id="_thajDx9">Then, the following guarantees hold simultaneously with probability at least 1 − δ for all T ≥ 1:</p><formula xml:id="formula_54" coords="25,83.97,420.82,280.19,11.71">• (a) The excess surrogate risk satisfies R (h T ) − R * ≤ r T ,</formula><p xml:id="_mGmZBcp">• (b) If the feasible region S satisfies the strength property with parameter µ &gt; 0, then the excess SPO risk satisfies</p><formula xml:id="formula_55" coords="25,189.82,482.72,255.49,25.34">R SPO (h T ) − R * SPO ≤ inf γ T ≥2b T 2ρ(C)b T µγ T + Ψ(γ T )ω S (C) ,</formula><p xml:id="_qM2UfnA">• (c) If the feasible region S is polyhedral, then the excess SPO risk satisfies</p><formula xml:id="formula_56" coords="25,83.97,518.94,456.03,70.25">R SPO (h T ) − R * SPO ≤ Ψ(2b T )ω S (C), • (d) The expectation of the number of labels acquired, E[n T ], deterministically satisfies E[n T ] ≤ pT + T t=1 Ψ(2b t−1 ) + δT .</formula><p xml:id="_bDt4gVJ">Remark 4 (Value of b t and p). In part (d) of Theorem 3, E[n T ] depends on both pT and T t=1 Ψ(2b t ). When the exploration probability p is large, pT in part (d) of Theorem 3 is large. On the other hand, in Theorem 3, the value of b t depends on r t , and r t furthermore is in the order of O(1/p). It implies that when the exploration probability p is small, b t is large, and T t=1 Ψ(2b t ) in part (d) of Theorem 3 is large. Hence, to minimize the label complexity, there is a trade-off when choosing the value of p. In Proposition 5, we will specify the value of p and provide an upper bound for E[n T ] which is sublinear in T .</p><p xml:id="_9yaRJtV">Although Theorem 3 does not require Assumptions 2 or 3, as will be shown later, to demonstrate the advantage of the supervised learning algorithm, another assumption, (Assumption 5), on the noise distribution is needed. We elaborate this in Section 5.2.</p><p xml:id="_tWrZkMC">Setting Parameters in MBAL-SPO. To conclude this section, we discuss the issue of setting the parameters for MBAL-SPO in practice. Although Theorems 1 and 3 provide the theoretical settings for the parameters r t and b t in the MBAL-SPO algorithm, how to set the scale of these parameters is an important question in practice. The complexity of the hypothesis class, the noise level and the distribution of features all impact the settings of these parameters. When the noise level is larger, or the cost vector c is further away from the degeneracy, the scale of b t for the algorithms should be larger. In addition, to set a proper scale of the parameters in practice, we need to consider the tradeoff between the budget of the labels (or the cost to acquire each label) and the efficiency of the learning process. A reasonable practical approach is to set a "burn in" period of T iterations where MBAL-SPO acquires all labels during the first T iterations. One can then use the distribution of values ν S (h T (x t )) for all previous features x t to inform the value of b T . For example, we can set the scale of b T as some order statistics of the past values ν S (h T (x t )) for t ∈ {1, . . . , T }, e.g., the mean or other quantile depending on the practical cost of acquiring labels versus the rate at which feature vectors are collected. Then, the value of b t for t ≥ T can be updated according to the value of b T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5." xml:id="_cBDqxuC">Risk Guarantees and Small Label Complexity Under Low Noise Conditions</head><p xml:id="_uEAERAg">To demonstrate the advantage of MBAL-SPO over supervised learning in Theorems 1 and 3, we need to analyze the functions φ and Ψ. In Appendix A, we present some natural low-noise conditions such that we can provide concrete examples of φ under the SPO+ loss. In these examples, φ satisfies that φ( ) ∼ √ . In Appendix A, we further show that Assumption 2 holds for SPO+ and derive the upper bound for η under some noise conditions. Given these results, in this section, we analyze the exact order of the label complexity and the risk bounds. These results demonstrate the advantages of MBAL-SPO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1." xml:id="_ZT579WP">Small Label Complexities</head><p xml:id="_ubKVB2P">In this section, we analyze the order of the label complexity for both hard rejection and soft rejection.</p><p xml:id="_byA9sqq">First, we characterize the noise as the level of near degeneracy in Assumption 4, which is similar in spirit to the low noise condition assumption in <ref type="bibr" coords="26,299.07,656.41,74.67,9.61" target="#b25">Hu et al. (2022)</ref>.</p><p xml:id="_NK3HrT9">Assumption 4 (Near-degeneracy condition). There exist constants b 0 , κ &gt; 0 such that</p><formula xml:id="formula_57" coords="26,202.55,707.82,206.90,17.29">Ψ(b) = P inf h * ∈H * {ν S (h * (x))} ≤ b ≤ (b/b 0 ) κ .</formula><p xml:id="_9XYTdGf">Assumption 4 controls the rate at which Ψ(b) -which measures the probability mass of features with small distance to degeneracy -approaches 0 as b approaches 0. In other words, for small enough b so that b b 0 &lt; 1, when the parameter κ is larger the probability near the degeneracy is smaller at a faster rate. When the above near-degeneracy condition in Assumption 4 holds and φ( ) satisfies that φ( ) ∼ O( √ ), we have the sublinear label complexity for the hard rejection in the polyhedral cases in Proposition 4.</p><p xml:id="_BBgUGFf">Proposition 4 (Small label complexity for hard rejections). Suppose that Assumptions 1, 2, 4 and the conditions in Proposition 2 hold. Suppose there exists a constant C φ ∈ (0, 1 36η 2 ) such that Assumption 1.(2) holds with φ( ) = C φ • √ . Under the same setting of Algorithm 1 in Theorem 1, for a fixed δ ∈ (0, 1], the following guarantees hold simultaneously with probability at least 1 − δ for all T ≥ 1:</p><p xml:id="_dYQWfqt">• The excess surrogate risk satisfies R (h T ) − R * ≤ Õ(T −1/4 ).</p><p xml:id="_BuEx3gY">• The excess SPO risk satisfies R SPO (h T ) − R * SPO ≤ Õ(T −κ/4 ). • The expectation of the number of labels acquired, conditional on the above guarantee on the excess surrogate risk, is at most Õ T 1−κ/4 for κ ∈ (0, 4), and Õ(1) for κ ∈ [4, ∞).</p><p xml:id="_y6kW8cn">The last claim in Propositon 4 indicates that the label complexity is sublinear. Notice that, as compared to Theorem 1, for simplicity, we state the bound on the label complexity conditional on the excess SPO+ risk guarantee that holds with probability at least 1 − δ. When κ &gt; 4, the label complexity is even finite. To compare this label complexity with supervised learning, we consider the excess SPO risk with respect to the number of labels n. Let n ← E[n T ] be a fixed value. Under the same assumptions and similar proof procedures, we can show that the excess SPO risk of the supervised learning is at most Õ(n −κ/2 ). In comparison, Proposition 4 indicates that the expected excess SPO risk of MBAL-SPO is at most Õ(n − κ 4−κ ). Thus, when κ &gt; 2, MBAL-SPO acquires much fewer labels than the supervised learning to achieve the same level of SPO risk. This demonstrates the advantage of MBAL-SPO over supervised learning.</p><p xml:id="_NN68YpQ">Remark 5 (Small label complexity under separability condition with SPO+ loss).</p><p xml:id="_DAmcxuy">Under the same setting as Theorem 2, when Proposition 2 holds, obviously, we have that</p><formula xml:id="formula_58" coords="27,72.00,616.30,114.04,13.21">R (h T ) − R * ≤ Õ(T −1/2</formula><p xml:id="_fcwcGcb">). If we further assume φ( ) ∼ √ , then following the same analysis in Proposition 4, we can obtain that R SPO+ (h T ) − R * SPO+ ≤ Õ(T −κ/2 ) and the expected number of labels is at most Õ(T 1−κ/2 ) for κ ∈ (0, 2) and Õ(1) for κ ∈ [2, ∞).</p><p xml:id="_YpeHFkp">Similar to the case of MBAL-SPO with hard rejections, when Assumption 4 and the condition that φ( ) ∼ O( √ ) hold, we obtain sublinear label complexity of Algorithm 1 with soft rejections in Proposition 5.</p><p xml:id="_REyWVhF">Proposition 5 (Small label complexity for soft rejections). Suppose Assumptions 1, 4 and the conditions in Proposition 2 hold. Suppose there exists a constant C φ &gt; 0 such that Assumption 1.</p><p xml:id="_uWM5uHQ">(2) holds with φ( κ+2) and H t ← H for all t, and b t , r t the same values as Theorem 3. For a fixed δ ∈ (0, 1], the following guarantees hold simultaneously with probability at least 1 − δ for all T ≥ 1: κ+2) .</p><formula xml:id="formula_59" coords="28,166.78,110.66,126.98,18.31">) = C φ • √ . Set p ← T − κ 2(</formula><formula xml:id="formula_60" coords="28,83.97,170.41,314.11,36.45">• The excess surrogate risk satisfies R (h T ) − R * ≤ Õ T − 1 2(κ+2) . • The excess SPO risk satisfies R SPO (h T ) − R * SPO ≤ Õ T − κ 2(</formula><p xml:id="_hUZmw69">• The expectation of the number of labels acquired, conditional on the above guarantee on the excess surrogate risk, is at most Õ T</p><formula xml:id="formula_61" coords="28,275.38,228.21,34.38,11.17">1− κ 2(κ+2)</formula><p xml:id="_uY9yYv6">for κ &gt; 0.</p><p xml:id="_S49WyzX">In Proposition 5, the larger the parameter in near-degeneracy condition κ is, the smaller the label complexity will be. We observe that in Proposition 5, when p = T − κ 2(κ+2) , the excess surrogate risk converges to zero at rate Õ(T − 1 κ+2 ), which is slower than the typical learning rate of supervised learning, which is O(T −1/2 ). In the next section, we demonstrate that the excess surrogate risk can be reduced to Õ(T −1/2 ) under some further conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2." xml:id="_XpA8ZtT">Small Label Complexity with Soft Rejections.</head><p xml:id="_bfQxCaS">In this section, we show that under certain conditions, the convergence rate of excess surrogate risk under soft rejection is Õ(T −1/2 ), which is the same as standard supervised learning (except for logarithmic factors). To achieve this rate, we assume the near-degeneracy function satisfies Assumption 5.</p><p xml:id="_fRRxgtX">Assumption 5 (Lower bound for Ψ). There exists K 1 &gt; 0, such that the near-degeneracy function satisfies Ψ(b) ≥ K 1 b, for all b ≥ 0.</p><p xml:id="_9tUry8N">Remark 6. Assumption 5 says that the near-degeneracy function Ψ(b) is at least in the order of Ω(b). Here is the intuitive illustration of why we need a lower bound for the near-degeneracy function Ψ. If there is no lower bound for Ψ, then the features could be distributed far away from the degeneracy and Ψ(b t ) could converge to zero at a very fast rate. In that case, the probability to have a near-degeneracy feature could be very small, and thus the probability to acquire the labels would be very small as well. That means the algorithm would have a slower speed to collect labels, and thus its learning rate would be slow. Therefore, a lower bound for Ψ in Assumption 5 is in need.</p><p xml:id="_mzaxTRj">This lower bound also implies that κ can not be great than 1, which is consistent with the common Tsybakov's noise condition in the active learning literature, where the parameter of the Tsybakov's noise condition (inverse of κ), should be larger than 1. See Hanneke (2011) as an example.</p><p xml:id="_Dc49h8s">Proposition 6 shows that under Assumption 5 and other assumptions, the excess surrogate risk of active learning, R (h t ) − R (h * ) converges to zero at rate Õ(T −1/2 ), when p &gt; 0. to zero at rate Õ( 1/T ), which is the same as the typical generalization error bound in the supervised learning except for the small logarithmic term. Thus, Proposition 6 implies that for the excess risk of the surrogate function, our active learning algorithm achieves the same order as the supervised learning. However, compared to supervised learning, active learning algorithms acquire much fewer labels. In illustration, when the near-degeneracy condition holds with κ = 1, the label complexity of MBAL-SPO is Õ(T 5/6 ) in Proposition 5. Therefore, the MBAL-SPO can achieve the same order of surrogate risk with a smaller number of acquired labels.</p><p xml:id="_KZ5WUUS">Compared to Proposition 4 where κ &gt; 0, Proposition 6 holds only for κ = 1. This means that when h * (x) is distributed far away from the degeneracy, such that the near-degeneracy condition holds with a large κ &gt; 1, using hard rejection can achieve a smaller label complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6." xml:id="_Zpt2nUg">Numerical Experiments</head><p xml:id="_nGhMwSh">In this section, we present the results of numerical experiments in which we empirically examine the performance of our proposed margin-based algorithm (Algorithm 1) under the SPO+ surrogate loss. We use the shortest path problem and personalized pricing problem as our exemplary problem classes. For both problems, we use (sub)gradient descent to minimize the SPO+ loss function in the MBAL-SPO algorithm. We set p ← 10 −5 and set H t ← H according to Theorem 3. The norm</p><p xml:id="_G9PcntA">• is set as the 2 norm. In both problems, to calculate the distance to the degeneracy, we use the result of Theorem 8 in El Balghiti et al. ( <ref type="formula" coords="29,290.61,556.36,19.10,9.61">2022</ref>), which was stated in Equation ( <ref type="formula" coords="29,472.43,556.36,4.26,9.61" target="#formula_14">4</ref>). We set the function φ as the square root function, which is used directly in the setting of the sequence {b t }.</p><p xml:id="_e2rpbnb">The numerical experiments were conducted on a Windows 10 Pro for Workstations system, with an Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz 20 cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1." xml:id="_M3x6ZRj">Shortest Path Problem</head><p xml:id="_c6kpkXy">We first present the numerical results for the shortest path problem. We consider a 3 × 3 (later also a 5 × 5) grid network, where the goal is to go from the southwest corner to the northeast corner, and the edges only go north or east. In this case, the feasible region S is composed of network flow constraints, and the cost vector c encodes the cost of each edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_7G429Tm">Data generation process.</head><p xml:id="_2GfKrXV">Let us now describe the process used to generate the synthetic experimental data. The dimension of the cost vector d is 12, corresponding to the number of edges in the 3 × 3 grid network. The number of features p is set to 5. The number of distinct paths is 6. Given a coefficient matrix B ∈ R d×p , the training data set {(x i , c i )} n i=1 and the test data set {(x i , ci )} n test i=1 are generated according to the following model.</p><p xml:id="_kj2WEaD">1. First, we identify six vectors µ j ∈ R p , j = 1, ..., 6, such that the corresponding cost vector Bµ j is far from degeneracy, that is, the distance to the closest degenerate cost vector ν S (Bµ j ) is greater than some threshold, and the optimal path under the cost vector Bµ j is the path j.</p><p xml:id="_XPNCQxX">2. Each feature vector x i ∈ R p is generated from a mixed distribution of six multivariate Gaussian distributions with equal weights. Each multivariate Gaussian distribution follows N (µ j , σ 2 m I p ), where the variance σ 2 m is set as 1/9. 3. Then, the cost vector c j is generated according to <ref type="figure" coords="30,532.83,292.30,4.35,9.61">d</ref>, where b j is the j th row of the matrix B. The degree parameter deg is set as 1 in our setting and j is a multiplicative noise term, which is generated independently from a uniform distribution [1 − ¯ , 1 + ¯ ]. Here, ¯ is called the noise level of the labels.</p><formula xml:id="formula_62" coords="30,331.00,285.53,200.32,18.30">c j = 1 + (1 + b T j x i / √ p) deg j , for j = 1, ...,</formula><p xml:id="_2sGCHzW">To determine the coefficient matrix B, we generate a random candidate matrix B multiple times, whose entries follow the Bernoulli distribution (0.5), and pick the first B such that µ j exists in</p><p xml:id="_v9z4uj7">Step 1 for each j = 1, ..., 6. The size of the test data set is 1000 sample points. In the context of our margin-based algorithm, we set r t = [d × ln(t) + ln(1/δ)]/t, where t is the iteration counter, d = 12 is the dimension of the cost vector, and δ is set as 10 −7 . According to Proposition 5, we set b t = 0.5 √ r t . The running time for one single run on a 3 × 3 grid to acquire 25 labels is about 10 minutes for the margin-based algorithm. The margin-based algorithm has good scalability as long as the calculation of the distance to the degeneracy ν S is fast, for example, in the case of relatively simple polyhedral sets. To further examine the performance of the margin-based algorithm on a larger-scale problem, we conduct a numerical experiment in a 5 × 5 grid network in the right plot of Figure <ref type="figure" coords="30,415.96,696.78,4.20,9.61" target="#fig_6">3</ref>, again shown with an 85 % confidence interval. We see that although both algorithms converge to the same optimal SPO This verifies the advantages of our algorithms under various conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2." xml:id="_6ANp45c">Personalized Pricing Problem</head><p xml:id="_TZwz4Kn">In this section, we present numerical results for the personalized pricing problem. Suppose that we have three types of items, indexed by j = 1, 2, 3. We have three candidate prices for these three items, which are $60, $80, and $90. Therefore, in total, we have 3 3 = 27 possible combinations of prices. Suppose that the dimension of the features of the customers is p = 6. When a customer is selected to survey, their answers will reveal the purchase probability for all three items at all possible prices. These purchase probabilities are generated on the basis of an exponential function of the form O(e −p ). We add additional price constraints between products, such that the first item has the highest price, and the third item has the lowest price. Please see the details in Appendix C.3.</p><p xml:id="_cbyCAYY">Because there are three items and three candidate prices, the dimension of the cost vector d j (p i ) is 9. Therefore, our predictor h(x) is a mapping from the feature space X ⊆ R 6 to the label space C ⊆ [0, 1] 9 . We assume that the predictor is a linear function, so the coefficient of h(x) is a (6 + 1) × 9 matrix, including the intercept. Unlike the shortest path problem which can be solved efficiently, the personalized pricing problem is NP-hard in general due to the binary constraints. In our case, since the dimensions of products and prices are only three, we enumerate all the possible solutions to determine the prices with the highest revenue.</p><p xml:id="_qEKHYNE">The test set performance is calculated on 1000 samples. In MBAL-SPO, we set r t = 250 [d × ln(t) + ln(1/δ)]/t, where t is the iteration counter, d is the dimension of the cost vector, which is 9, and δ is set as 10 −7 . According to Proposition 5, we set b t = 0.5 × √ r t . The scales of  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7." xml:id="_erWmeQj">Conclusions and Future Directions</head><p xml:id="_h9rk7at">Our work develops the first active learning algorithms in the predict-then-optimize framework. We consider the SPO loss function and its tractable surrogate loss functions and provide a practical margin-based active learning algorithm (MBAL-SPO). We provide two versions of MBAL-SPO and develop excess risk guarantees for both versions. Furthermore, we provide upper bounds on the label complexity of both versions and show that the label complexity can be better than the supervised learning approach under some natural low-noise conditions. Our numerical experiments also demonstrate the practical value of our proposed algorithm. There are several intriguing future directions. Since directly minimizing the SPO loss function is challenging, one valuable direction is applying the results in Lemma 7 of <ref type="bibr" coords="37,228.36,115.86,95.02,8.74" target="#b31">Liu and Grigas (2021)</ref>, for any κ ∈ (0, κ], it holds that</p><formula xml:id="formula_63" coords="37,164.10,133.52,283.80,31.19">E (c + 2∆) T (w * (c) − w * (c + 2∆))|x ≥ ακκe − 3κ 2 +3 ξ2 + c 2 2 2σ 2 4 √ 2πσ 2 • Ξ S d S .</formula><p xml:id="_DKHVENn">Let κ = min{κ, σ}, it holds that</p><formula xml:id="formula_64" coords="37,157.44,191.58,297.12,27.14">E[ SPO+ (h(x), c) − SPO+ (h * (x), c)|x] ≥ αΞ S 4 √ 2πe 3(1+β 2 ) 2 • min κ 2 M , κ .</formula><p xml:id="_RtvheY3">Define the function ι(κ</p><formula xml:id="formula_65" coords="37,72.00,226.48,468.25,31.79">) := κ 2 M for κ ∈ [0, M 2 ] and ι(κ) := κ − M 4 for κ ∈ [ M 2 , ∞), we have ι(κ) is the convex biconjugate of min κ 2 M , κ</formula><p xml:id="_Wk2ZbEU">. By taking the expectation on x, it holds that</p><formula xml:id="formula_66" coords="37,81.96,268.69,384.23,81.56">E[ SPO+ (h(x), c) − SPO+ (h * (x), c)] ≥ αΞ S 4 √ 2πe 3(1+β 2 ) 2 • E x [ι( h(x) − h * (x) )] Since M ≥ max{σ, 1}, taking M = 2ρ( Ĉ), we obtain that R SPO+ (h) − R SPO+ (h * ) ≥ αΞ S 8 √ 2πρ( Ĉ)e 3(1+β 2 ) 2 • E x [ h(x) − h * (x) 2 ]</formula><p xml:id="_J3XhEFv">Then, combining the result with Assumption 6, we obtain Lemma 5.</p><p xml:id="_DPwp4K6">Strongly-convex feasible region. Next, we consider the case where the feasible region S is a level-set of a strongly-convex and smooth function. In the spirit of Definition 4.1 in <ref type="bibr" coords="37,391.33,394.34,92.56,8.74" target="#b31">Liu and Grigas (2021)</ref>, we consider two related classes of rotationally symmetric distributions with bounded conditional coefficient of variation.</p><p xml:id="_s8n3r6T">These distribution classes are formally defined in Definition 5 below, and include the multi-variate Gaussian, Laplace, and Cauchy distributions as special cases.</p><p xml:id="_xzKJhpT">Definition 5 (P β 1 ,β 2 distribution). We define P rot symm as the class of joint distributions D with conditional rotational symmetry in the norm • , namely for all x ∈ X and c = E[c|x], there exists a function</p><formula xml:id="formula_67" coords="37,72.00,498.60,398.72,36.34">q(•) : [0, ∞) → [0, ∞) such that P(c|x) = q( c − c ). For constants β 1 , β 2 ∈ (0, 1), define P β 1 ,β 2 := D ∈ P rot symm : For any c 1 ∈ R d , P c|x (0 ≤ c T c 1 ≤ β 1 c 1 c ) ≥ β 2 .</formula><p xml:id="_5UDNBNr">Lemma 6 provides a bound for the pointwise error for strongly-convex feasible regions under the class of distributions in P β 1 ,β 2 specified in Definition 5.</p><p xml:id="_bsaHdMN">Lemma 6 (Example of φ). Given • as the 2 norm, let f : R d → R be a µ S -strongly convex and L Ssmooth function for some L S ≥ µ S &gt; 0. Suppose that the feasible region S is defined by</p><formula xml:id="formula_68" coords="37,438.51,625.61,101.49,10.19">S = {w ∈ R d : f (w) ≤ r}</formula><p xml:id="_Zas2ahk">for some constant r &gt; f min := min w f (w). Suppose that Assumption 6 holds and that the hypothesis class H is well-specified, i.e., h * (x) = E[c|x], for all x ∈ X . When the distribution D ∈ P β 1 ,β 2 , then it holds that for almost all x ∈ X ,</p><formula xml:id="formula_69" coords="37,78.54,693.99,454.92,32.79">R SPO+ (h) − R SPO+ (h * ) ≤ ⇒ h(x) − h * (x) 2 ≤ κµ 2 S r 1/2 2 1/2 β 2 L 5/2 S min 2(1 − β 2 1 ) ρ(C, Ĉ) , √ 17 + 8β 1 − 1 − 4β 1 4ρ(C, Ĉ) −1 • .</formula><p xml:id="_myUr3Ux">Lemma 6 implies that for the strongly-convex feasible region, if the distribution D ∈ P β 1 ,β 2 , we have φ( ) ≤ O( √ ). Since Theorem 1 also requires Assumption 2 holds, in Lemma 7 below, we provide the conditions that Assumption 2 holds for the SPO+ loss.</p><p xml:id="_5P4HxA7">Proof of Lemma 6 For given x ∈ X , let c = E[c|x] and ∆ = h(x) − h * (x). By applying the results in Theorem C.2 of <ref type="bibr" coords="38,143.34,149.72,95.02,8.74" target="#b31">Liu and Grigas (2021)</ref>, it holds that</p><formula xml:id="formula_70" coords="38,131.49,166.89,349.02,26.50">E [ SPO+ (h(x), c) − SPO+ (h * (x), c)|x] ≥ µ 2 S r 1/2 2 1/2 L 5/2 S • E c|x c + 2∆ − c T (c + 2∆) c .</formula><p xml:id="_2tMY3PQ">It is easy to verify that c</p><formula xml:id="formula_71" coords="38,197.11,200.04,161.94,15.21">+ 2∆ − c T (c+2∆) c ≥ 0, for any c ∈ R d .</formula><p xml:id="_R96zyGn">For any ∆, we define a subspace ∆ ⊥,β 1 by ∆</p><formula xml:id="formula_72" coords="38,72.00,220.11,468.00,27.60">⊥,β 1 = {c ∈ C : 0 ≤ c T ∆ ≤ β 1 ∆ c }. By Definition 5, we have that P c|x (c ∈ ∆ ⊥,β 1 ) ≥ β 2 &gt; 0, for any x ∈ X .</formula><p xml:id="_3K52qft">Thus, we have that</p><formula xml:id="formula_73" coords="38,70.83,272.64,469.54,324.08">E c + 2∆ − c T (c + 2∆) c |x ≥ P c|x (c ∈ ∆ ⊥,β 1 )E c + 2∆ − c T (c + 2∆) c c ∈ ∆ ⊥,β 1 ≥ β 2 E c + 2∆ − c T (c + 2∆) c c ∈ ∆ ⊥,β 1 = β 2 E c + 2∆ − c − 2c T ∆ c c ∈ ∆ ⊥,β 1 . Next, we show that E c + 2∆ − c − 2c T ∆ c c ∈ ∆ ⊥,β 1 ≥ 1 16ρ(C) ∆ 2 . Since c ≤ ρ(C), we have that c + 2∆ − c − 2c T ∆ c = c 2 + 4 ∆ 2 + 4c T ∆ − c − 2c T ∆ c ≥ c 2 + 4 ∆ 2 + 4β 1 c ∆ − c − 2β 1 ∆ (6) ≥ ρ(C) 2 + 4 ∆ 2 + 4β 1 ρ(C) ∆ − ρ(C) − 2β ∆ (7) Inequality (6) is because c 2 + 4 ∆ 2 + 4β 1 c ∆ − c −2β 1 ∆ is a decreasing function of β 1 . Inequality (7) is because c 2 + 4 ∆ 2 + 4β 1 c ∆ − c − 2β 1 ∆ is a decreasing function of c when β 1 &lt; 1. Since ∆ ≤ 2ρ(C, Ĉ), by minimizing function f (x) = √ ρ(C) 2 +2β 1 ρ(C)x+x 2 −ρ(C)−β 1 x x 2 over (0, 2ρ(C, Ĉ)), we have that ρ(C) 2 + 4 ∆ 2 + 4β 1 ρ(C)∆ − ρ(C) − 2β 1 ∆ ∆ 2 &gt; min 2(1 − β 2 1 ) ρ(C, Ĉ) , √ 17 + 8β − 1 − 4β 1 4ρ(C, Ĉ) Thus, we have that E c + 2∆ − c c ∈ ∆ ⊥,β 1 ≥ min 2(1−β 2 1 ) ρ(C, Ĉ) , √ 17+8β 1 −1−4β 1 4ρ(C, Ĉ) ∆ 2 .</formula><p xml:id="_PuWFS2N">Thus, we conclude that for all P ∈ P β 1 ,β 2 , it holds that</p><formula xml:id="formula_74" coords="38,76.98,617.26,477.22,32.79">h(x) − h * (x) 2 ≤ µ 2 S r 1/2 2 1/2 β 2 L 5/2 S min 2(1 − β 2 1 ) ρ(C, Ĉ) , √ 17 + 8β 1 − 1 − 4β 1 4ρ(C, Ĉ) −1 • E c|x [ SPO+ (h(x), c) − SPO+ (h * (x), c)] .</formula><p xml:id="_aYdcX59">Taking the expectation of both sides with x, we obtain that</p><formula xml:id="formula_75" coords="38,86.04,673.34,439.91,32.79">E[ h(x) − h * (x) 2 ] ≤ µ 2 S r 1/2 2 1/2 β 2 L 5/2 S min 2(1 − β 2 1 ) ρ(C, Ĉ) , √ 17 + 8β 1 − 1 − 4β 1 4ρ(C, Ĉ) −1 • (R SPO+ (h) − R SPO+ (h * )).</formula><p xml:id="_KBHG6PB">Then, combining the result with Assumption 6, we obtain Lemma 6.</p><p xml:id="_bM8qH2V">Lemma 7 (Existence of η for SPO+ ). Given • as the 2 norm, let f : R d → R be a µ S -strongly convex and L S -smooth function for some L S ≥ µ S &gt; 0. Suppose that the feasible region S is defined by S = {w ∈ R d : f (w) ≤ r} for some constant r &gt; f min := min w f (w). Suppose the hypothesis class H is well-specified, i.e.,</p><formula xml:id="formula_76" coords="39,71.47,131.61,468.36,61.44">h * (x) = E[c|x], for all x ∈ X . Suppose distribution D ∈ {D ∈ P rot symm : P c|x ( c ≥ β) = 1, for all x ∈ X }, for some positive β &gt; 0. Then, SPO+ (•, c) satisfies that for all x ∈ X , c 1 ∈ C and h * ∈ H * , |E[ SPO+ (c 1 , c) − SPO+ (h * (x), c)|x]| ≤ L 2 S ρ(C) √ r − f min √ 2µ 1.5 S 4 β c 1 − h * (x) 2 .</formula><p xml:id="_hMQegpy">Lemma 7 shows that when the feasible region is strongly convex and the hypothesis class is well-specified, and when the distribution of cost vectors is separated from the origin with probability 1, then η in Assumption 2 is finite for the SPO+ loss.</p><p xml:id="_ubdEkfP">Proof of Lemma 7 Since the hypothesis class is well-specified, we denote h * (x) by c, given x ∈ X . Then, we define ∆ = c 1 − c. According to Theorem 1 in <ref type="bibr" coords="39,291.46,268.29,135.51,8.74" target="#b15">Elmachtoub and Grigas (2022)</ref>, we have that the excess SPO+ risk at x for the prediction c 1 is</p><formula xml:id="formula_77" coords="39,150.88,308.87,310.24,10.95">E[ SPO+ (c + ∆, c) − SPO+ (c, c)|x] = E[(c + 2∆) T (w * (c) − w * (c + 2∆))|x]</formula><p xml:id="_UNsDGGs">According to Lemmas 1 and 2 in Liu and Grigas (2021), we have that for any c 1 , c 2 ∈ C, it holds that</p><formula xml:id="formula_78" coords="39,182.34,347.21,247.31,32.42">c T 1 (w * (c 2 ) − w * (c 1 )) ≤ L 2 S ρ(C) √ r − f min √ 2µ 1.5 S c 1 c 1 − c 2 c 2 2 .</formula><p xml:id="_QKMHxt2">Replacing c 1 and c 2 with c + 2∆ and c, we obtain that</p><formula xml:id="formula_79" coords="39,135.71,399.14,340.59,32.42">E[ SPO+ (c + ∆, c) − SPO+ (c, c)|x] ≤ L 2 S ρ(C) √ r − f min √ 2µ 1.5 S E c c − c + 2∆ c + 2∆ 2 .</formula><p xml:id="_dhQYQUW">Thus, to prove Lemma 7, it suffices to show that c c − c+2∆ c+2∆ ≤ 4 β ∆ for any realized c and any ∆ ∈ R d . We consider two cases: (1) 2∆ ≥ c , and (2) 2∆ ≤ c .</p><p xml:id="_UU6TukA">In the first case, since c ≥ β, we have that 2∆</p><formula xml:id="formula_80" coords="39,82.55,478.03,457.72,30.60">≥ c ≥ β. Since c c − c+2∆ c+2∆ ≤ 2, we have that c c − c+2∆ c+2∆ ≤ 4 ∆ β .</formula><p xml:id="_kRFa8BR">In the other case, when 2∆ ≤ c , we have</p><formula xml:id="formula_81" coords="39,227.46,536.39,168.79,22.47">c c − c + 2∆ c + 2∆ = 2 − 2 c T (c + 2∆) c c + 2∆ .</formula><p xml:id="_MxXVwbX">We use θ ∈ [0, π 2 ) to denote the angle between c and c + 2∆, then, we have</p><formula xml:id="formula_82" coords="39,71.75,572.75,468.25,30.60">c c − c+2∆ c+2∆ = 2 − 2 cos(θ) ≤ 2 sin(θ). Since 2 ∆ ≥ 2 c sin(θ), we have that c c − c+2∆ c+2∆ ≤ 2 ∆ c ≤ 2 β ∆</formula><p xml:id="_Ky4t3Ac">. Thus, we obtain Lemma 7.</p><p xml:id="_kjUbHQu">In conclusion of Appendix A, it is worth noting that while our analysis in this section focused on the SPO+ loss function, similar results can be obtained for commonly used loss functions such as squared 2 norm loss.</p><p xml:id="_MgqsAkv">For example, under some noise conditions, we can also obtain φ( ) ∼ √ and the upper bound for η under the squared 2 norm loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_k2nVwST">Appendix B: Proofs</head><p xml:id="_3XaYXaG">In this appendix, we provide the proofs that are omitted in the main body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_PuhKZxM">B.1. Proofs for Section 3</head><p xml:id="_x7ewfqj">Proof of Lemma 1 Without loss of generality, assume that ν S (c 1 ) ≥ ν S (c 2 ), i.e., we have that 0 ≤ c 1 − c 2 &lt; ν S (c 1 ). We also claim that ν S (c 2 ) &gt; 0. Indeed, since ν S is a 1-Lipschitz distance function, it holds that ν S (c 1 ) − ν S (c 2 ) ≤ c 1 − c 2 &lt; ν S (c 1 ) and hence ν S (c 2 ) &gt; 0. As above, let {v j : j = 1, ..., K} be the extreme points of S, i.e., S = conv(v 1 , . . . , v K ). Since ν S (c 1 ) &gt; 0 and ν S (c 2 ) &gt; 0, both w * (c 1 ) and w * (c 2 ) must be extreme points solutions, i.e., w * (c 1 ) = v j 1 and w * (c 1 ) = v j 2 for some indices j 1 and j 2 .</p><p xml:id="_UrBzHZV">We now prove the lemma by contradiction. If w * (c 1 ) = w * (c 2 ), then by (4), the following two inequalities hold:</p><formula xml:id="formula_83" coords="40,171.34,222.16,269.33,24.02">ν S (c 1 ) ≤ c T 1 (w * (c 2 ) − w * (c 1 )) w * (c 2 ) − w * (c 1 ) * , ν S (c 2 ) ≤ c T 2 (w * (c 1 ) − w * (c 2 )) w * (c 1 ) − w * (c 2 ) * .</formula><p xml:id="_W4QyT9Q">We add up both sides of the above two inequalities, and get</p><formula xml:id="formula_84" coords="40,114.27,273.38,383.46,24.02">ν S (c 1 ) + ν S (c 2 ) ≤ (c 1 − c 2 ) T (w * (c 2 ) − w * (c 1 )) w * (c 2 ) − w * (c 1 ) * ≤ c 1 − c 2 w * (c 2 ) − w * (c 1 ) * w * (c 2 ) − w * (c 1 ) * = c 1 − c 2 ,</formula><p xml:id="_QXszgNG">where the second inequality uses Hölder's inequality. Because</p><formula xml:id="formula_85" coords="40,76.98,306.62,463.02,26.86">c 1 − c 2 &lt; ν S (c 1 ), we have that ν S (c 1 ) + ν S (c 2 ) ≤ c 1 − c 2 &lt; ν S (c 1</formula><p xml:id="_eqX9JZS">). This implies that ν S (c 2 ) &lt; 0, which contradcits that ν S is a non-negative distance function.</p><p xml:id="_gDkEkey">Thus, we conclude that w * (c 1 ) = w * (c 2 ).</p><p xml:id="_rcfB7NE">Proof of Lemma 2 Let t ≥ 0 be given. First, we show that, for any given</p><formula xml:id="formula_86" coords="40,71.75,357.62,468.25,62.25">x ∈ X , inf h * ∈H * {ν S (h * (x))} ≥ 2b t implies that ν S (h t (x)) ≥ b t . Indeed, since ν S is a 1-Lipschitz distance function, we have that |ν S (h * (x)) − ν S (h t (x))| ≤ h t (x) − h * (x) for all h * ∈ H * . Since H * ⊆ H * and Dist H * (h t ) ≤ b t , we have that Dist H * (h t ) ≤ Dist H * (h t ) ≤ b t .</formula><p xml:id="_abqWUVJ">Hence, for any &gt; 0, there exists h * ∈ H * satisfying</p><formula xml:id="formula_87" coords="40,173.12,434.63,260.07,10.85">ν S (h * (x)) − ν S (h t (x)) ≤ h t (x) − h * (x) ≤ h t − h * ∞ ≤ b t +</formula><p xml:id="_W3CWRGg">Since the result holds for all &gt; 0, we conclude that ν S (h t (x))</p><formula xml:id="formula_88" coords="40,72.00,460.74,468.00,27.65">≥ inf h * ∈H * {ν S (h * (x))} − b t . Furthermore, since inf h * ∈H * {ν S (h * (x))} ≥ 2b t , it holds that ν S (h t (x)) ≥ 2b t − b t = b t .</formula><p xml:id="_nc7RRWH">According to Algorithm 1, a label for x t is always acquired at</p><formula xml:id="formula_89" coords="40,374.81,496.23,167.12,9.46">iteration t ≥ 1 if ν S (h t−1 (x t )) &lt; b t−1 .</formula><p xml:id="_xGgG3r5">Otherwise, if ν S (h t−1 (x t )) ≥ b t−1 , then a label is acquired with probability p. Therefore, using the argument above, the label probability at iteration t is</p><formula xml:id="formula_90" coords="40,132.74,556.43,346.52,55.70">P(acquire a label for x t ) = P(ν S (h t−1 (x t )) &lt; b t−1 ) + pP(ν S (h t−1 (x t )) ≥ b t−1 ) ≤ P(ν S (h t−1 (x t )) &lt; b t−1 ) + p ≤ P inf h * ∈H * {ν S (h * (x t ))} &lt; 2b t−1 + p ≤ Ψ(2b t−1 ) + p</formula><p xml:id="_m67wmT5">Then, the expected number of acquired labels after T total iterations is at most T t=1 P(acquire a label for x t ) ≤ pT + T t=1 Ψ(2b t−1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_HqDv9GT">B.2. Proofs for Section 4</head><p xml:id="_PTgXhfH">Proof of Proposition 2 We use the results in <ref type="bibr" coords="40,308.66,682.12,100.84,8.74" target="#b35">Rakhlin et al. (2015a)</ref> to prove that ln(N</p><formula xml:id="formula_91" coords="40,72.00,681.66,468.00,29.46">1 (α, rew • H, T )) ≤ d θ ln 1 + 2ρ(Θ)L 1 L 2 αp I{ p&gt;0}</formula><p xml:id="_MVc7kMZ">. We use N ∞ (α, rew • H,z z z) to denote the sequential covering number with respect to the ∞ norm on tree z z z, defined analogousy to Definition 3. In other words, N ∞ (α, rew • H,z z z) is the size of the minimal sequential cover V of real-valued trees on a tree z of depth T such that, for all h ∈ H and all paths σ σ σ ∈ {±1} T , there exists a real-valued tree</p><formula xml:id="formula_92" coords="41,229.36,97.48,153.29,36.14">v v v ∈ V such that max t=1,...,T {|v v v t (σ σ σ) − rew (h;z z z t (σ σ σ))|} ≤ α.</formula><p xml:id="_8Hj2hWU">We further define the sequential covering number with respect to the ∞ norm by</p><formula xml:id="formula_93" coords="41,72.00,141.13,468.00,27.51">N ∞ (α, rew • H, T ) := sup z z z {N ∞ (α, rew • H,z z z)}.</formula><p xml:id="_7bDen2u">Since the surrogate loss is an L 2 -Lipschitz function of h(x t ) with respect to ∞ norm, and H is a class of functions smoothly-parameterized by θ ∈ Θ with respect to the ∞ norm with parameter L 1 , we have that the composition function rew (h; •) is also smoothly-parameterized by θ ∈ Θ with respect to the ∞ norm with parameter L 1 L 2 pI{ p&gt;0} , for any given c t , d M t , and q t . We denote the i.i.d. ∞ covering number of function class { rew (h; •)|h ∈ H} at scale α by N∞ (α, rew • H), which was defined in Section 4.1 after the Definition 3.</p><p xml:id="_7jaYdrE">Since the hypothesis class is smoothly parameterized, the i.i.d. covering number, N∞ (α, rew • H) is at most N∞ ( αp I{ p&gt;0} L 1 L 2 , Θ), for example, see Theorem 2.7.11 in <ref type="bibr" coords="41,305.43,291.40,89.64,8.74" target="#b41">Wellner et al. (2013)</ref>. By Example 5.8 in Wainwright (2019), we have that ln( N∞ (α, Θ))</p><formula xml:id="formula_94" coords="41,223.25,305.53,310.37,14.21">≤ d θ ln(1 + 2ρ(Θ) α ), we have that ln( N∞ (α, rew • H)) ≤ d θ ln(1 + 2ρ(Θ)L 1 L 2 αp I{ p&gt;0}</formula><p xml:id="_UcUnFp7">). Equation ( <ref type="formula" coords="41,128.42,324.69,8.68,8.74" target="#formula_121">14</ref>) in <ref type="bibr" coords="41,155.98,324.69,93.55,8.74" target="#b35">Rakhlin et al. (2015a)</ref> indicates that the sequential ∞ covering number is upper bounded by the i.i.d. ∞ covering number, i.e., ln(N ∞ (α, rew • H, T )) ≤ ln( N∞ (α, rew • H)), for all T ≥ 1. (Intuitively, if two functions f and g satisfies f − g ∞ ≤ α, then for the values of nodes on any path, z 1 , z 2 , ..., z T , we</p><formula xml:id="formula_95" coords="41,72.00,372.31,444.61,14.02">have that |f (z i ) − g(z i )| ≤ α, for i = 1, ..., T .) Thus, ln(N ∞ (α, rew • H, T )) is at most d θ ln(1 + 2ρ(Θ)L 1 L 2 αp I{ p&gt;0}</formula><p xml:id="_Fk3RNXj">). Then, by <ref type="bibr" coords="41,124.38,391.28,93.95,8.74" target="#b36">Rakhlin et al. (2015b)</ref>, we have that</p><formula xml:id="formula_96" coords="41,72.00,390.82,468.00,28.81">N 1 (α, rew • H, T ) ≤ N ∞ (α, rew • H, T ). Thus, ln(N 1 (α, rew • H, T )) is at most d θ ln(1 + 2ρ(Θ)L 1 L 2 αp I{ p&gt;0}</formula><p xml:id="_xaQVekk">). Thus, we have that for any fixed α &gt; 0, ln(N</p><formula xml:id="formula_97" coords="41,72.00,407.46,468.00,28.81">1 (α, rew • H, T )) ≤ d θ ln(1 + 2ρ(Θ)L 1 L 2 αp I{ p&gt;0} ) ≤ O(ln( 1 αp I{ p&gt;0}</formula><p xml:id="_eyeBnaZ">)). Proof of Lemma 3 Let t ∈ {1, . . . , T } be fixed. Since p = 0, the re-weighted loss function can be written as rew (h; z t ) = (h(x t ), c t )d M t = (h(x t ), c t )I{ν S (h t−1 (x t )) &lt; b t−1 }, where h refers to a generic h ∈ H throughout. Recall that (h; z t ) denotes (h(x t ), c t ) and notice that we have the following simple decomposition:</p><formula xml:id="formula_98" coords="41,224.27,495.04,167.61,11.82">(h; z t ) = (h; z t )(1 − d M t ) + (h; z t )d M t ,</formula><p xml:id="_xR2BsAC">Hence, by the definition of f t (h), we have</p><formula xml:id="formula_99" coords="41,145.24,538.53,321.53,12.16">E[ (h; z t )|F t−1 ] = f t (h) + E[ (h; z t )d M t |F t−1 ] = f t (h) + E[ rew (h; z t )|F t−1 ].</formula><p xml:id="_QcZQzaS">Since (x 1 , c t ), . . . , (x T , c T ) are i.i.d. random variables following distribution D, (x t , c t ) is independent of F t−1 and hence</p><formula xml:id="formula_100" coords="41,167.90,593.74,372.10,12.16">R (h) = E[ (h; z t )] = E[ (h; z t )|F t−1 ] = f t (h) + E[ rew (h; z t )|F t−1 ].<label>(8)</label></formula><p xml:id="_vQrdKNB">Consider (8) applied to both h ∈ H and h * ∈ H * and averaged over t ∈ {1, . . . , T } to yield:</p><formula xml:id="formula_101" coords="41,112.45,632.05,427.55,29.15">R (h) − R (h * ) = 1 T T t=1 f t (h) − f t (h * ) + 1 T T t=1 (E[ rew (h; z t )|F t−1 ] − E[ rew (h * ; z t )|F t−1 ])<label>(9)</label></formula><p xml:id="_xEmQwD5">Thus, by the definition of Z t h , ( <ref type="formula" coords="41,208.23,666.97,4.24,8.74" target="#formula_101">9</ref>) is equivalently written as: </p><formula xml:id="formula_102" coords="41,120.82,682.54,414.76,29.15">R (h) − R (h * ) = 1 T T t=1 f t (h) − f t (h * ) + 1 T T t=1 Z t h + 1 T T t=1 ( rew (h; z t ) − rew (h * ; z t )) .<label>(10</label></formula><formula xml:id="formula_103" coords="42,72.00,96.36,468.00,26.46">f 1 (h) − f 1 (h * ) ≤ ω ( Ĉ, C) ≤ ηb 2</formula><p xml:id="_ungqzcf">0 . Now, consider t ≥ 2 and assume that parts (a) and (b) hold for all t ∈ {1, . . . , t − 1}. Namely, for all t ∈ {1, . . . , t − 1}, the following two conditions hold: (a) h * ∈ Ht −1 , and (b)</p><formula xml:id="formula_104" coords="42,385.06,145.81,156.87,12.77">sup h∈H t−1 f t(h) − f t(h * ) ≤ ηb 2 t−1 .</formula><p xml:id="_YHG5ZBF">Then, our goal is to show that the two claims hold for t.</p><p xml:id="_vJAEQZ9">First, we prove (a). Recall that h * denotes the unique minimizer of the surrogate risk R , and h t−1 denotes the predictor from iteration t − 1 of Algorithm 1. By Lemma 3, we have that</p><formula xml:id="formula_105" coords="42,72.00,210.80,463.46,144.86">R (h t−1 ) − R (h * ) = 1 t − 1 t−1 i=1 f i (h t−1 ) − f i (h * ) + 1 t − 1 t−1 i=1 Z i h t−1 + 1 t − 1 t−1 i=1 ( rew (h t−1 ; z i ) − rew (h * ; z i )) . Since R (h t−1 ) − R (h * ) ≥ 0, we have that 1 t − 1 t−1 i=1 ( rew (h * ; z i ) − rew (h t−1 ; z i )) ≤ 1 t − 1 t−1 i=1 f i (h t−1 ) − f i (h * ) + 1 t − 1 t−1 i=1 Z i h t−1 ≤ 1 t − 1 t−1 i=1 sup h∈H i−1 f i (h) − f i (h * ) + 1 t − 1 t−1 i=1 Z i h t−1 ≤ 1 t − 1 t−1 i=1 ηb 2 i−1 + r t−1 ,</formula><p xml:id="_7uv3zjt">where the second inequality uses h t−1 ∈ H t−2 ⊆ H i−1 for i ∈ {1, . . . , t − 1}, and the third inequality uses assumption (b) of induction and the assumption that r t ≥ sup h∈H</p><formula xml:id="formula_106" coords="42,369.50,374.83,93.92,13.69">1 t t i=1 Z i h for t ≥ 1.</formula><p xml:id="_TVwbRB6">Recall from Algorithm 1 that the reweighted loss function at iteration </p><formula xml:id="formula_107" coords="42,73.20,390.97,466.81,31.14">t − 1 is ˆ t−1 (h) = 1 t (x,c)∈W t−1 (h(x), c) = 1 t−1 t−1 i=1 rew (h; z i ),</formula><formula xml:id="formula_108" coords="42,166.74,491.62,112.09,10.35">h − h * ∞ ≤ φ(R (h) − R * )</formula><p xml:id="_ueX7bHP">. By Assumption 2, we then have that</p><formula xml:id="formula_109" coords="42,155.72,511.73,379.85,89.56">| f t (h) − f t (h * )| = E[ (h(x t ), c t ) − (h * (x t ), c t )|d M t = 0]P(d M t = 0|F t−1 ) ≤ E[ (h(x t ), c t ) − (h * (x t ), c t )|d M t = 0] ≤ E[E[ (h(x t ), c t ) − (h * (x t ), c t )|x t ]|d M t = 0] ≤ ηE[ h(x t ) − h * (x t ) 2 |d M t = 0] ≤ η (φ(R (h) − R * )) 2 . (<label>11</label></formula><formula xml:id="formula_110" coords="42,535.57,592.56,4.43,8.74">)</formula><p xml:id="_BHRpnMQ">By Lemma 3, we have that</p><formula xml:id="formula_111" coords="42,104.25,627.81,435.75,95.88">R (h) − R (h * ) = 1 t − 1 t−1 i=1 f i (h) − f i (h * ) + 1 t − 1 t−1 i=1 Z i h + 1 t − 1 t−1 i=1 ( rew (h; z i ) − rew (h * ; z i )) ≤ 1 t − 1 t−1 i=1 ηb 2 i−1 + r t−1 + 1 t − 1 t−1 i=1 ( rew (h; z i ) − rew (h * ; z i )) = 1 t − 1 t−1 i=1 ηb 2 i−1 + r t−1 + ˆ t−1 (h) − ˆ t−1 (h * ),<label>(12)</label></formula><p xml:id="_bwS3Dyv">where the inequality follows by assumption (b) of induction since h ∈ H </p><formula xml:id="formula_112" coords="43,72.00,151.53,454.52,50.74">ˆ t−1, * ≤ ˆ t−1 (h) ≤ ˆ t−1, * + r t−1 + 1 t − 1 t−1 i=1 ηb 2 i−1 , and ˆ t−1, * ≤ ˆ t−1 (h * ) ≤ ˆ t−1, * + r t−1 + 1 t − 1 t−1 i=1 ηb 2 i−1 , hence ˆ t−1 (h) − ˆ t−1 (h * ) ≤ r t−1 + 1 t−1 t−1</formula><p xml:id="_HZ79rUT">i=1 ηb 2 i−1 and by combining with (12) we have</p><formula xml:id="formula_113" coords="43,218.16,210.64,175.68,29.15">R (h) − R (h * ) ≤ 2η t − 1 t−1 i=1 b 2 i−1 + 2r t−1 .</formula><p xml:id="_aSQwhhj">Combining the above inequality with (11) yields</p><formula xml:id="formula_114" coords="43,172.91,264.56,266.19,32.20">| f t (h) − f t (h * )| ≤ η φ 2η t − 1 t−1 i=1 b 2 i−1 + 2r t−1 2 = ηb 2 t−1 ,</formula><p xml:id="_SNczUy6">using the definition of b t−1 . Since h ∈ H t−1 is arbitrary, the conclusion in part (b) follows.</p><p xml:id="_tgZYxwA">Proof of Theorem 1 We provide the proof of each part separately.</p><p xml:id="_bYrYDbc">Part (a). Recall that h * is the unique minimizer of the surrogate risk R under Assumption 2 and that h T is the predictor from iteration T of Algorithm 1. By Lemma 3, we have the following decomposition:</p><formula xml:id="formula_115" coords="43,111.22,373.65,428.78,62.43">R (h T ) − R (h * ) = 1 T T t=1 f t (h T ) − f t (h * ) + 1 T T t=1 Z t h T + 1 T T t=1 ( rew (h T ; z t ) − rew (h * ; z t )) = 1 T T t=1 f t (h T ) − f t (h * ) + 1 T T t=1 Z t h T + ˆ T (h T ) − ˆ T (h * ),<label>(13)</label></formula><p xml:id="_87Bys4e">where we recall that the empirical re-weighted loss in Algorithm 1 is ˆ T (h</p><formula xml:id="formula_116" coords="43,73.20,444.72,466.80,30.05">) := 1 T (x,c)∈W T (h(x), c) = 1 T T t=1</formula><p xml:id="_YtQnuHa">rew (h; z t ) in this case (since p = 0). We will show that r t ≥ sup h∈H</p><formula xml:id="formula_117" coords="43,405.31,461.09,42.75,13.69">1 t t i=1 Z i h</formula><p xml:id="_mMs4x4Z">simultaneously for all t ≥ 1 with probability at least 1 − δ in order to apply Lemma 4, again with probability at least 1 − δ.</p><p xml:id="_xJ778mN">Indeed, suppose that the conclusions of Lemma 4 do hold. Then, by part (a), we have h * ∈ H T −1 and therefore ˆ T (h T ) ≤ ˆ T (h * ) by the update in Line 19 of Algorithm 1. By the nested structure of the H t sets, we have h T ∈ H T −1 ⊆ H t−1 for all t ∈ {1, . . . , T } and therefore, by part (b), we have that</p><formula xml:id="formula_118" coords="43,433.94,530.60,108.00,11.66">| f t (h T ) − f t (h * )| ≤ ηb 2 t−1 .</formula><p xml:id="_j8KQyDC">Thus, combining these inequalities with (13) yields:</p><formula xml:id="formula_119" coords="43,228.80,566.93,154.41,29.15">R (h T ) − R (h * ) ≤ r T + 1 T T −1 t=0 ηb 2 t ,</formula><p xml:id="_2mFFu7d">which is the result in part (a).  2T 2 N 1( ω ( Ĉ,C)/T, rew •H,T ) &gt; 1 − δ 2T 2 . Finally, applying the union bound over all T ≥ 1, we obtain that r T ≥ sup h∈H 1 T T t=1 Z t h simultaneously for all T ≥ 1 with probability at</p><formula xml:id="formula_120" coords="44,72.00,136.20,301.82,36.33">least 1 − δ 2 ∞ T =1 1 T 2 = 1 − δπ 2 12 &gt; 1 − δ.</formula><p xml:id="_JCxvCk9">Part (b). In this part of the proof, we do not assume that H * is a singleton as the same proof will apply later for Theorem 3. For any h * ∈ H * , the excess SPO risk can be written as</p><formula xml:id="formula_121" coords="44,83.01,216.88,452.57,11.92">R SPO (h T ) − R * SPO = E (x,c)∼D [c T (w * (h T (x)) − w * (h * (x)))] ≤ E (x,c)∼D [ c w * (h T (x)) − w * (h * (x)) * ]. (<label>14</label></formula><formula xml:id="formula_122" coords="44,535.57,218.27,4.43,8.74">)</formula><p xml:id="_nn9CexQ">We apply the conclusion of part (a), R (h</p><formula xml:id="formula_123" coords="44,247.33,238.89,108.50,13.69">T ) − R * ≤ r T + η T T −1 t=0 b 2 t</formula><p xml:id="_GJqx5Yp">, with probability at least 1 − δ. Specifically, we shall provide a bound on ∆ SPOa</p><formula xml:id="formula_124" coords="44,209.31,255.71,261.01,13.69">T := E[R SPO (h T ) − R * SPO |R (h T ) − R * ≤ r T + η T T −1 t=0 b 2 t ]</formula><p xml:id="_6dMfVgd">. If b T = 0, then φ(r T ) = 0 and Assumption 1.(2) implies that h T (x) = h * (x) almost everywhere and hence ∆ SPOa for any &gt; 0 there exists h * ∈ H * such that for almost every x ∈ X ,</p><formula xml:id="formula_125" coords="44,135.03,341.17,404.98,29.15">h T (x) − h * (x) ≤ φ r T + η T T −1 t=0 b 2 t + ≤ φ 2r T + 2η T T −1 t=0 b 2 t + = b T + .<label>(15)</label></formula><p xml:id="_cFEXpAz">Since S satisfies the strength property with parameter µ &gt; 0, we apply part (a) of Theorem 3 of El Balghiti et al. ( <ref type="formula" coords="44,102.08,393.74,18.45,8.74">2022</ref>) to yield (in our notation) for any h * ∈ H * and x ∈ X :</p><formula xml:id="formula_126" coords="44,138.06,409.91,401.95,23.03">w * (h T (x)) − w * (h * (x)) * ≤ 1 µ min{ν S (h T (x)), ν S (h * (x))} h T (x) − h * (x) .<label>(16)</label></formula><p xml:id="_8CqnKQp">Now, let γ T ≥ 2b T &gt; 0 be a given parameter. For a given x ∈ X we consider two cases: (i) ν S (h * (x)) &gt; γ T , and (ii) ν S (h * (x)) ≤ γ T . Under case (i), we have by the 1-Lipschitzness of ν S (•) that ν S (h</p><formula xml:id="formula_127" coords="44,76.98,455.49,463.02,27.16">T (x)) ≥ ν S (h * (x)) − h T (x) − h * (x) &gt; γ T − b T ≥ γ T − γ T /2 = γ T /2</formula><p xml:id="_bCEW5Y8">. Thus, we have that min{ν S (h T (x)), ν S (h * (x))} ≥ γ T /2. For case (i) we will combine together ( <ref type="formula" coords="44,228.29,490.00,8.21,8.74" target="#formula_121">14</ref>), (15), and ( <ref type="formula" coords="44,296.78,490.00,8.21,8.74" target="#formula_126">16</ref>), and use the fact that c ≤ ρ(C). For case (ii), we apply the worst case bound R SPO (h T ) − R * SPO ≤ ω S (C) and note that the probability of case (ii) occurring is at most P (inf h * ∈H * {ν S (h * (x))} ≤ γ T ) = Ψ(γ T ). Overall, we have</p><formula xml:id="formula_128" coords="44,114.13,539.80,383.74,23.03">∆ SPOa T ≤ ρ(C) w * (h T (x)) − w * (h * (x)) * + Ψ(γ T )ω S (C) ≤ 2ρ(C)(b T + ) µγ T + Ψ(γ T )ω S (C).</formula><p xml:id="_scb6UFK">We take → 0, and since γ T ≥ b T is arbitrary we take the infimum over γ T to yield part (b).</p><p xml:id="_rkYPGvK">Part (c). Again, in this part of the proof, we do not assume that H * is a singleton as the same proof will apply later for Theorems 2 and 3. Recall that for any h * ∈ H * , the excess SPO risk can be written as</p><formula xml:id="formula_129" coords="44,178.09,624.66,255.83,11.92">R SPO (h T ) − R * SPO = E (x,c)∼D [c T (w * (h T (x)) − w * (h * (x)))].</formula><p xml:id="_D2r5fgZ">Again, we apply part (a) with probability at least 1 − δ. Recall that, by Assumption 1.(1), we have that Dist H * (h T ) ≤ Dist H * (h T ). Then, by combining Assumption 1.(2) with part (a), with probability at least 1 − δ, for any &gt; 0 there exists h * ∈ H * such that for almost every x ∈ X ,</p><formula xml:id="formula_130" coords="44,135.03,698.52,400.55,29.15">h T (x) − h * (x) ≤ φ r T + η T T −1 t=0 b 2 t + ≤ φ 2r T + 2η T T −1 t=0 b 2 t + = b T + . (<label>17</label></formula><formula xml:id="formula_131" coords="44,535.57,708.25,4.43,8.74">)</formula><p xml:id="_eHMpfVx">For a given x ∈ X we consider two cases: (i) ν S (h * (x)) ≥ 2b T , and (ii) ν S (h * (x)) &lt; 2b T . Under case (i), we have that max{ν S (h T (x)), ν S (h * (x))} ≥ 2b T &gt; b T + for &lt; b T ; thus combining Lemma 1 and (17) yields that w * (h T (x)) = w * (h * (x)), and hence R SPO (h T ) − R * SPO = 0, for almost every x ∈ X under case (i). For case (ii), we also apply the worst case bound R SPO (h T ) − R * SPO ≤ ω S (C) and note that the probability of case (ii) occurring is at most P (inf h * ∈H * {ν S (h * (x))} &lt; 2b T ) ≤ Ψ(2b T ). Therefore, overall we have with probability at</p><formula xml:id="formula_132" coords="45,72.00,164.49,308.04,27.20">least 1 − δ, R SPO (h T ) − R * SPO ≤ Ψ(2b T )ω S (C).</formula><p xml:id="_x4CmcMP">Part (d). First note that, by Assumption 1.</p><p xml:id="_tBgM8AM">(2), we have that Dist</p><formula xml:id="formula_133" coords="45,70.78,199.78,469.49,61.29">H * (h 0 ) ≤ φ(R (h 0 ) − R * ) ≤ φ(ω ( Ĉ, C)) ≤ φ(r 0 ) ≤ b 0 . Again, we apply part (a) with probability at least 1−δ. Indeed, when R (h T )−R * ≤ r T + η T T −1 t=0 b 2 t holds, by Assumption 1.(2), we have that Dist H * (h T ) ≤ φ(R (h T ) − R * ) ≤ φ(r T + η T T −1 t=0 b 2 t ) ≤ b T . Thus, part<label>(</label></formula><p xml:id="_2PS8y8k">a) implies Dist H * (h t ) ≤ b t holds simultaneously for all t ≥ 0 with probability at least 1 − δ. By Lemma 2, since p = 0, conditional on part (a), the label complexity is at most T t=1 Ψ(2b t−1 ). With probability at most δ, we consider the worst case label complexity T and hence arrive at the overall label complexity bound of T t=1 Ψ(2b t−1 ) + δT .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_DPfPcsc">Proof of Proposition 3</head><p xml:id="_PqMFrGv">Let h ∈ H satisfy the conditions in Assumption 3. We will show that R SPO+ ( h) = 0 and therefore, since 0 ≤ R SPO (h) ≤ R SPO+ (h) for all h ∈ H, we have that R * SPO+ = R * SPO = 0 and h is a minimizer for both.</p><p xml:id="_Kk9S7YB">Recall that for prediction ĉ ∈ R d and realized cost vector c ∈ R d , the SPO+ satisfies as</p><formula xml:id="formula_134" coords="45,174.57,406.46,267.02,70.02">SPO+ (ĉ, c) := max w∈S (c − 2ĉ) T w + 2ĉ T w * (c) − c T w * (c) = − min w∈S (2ĉ − c) T w + 2ĉ T w * (c) − c T w * (c) = (c − 2ĉ) T w * (2ĉ − c) + 2ĉ T w * (c) − c T w * (c) = 2ĉ T (w * (c) − w * (2ĉ − c)) + c T (w * (2ĉ − c) − w * (c)).</formula><p xml:id="_NEmXB4D">Under Assumption 3 in the polyhedral case, we have by Lemma 1 that w * ( h(x)) = w * (c) with probability one over (x, c) ∼ D. Similarly, we have that ( Recall that (x 1 , c 1 ), (x 2 , c 2 ), . . . is the sequence of features and corresponding cost vectors of Algorithm 1. It is assumed that this sequence is an i.i.d. sequence from the distribution D and note that c t is only observed when we do not reject x t , i.e., when d M t = I(ν S (h t−1 (x t )) &lt; b t−1 ) = 1. In a slight abuse of notation, in this proof only, let us define Z t</p><formula xml:id="formula_135" coords="45,276.91,503.89,243.16,12.09">2 h(x) − c) − h(x) = h(x) − c ≤ ν S ( h(x)) &lt; ν S ( h(x)),</formula><formula xml:id="formula_136" coords="45,181.75,656.61,311.36,10.43">h := E[ SPO+ (h(x t ), c t )] − SPO+ (h(x t ), c t ) = R SPO+ (h) − SPO+ (h(x t ), c t )</formula><p xml:id="_TXPHssq">. Following the template of Lemma 4, the main idea of the proof is to show that, when r t ≥ sup h∈H 1 t t i=1 Z i h for all t ≥ 1, we have that:</p><p xml:id="_Vr2xs4f">(A) max t∈{1,...,T } { SPO+ (h T (x t ), c t )} = 0 with probability 1 for all T ≥ 1.</p><p xml:id="_3jAKy7d">In other words, h T achieves zero SPO+ loss across the entire sequence (x 1 , c 1 ), . . . , (x T , c T ). In fact, we show a strong result, which is that (A) holds for all minimizers of the empirical reweighted loss at iteration T . The proof of this is by strong induction, and we defer it to the end.</p><p xml:id="_vUsa3Cm">Notice that max t∈{1,...,T } { SPO+ (h T (x t ), c t )} = 0 implies, of course, that h T achieves zero (and hence minimizes) empirical risk 1 T T t=1 SPO+ (h(x t ), c t ). Thus, using r t ≥ sup h∈H</p><formula xml:id="formula_137" coords="46,72.00,146.28,468.00,56.11">1 t t i=1 Z i h for all t ≥ 1, we have that R SPO+ (h T ) = R SPO+ (h T ) − 1 T T t=1 SPO+ (h T (x t ), c t ) = 1 T T t=1 Z t h T ≤ r T ,</formula><p xml:id="_KBmhYe9">which is the result of part (a). </p><formula xml:id="formula_138" coords="46,99.40,273.88,413.20,29.15">P sup h∈H R SPO+ (h) − 1 T T t=1 SPO+ (h(x t ), c t ) ≥ ≤ 2 N1 (α, SPO+ • H) exp − T ( − 2α) 2 2ω ( Ĉ, C) 2 .</formula><p xml:id="_DUqa9Rr">Considering α = ω ( Ĉ,C) Proof of Claim (A). It remains to show that (A) holds for all T ≥ 1, which we prove by strong induction. In fact, we prove a stronger variant of (A) as follows. Recall that ˆ T (h) = 1 T (x,c)∈W T SPO+ (h(x), c) is the empirical reweighed loss at iteration T . Define H 0 T := {h ∈ H : ˆ T (h) = 0} = {h ∈ H : SPO+ (h(x), c) = 0 for all (x, c) ∈ W T }. The set H 0 T is exactly the set of minimizers of ˆ T (h), with probability 1, since Proposition 3 (in particular R * SPO+ = 0) implies that ˆ T (h * ) = 0 with probability 1. Hence, h T ∈ H 0 T with probability 1. Let us also define H0</p><p xml:id="_NYJ8kuJ">T := {h ∈ H : SPO+ (h(x t ), c t ) = 0 for all t = 1, . . . , T }. Clearly, H0</p><p xml:id="_J8Njv5a">T ⊆ H 0 T for all T ≥ 1. Note also that both collections of sets are nested, i.e., H 0</p><formula xml:id="formula_139" coords="46,311.01,465.37,207.58,13.04">T ⊆ H 0 T −1 ⊆ • • • H 0 1 ⊆ H and H0 T ⊆ H0 T −1 ⊆ • • • H0</formula><p xml:id="_zfE2yvJ">1 ⊆ H. Now, we will use strong induction to prove, when r t ≥ sup h∈H 1 t t i=1 Z i h for all t ≥ 1, we have that:</p><p xml:id="_WCssTWw">( Ā) H 0 T = H0 T with probability 1 for all T ≥ 1.</p><p xml:id="_4TyR5XA">Note that ( Ā) implies (A) since h T ∈ H 0 T with probability 1. To prove the base case T = 1, we observe that b 0 ≥ ρ( Ĉ), and thus, ν S (h(x 1 )) ≤ h(x 1 ) ≤ ρ( Ĉ) ≤ b 0 for any h ∈ H and any x 1 ∈ X . Thus, we have that d M 1 = 1 with probability 1 and the sample (x 1 , c 1 ) is added to working set W 1 . By definition of H 0 1 , for h ∈ H 0 1 we have that SPO+ (h(x 1 ), c 1 ) = 0. Hence, h ∈ H0 1 and so we have proven that H 0 1 ⊆ H0 1 . Now, consider T ≥ 2 and assume that ( Ā) holds for all T ∈ {1, . . . , T − 1}. We need to show that H 0 T ⊆ H0 T , so let h ∈ H 0 T be given. By the induction hypothesis, we have that h ∈ H 0 T −1 = H0 T −1 , and therefore we have SPO+ (h(x t ), c t ) = 0 for all t ∈ {1, . . . , T − 1}. Thus, to show that h ∈ H0 T , it suffices to show that SPO+ (h(x T ), c T ) = 0 with probability 1. There are two cases to consider. First, if d M T = 1, then the sample (x T , c T ) is added to working set W T and thus, by definition of H 0 T , for h ∈ H 0 T we have that SPO+ (h(x T ), c T ) = 0. Hence, h ∈ H0 T and so we have proven that H 0 T ⊆ H0 T .</p><p xml:id="_8ZxmXej">Second, let us consider the case where d M T = 0 so we do not acquire the label c T . In this case, we have that W T = W T −1 , H 0 T = H 0 T −1 , and, by the rejection criterion, ν S (h T −1 (x T )) ≥ b T −1 . For the given h ∈ H 0 T , to show that SPO+ (h(x T ), c T ) = 0 with probability 1 recall from the proof of Proposition 3 that is suffices to show that w * (2h(x T ) − c T ) = w * (c T ) with probability 1 over c T drawn from the conditional distribution given x T .</p><p xml:id="_XD5ANWh">To prove this, first note that</p><formula xml:id="formula_140" coords="47,144.73,169.48,322.53,29.15">R SPO+ (h) = R SPO+ (h) − 1 T − 1 T −1 t=1 SPO+ (h(x t ), c t ) = 1 T − 1 T −1 t=1 Z t h ≤ r T −1 ,</formula><p xml:id="_9xjkn9C">where the first equality uses that h ∈ H 0 T = H 0 T −1 = H0 T −1 and the inequality uses the assumption that r t ≥ sup h∈H</p><formula xml:id="formula_141" coords="47,72.00,223.52,468.00,29.52">1 t t i=1 Z i h for all t ≥ 1. By similar reasoning, we have that h T −1 ∈ H 0 T −1 = H0 T −1 satisfies R SPO+ (h T −1 ) ≤ r T −1 .</formula><p xml:id="_rErTDkY">Let &gt; 0 be fixed. Now, by Assumption 1 and Proposition 3, there exists h</p><formula xml:id="formula_142" coords="47,72.00,242.59,467.50,52.14">* 0 ∈ H * SPO+ such that h(x T ) − h * 0 (x T ) ≤ φ(R SPO+ (h)) + ≤ φ(r T −1 ) + ≤ τ (1 − ) τ (1 − ) + 2 b T −1 + ,</formula><p xml:id="_ZxBWnJY">and there exists h</p><formula xml:id="formula_143" coords="47,71.64,302.48,469.74,151.00">* 1 ∈ H * SPO+ such that h T −1 (x T ) − h * 1 (x T ) ≤ φ(R SPO+ (h T −1 )) + ≤ φ(r T −1 ) + ≤ τ (1 − ) τ (1 − ) + 2 b T −1 + , where we have used b T −1 = (1 + 2 τ (1−ρ) )φ(r T −1 ) in both inequalities above. Since both h * 0 , h * 1 ∈ H * SPO+ , according to the proof of Proposition 3, we have that w * (2h * 0 (x T ) − c T ) = w * (c T ) = w * (2h * 1 (x T ) − c T ) with probability 1. By the rejection criterion, ν S (h T −1 (x T )) ≥ b T −1 , and the 1-Lipschitzness of ν S , we have ν S (h * 1 (x T )) = ν S (h * 1 (x T ) − h T −1 (x T ) + h T −1 (x T )) ≥ ν S (h T −1 (x T )) − h * 1 (x T ) − h T −1 (x T ) − ≥ b T −1 − τ (1 − ) τ (1 − ) + 2 b T −1 − = 2 τ (1 − ) + 2 b T −1 − .</formula><p xml:id="_WT9DWqW">By the second part of Assumption 3, we have that</p><formula xml:id="formula_144" coords="47,132.78,485.50,346.45,25.18">ν S (h * 0 (x T )) ≥ τ sup h ∈H * SPO+ {ν S (h (x T ))} ≥ τ ν S (h * 1 (x T )) ≥ 2τ τ (1 − ) + 2 b T −1 − τ.</formula><p xml:id="_p9vkr7y">By viewing 2h(x T ) − c T and 2h * 0 (x T ) − c T as c 1 and c 2 in Lemma 1, we have</p><formula xml:id="formula_145" coords="47,139.61,540.42,337.76,22.31">(2h(x T ) − c T ) − (2h * 0 (x T ) − c T ) = 2 h(x T ) − h * 0 (x T ) ≤ 2τ (1 − ) τ (1 − ) + 2 b T −1 + 2 .</formula><p xml:id="_dkN6rAg">By the 1-Lipschitzness of ν S and the first part of Assumption 3, we have</p><formula xml:id="formula_146" coords="47,126.16,598.17,359.69,38.13">ν S (2h * 0 (x T ) − c T ) ≥ ν S (h * 0 (x T )) − h * 0 (x T ) − c T ≥ (1 − )ν S (h * 0 (x T )) ≥ (1 − ) 2τ τ (1 − ) + 2 b T −1 − τ = 2τ (1 − ) τ (1 − ) + 2 b T −1 − (1 − ) τ.</formula><p xml:id="_J4p4JU9">By taking → 0 and considering an appropriate convergent subsequence in the compact set H * SPO+ , the two inequalities above are satisfied for some h * 0 ∈ H * SPO+ with = 0. In particular, this implies that the conditions in Lemma 1 are satisfied and we have that w * (2h(x T ) − c T ) = w * (2 h * 0 (x T ) − c T ) = w * (c T ) with probability 1. Hence, we have shown that SPO+ (h(x T ), c T ) = 0 with probability 1, and so we have proven that H 0 T ⊆ H0 T .</p><p xml:id="_DgHQJfk">Proof of Theorem 3 We provide the proof of part (a), as the proofs of parts (b), (c), and (d) are completely analogous to Theorem 1.</p><p xml:id="_Svmb4Fd">Let us now prove part (a). When T = 0, part (a) holds by the definition of r 0 ≥ ω ( Ĉ, C). Otherwise, let T ≥ 1 be given. For any t ∈ {1, . . . , T }, recall that the re-weighted loss function at iteration t is in this case given by rew (h; z</p><formula xml:id="formula_147" coords="48,143.50,150.47,198.86,11.32">t ) := d M t (h(x t ), c t ) + (1 − d M t )(q t /p) (h(x t ), c t )</formula><p xml:id="_Aah9hKe">. Since p &gt; 0, and q t is a random variable that independent of x t , c t , and d M t , we condition on the two possible values of q t ∈ {0, 1} and obtain the following decomposition:</p><formula xml:id="formula_148" coords="48,150.59,211.77,310.82,31.70">E[ rew (h; z t )|F t−1 ] = E[ (h(x t ), c t )d M t |F t−1 ] + E[ (h(x t ), c t )(1 − d M t )|F t−1 ] = E[ (h(x t ), c t )|F t−1 ] = E[ (h(x t ), c t )] = R (h),</formula><p xml:id="_qFG5dSa">where we have also used that (x t , c t ) is independent of F t−1 . In other words, the conditional expectation of re-weighted surrogate loss at iteration t equals the surrogate risk. Consider the above applied to both h ∈ H and h * ∈ H * and averaged over t ∈ {1, . . . , T } to yield:</p><formula xml:id="formula_149" coords="48,164.61,315.21,375.38,29.15">R (h) − R (h * ) = 1 T T t=1 (E[ rew (h; z t )|F t−1 ] − E[ rew (h * ; z t )|F t−1 ])<label>(18)</label></formula><p xml:id="_wjuFV9a">As before, we denote the discrepancy between the expectation and the true excess re-weighted loss of predictor h at time t by Z t h , i.e., Z t</p><formula xml:id="formula_150" coords="48,72.00,370.09,468.27,47.42">h := E[ rew (h; z t ) − rew (h * ; z t )|F t−1 ] − ( rew (h; z t ) − rew (h * ; z t )). Recall that the empirical re-weighted loss in Algorithm 1 is ˆ T (h) = 1 T (x,c)∈W T (h(x), c) + 1 p (x,c)∈ WT (h(x), c) = 1 T T t=1</formula><p xml:id="_SrtMWmf">rew (h; z t ). Thus, (18) is equivalently written as:</p><formula xml:id="formula_151" coords="48,172.98,426.45,367.03,29.15">R (h) − R (h * ) = 1 T T t=1 Z t h + 1 T T t=1 ( rew (h; z t ) − rew (h * ; z t )) ,<label>(19)</label></formula><p xml:id="_22YmPaF">for any h ∈ H. To bound the term 1 T T t=1 Z t h , we apply Proposition 1 twice to both h and h * , with α ← ω ( Ĉ,C) with probability at least 1 − δ 2T 2 . Since h T is the minimizer of the empirical re-weighted loss ˆ T (h) over H, we have that rew (h T ; z t ) − rew (h * ; z t ) ≤ 0 in (19) and we obtain that R (h) − R (h * ) ≤ r T with probability at least 1 − δ 2T 2 . Finally, applying the union bound over all T ≥ 1, we obtain that R (h) − R (h * ) ≤ r T simultaneously for all T ≥ 1 with probability at least 1 − δ, which is the result of part (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Sh6XYdU">B.3. Proofs for Section 5</head><p xml:id="_4RTNXCw">Proof of Proposition 4 Since φ( ) = C φ √ , and C φ ∈ (0, 1 36L 2 ), we set C = r 1 5L . We use induction to prove that b T ≤ C/T −1/4 , for all T . For simplicity, we ignore the log term when analyzing the order, and assume that r t ≤ r 1 √ t . We assume b t ≤ C/t −1/4 , for 1 ≤ t ≤ T − 1.</p><p xml:id="_hjVGGAc">Then, since b t = 2φ(2r t + 2L t t−1 i=0 b 2 i ), we have that when t = T ,</p><formula xml:id="formula_152" coords="49,246.93,105.58,116.68,141.49">b t = 2C φ r t + 2L t t−1 i=0 b 2 i ≤ 2C φ r 1 √ t + 2L t t−1 i=0 b 2 i ≤ 2C φ r 1 √ t + 2L t t−1 i=0 C2 √ i ≤ 2C φ r 1 √ t + 4L t C√ t.</formula><p xml:id="_JxAZkqV">The first inequality is by r t ≤ r 1 √ t . The last inequality is from the fact that</p><formula xml:id="formula_153" coords="49,398.25,252.41,84.30,19.50">1 t t−1 i=0 i −1/2 ≤ 2 √ t.</formula><p xml:id="_CdGRxjQ">Then, we plug in the value of C, we have that b t ≤ C/t −1/4 , when t = T . Thus, R SPO+ (h T ) − R * SPO+ ≤ Õ(T −1/4 ). Consequently, for the polyhedral case, R SPO (h T ) − R * SPO ≤ 2Ψ(2b T )ω S (C) ≤ Õ(T −κ/4 ). For the strongly-convex feasible region, we set γ T = b, and then we can obtain the same order Õ(T −κ/4 ) for the excess SPO risk.</p><p xml:id="_68gMHYf">Next, we consider the bound for the label complexity. We set δ as a very small number, for example, δ ≤ Õ(1/T 3 ), so we can ignore the last term in the label complexity in part (d). Then, we have that 2 ) , which is larger than Õ(T −1/2 ). Moreover, the dependence on p comes from the bound on the re-weighted loss, since the re-weighted loss is upper-bounded by ω ( Ĉ,C) p . When T → ∞, p → 0, and thus, the re-weighted loss tends to infinity.</p><formula xml:id="formula_154" coords="49,72.00,380.50,51.46,11.98">E[n t ] ≤ Õ(2</formula><p xml:id="_Ss5DWzx">Given the output predictor h T at iteration T , recall that Z t h T := E[ rew (h T ; z t ) − rew (h * ; z t )|F t−1 ] − ( rew (h T ; z t ) − rew (h * ; z t )). Since E[Z t</p><p xml:id="_45CA8tC">h T ] = 0, we have that T t=1 Z t h T is a martingale. Thus, if we can further remove the dependence on p and show that Z t h T is finite for all T ≥ 0, then we can apply the Azuma's Inequality and achieve the convergence rate Õ(T −1/2 ) for 1 </p><formula xml:id="formula_155" coords="50,111.40,243.99,428.60,23.13">| (h T (x), c) − (h * (x), c)| ≤ L κ h T (x) − h * (x) ≤ L κ b T ≤ L κ K 1 Ψ(b T ) ≤ L κ K 1 b κ T ≤ Õ(T − κ 2(κ+2) ),<label>(20)</label></formula><p xml:id="_wChD2Fk">for all x ∈ X . Recall that rew (h; z t ) := d M t (h(x t ), c t ) + (1 − d M t ) q t p (h(x t ), c t ). Since when d M t = 1, rew (h; z t ) is obviously upper bounded by ω S ( Ĉ, C), and thus Z t h T is obviously bounded. Therefore, to show Z t h T is bounded, it suffices to consider the case when d M t = 0. When d M t = 0, we have that rew (h; z t ) = q t p (h(x t ), c t ) and</p><formula xml:id="formula_156" coords="50,120.89,323.18,371.41,22.31">q t p | (h T (x), c) − (h * (x), c)| ≤ 1 p | (h T (x), c) − (h * (x), c)| ≤ T κ 2(κ+2) Õ(T − κ 2(κ+2) ) = Õ(1).</formula><p xml:id="_2pQBttm">The above implies that there exists h * ∈ H * , such that rew (h; z t ) − rew (h * ; z t ) is bounded by a ln(T ) term.</p><p xml:id="_6Pq35b8">We can set p ← T − κ 2(κ+2) (ln(T ))</p><p xml:id="_MTarKfu">α 1 to avoid the log term in the order, for some α 1 &gt; 0 and reduce the right hand side to a constant that is independent of T and p. Therefore, Z t h T is also bounded when d M t = 0. We denote the upper bound of Z t h T by C 1 &gt; 0. Thus, when applying the Azuma's inequality to the sequence T t=1 Z t h T , and taking the average, we can remove the dependence on p and have that 1 T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_8MKJA8x">T t=1</head><p xml:id="_NKXDJZn">Z t h T ≤ , with probability at least 1 − 2e</p><formula xml:id="formula_157" coords="50,81.96,470.84,250.79,65.38">− 2 T 2C 1 . Recall that R (h T ) − R (h * ) = 1 T T t=1 Z t h T + 1 T T t=1</formula><p xml:id="_AG462yH">( rew (h T ; z t ) − rew (h * ; z t )) .</p><p xml:id="_DJgHK36">Since rew (h T ; z t ) − rew (h * ; z t ) ≤ 0, we conclude that R (h T ) − R (h * ) is at most . Setting 2e</p><formula xml:id="formula_158" coords="50,72.00,541.52,468.00,32.25">− 2 T 2C 1 = δ, we obtain that R (h T ) − R (h * ) ≤ 2 2C 1 ln(2/δ)</formula><p xml:id="_y94ryAd">T with probability at least 1 − δ. Thus, we conclude that R (h T ) − R (h * ) converges to zero at rate Õ( ln(T )/T ) when the condition in Proposition 2 holds.</p><p xml:id="_Hf5a6DQ">Finally, having proved Proposition 6, we remark that the Lipschitz assumption of function (•, c) in Proposition 6 can be relaxed to the following condition. Define X S h := {x ∈ X : ν S (h(x)) ≤ max h * ∈H * : h − h * ∞ }. Then, the Lipschitz assumption can be relaxed as follows. There exists a constant L κ &gt; 0, such that | (h 1 (x), c) − (h * (x), c)| ≤ L κ h 1 − h * ∞ , for all h 1 ∈ H, all h * ∈ H * , all x ∈ X S h 1 , and all c ∈ C. When x ∈ X S h T , Equation (20) still holds, so the re-weighted loss is finite. When x ∈ X S h T , p is one. | (h T (x), c) − (h * (x), c)| is obviously finite since C is a bounded set. Thus, we can conclude the same results under the relaxed Lipschitz conditions. This assumption only assumes the Lipschitz property in the region far away from the degeneracy. We also change the value of the minimum label probability p in the soft rejection to see its impact on the performance. Figure <ref type="figure" coords="52,163.53,290.71,5.00,8.74" target="#fig_5">6</ref> shows the percentage of labeled samples in the first 30 samples, and the excess SPO risk when the number of labeled samples is 10.</p><p xml:id="_36aFc7c">Figure <ref type="figure" coords="52,114.10,325.32,5.08,8.74" target="#fig_5">6</ref> shows that the minimum label probability p has no significant impact on the excess SPO risk.</p><p xml:id="_ngCSkZj">Intuitively, when p is larger, the percentage of labeled samples is larger. In practice, we can set p as a very small positive number that is close to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_HY2rKc7">C.2. Additional Results of Numerical Experiments.</head><p xml:id="_Q86rXhA">To assess the performance of our active learning algorithm under different noise levels, we change the variance of features and the noise level of labels when generating the data and demonstrate the results in Figures <ref type="figure" coords="52,535.46,418.29,5.04,8.74;52,72.00,435.59,23.24,8.74" target="#fig_22">7   and 8</ref>.   Recall that the cost vector is gerenated according to c j = 1 + (1 + b T j x i / √ p) deg j . Next, we further show the result when changing the degree of the model. When the degree is not one, the true model is not contained in our hypothesis class. The results in Figure <ref type="figure" coords="53,271.38,422.83,4.97,8.74" target="#fig_23">9</ref> show that when the model has a higher degree, the training process has a higher excess SPO risk at the beginning of the process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_5MTY9u8">C.3. Data Generation for Personalized Pricing</head><p xml:id="_6MFJgJU">In this section, we provide the parameter values for generating synthetic data in the personalized pricing experiment. Given a coefficient vector B j ∈ R 5 and A j ∈ R 5 , the demand function for item j is generated as d j (p i ) = e B T j X+A T j Xp i . Here, is a noise term drawn from a uniform distribution on [1 − ¯ , 1 + ¯ ]. We set ¯ = 0.1. A T j X can be viewed as the price elasticity. The customer feature vector is drawn from a mixed Gaussian distribution with seven different centers µ k . The value of these centers µ k , k = 1, 2, ..., 7 and the value of A j and B j , j = 1, 2, 3 are carefully chosen so that h * (X) is not a degenerate cost vector for any µ k , k = 1, 2, ..., 7. Please find the value of these parameters at the end of this appendix. The variance of the feature for each Gaussian distribution is set as 0.01 2 , which is on the same scale as the features.</p><p xml:id="_K5ZVUSZ">We further have the following monotone constraints for the prices of these three items. Let the decision variable w i,j indicate whether price i is selected for item j. Then, the constraints are as follows.</p><p xml:id="_ma9g9KK">w 1,j + w 2,j + w 3,j ≤ 1, j = 1, 2, 3 (20a)</p><p xml:id="_8zKtHct">w 2,1 ≤ w 2,2 + w 3,2 (20b)</p><formula xml:id="formula_159" coords="54,277.23,654.42,262.77,9.56">w 3,1 ≤ w 3,2<label>(20c)</label></formula><p xml:id="_rsrwSUR">w 2,2 ≤ w 2,3 + w 3,3 (20d)</p><formula xml:id="formula_160" coords="54,277.23,693.66,262.77,9.56">w 3,2 ≤ w 3,3<label>(20e)</label></formula><p xml:id="_4UjEkkV">w i,j ∈ {0, 1}, i, j = 1, 2, 3 (20a) requires each item can only select one price point. ( <ref type="formula" coords="55,323.51,81.11,13.95,8.74" target="#formula_155">20b</ref>) and (20c) require that the price of item 2 be no less than the price of item 1. ( <ref type="formula" coords="55,217.72,98.29,13.95,8.74" target="#formula_155">20d</ref>) and (20e) require that the price of item 3 be no less than the price of item 2.</p><p xml:id="_uuKUAv8">Since the purchase probability is d j (p i ) = exp(B T j X + A T j Xp i ), we need to specify the following parameters for generating the purchase probability given the feature X: A j and B j , j = 1, 2, 3. The feature vector is from a mixed Gaussian distribution with seven centers. The optimal prices of three items for these seven centers are ($60, $60, $60), ($60, $80, $90), ($90, $90, $90), ($80, $80, $80), ($60, $60, $80), ($80, $90, $90), and ($60, $60, $90) respectively. To generate such centers, we consider the following values for X, A j and B j . Then, we set</p><formula xml:id="formula_161" coords="55,164.51,253.70,278.82,66.02">A 1 =        0 0 0 1 0 0        , A 2 =        0 0 0 0 1 0        , A 3 =        0 0 0 0 0 1        , B 1 =        1 0 0 0 0 0        , B 2 =        0 1 0 0 0 0        , B 3 =        0 0 1 0 0 0       </formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="12,134.00,476.67,33.54,7.96;12,183.48,476.67,294.53,7.96;12,129.94,313.59,118.12,141.74"><head></head><label></label><figDesc xml:id="_r5ke4RJ">Figure 1Active learning reduces the label complexity, given the same prediction.</figDesc><graphic coords="12,129.94,313.59,118.12,141.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="12,479.84,659.99,61.68,9.61;12,72.00,678.68,468.00,9.61;12,72.00,697.36,468.00,9.61;12,72.00,716.04,468.00,9.61;13,72.13,227.00,467.85,9.61"><head></head><label></label><figDesc xml:id="_mdjauqr">Furthermore, confidence is related to both the size of the confidence region (which is often dictated by the number of labeled samples we have acquired) and the location of the prediction relative to the structure of S. El Balghiti et al. (2022) introduced the notion of "distance to degeneracy," which (a) Prediction is close to degenerate cost vectors(b) Prediction is far from degenerate cost vectors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="13,93.26,241.01,33.54,7.96;13,142.74,241.01,376.00,7.96;13,124.74,77.95,128.51,141.73"><head>Figure 2</head><label>2</label><figDesc xml:id="_DMfKcMH">Figure 2The SPO loss function reduces the label complexity, given the same size confidence region.</figDesc><graphic coords="13,124.74,77.95,128.51,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="13,72.00,310.45,468.00,9.61;13,72.00,329.61,468.00,9.61;13,72.00,348.76,468.00,9.61;13,72.00,367.91,469.52,9.61;13,71.61,387.06,468.40,9.61;13,72.00,406.21,468.00,9.61;13,72.00,425.36,468.00,9.61;13,72.00,444.51,468.00,9.61;13,72.00,463.66,469.29,9.61;13,72.00,482.81,468.00,9.61;13,72.00,501.97,61.50,9.61;13,82.96,521.12,457.33,10.22;13,71.72,540.27,105.12,9.61"><head></head><label></label><figDesc xml:id="_aphgf2M">prediction. In fact, El Balghiti et al. (2022) argue that the distance to degeneracy provides a notion of confidence associated with a prediction that generalizes the notion of "margin" in binary and multiclass classification problems. El Balghiti et al. (2022) use the distance to degeneracy to provide tighter generalization guarantees for the SPO loss and its associated margin loss. In our context, we adopt the distance to degeneracy in order to determine whether or not to acquire labels. It is motivated by our intuition from the previously discussed examples wherein the labels of samples should be more informative if their predicted cost vectors are closer to degeneracy. In turn, we develop a generalization of margin-based active learning algorithms that utilize the distance to degeneracy as a confidence measure to determine those samples whose labels should (or should not) be acquired. Definition 1 reviews the notion of distance to degeneracy as defined by El Balghiti et al. (2022). Definition 1. (Distance to Degeneracy, El Balghiti et al. (2022)). The set of degenerate cost vector predictions is C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="17,475.57,486.30,64.83,9.61;17,72.00,505.21,468.00,9.61;17,72.00,524.13,468.40,9.61;17,72.00,543.04,467.99,9.61;17,72.00,561.95,140.89,9.61;17,212.89,566.30,4.89,6.12;17,218.66,561.95,10.66,9.61;17,229.32,560.71,4.08,6.12;17,233.90,561.95,306.10,9.61;17,72.00,580.86,208.12,9.61;17,82.96,599.77,419.58,10.47;17,505.16,599.77,34.84,9.61"><head></head><label></label><figDesc xml:id="_SWKpzAx">This property enables us to analyze the performance of MBAL-SPO under SPO+ and surrogate loss function respectively in the next two sections. Assumption 1 is related to the uniform calibration property studied in Ho-Nguyen and Kılınç-Karzan (2022) in the SPO context. Next, to measure how the density of the distribution ν S (h * (x)) is allocated near the points of degeneracy, we define the near-degeneracy function Ψ in Definition 2. Definition 2 (Near-degeneracy function). The near-degeneracy function Ψ : R + → [0, 1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="29,83.20,80.43,386.07,10.22;29,469.27,84.78,4.79,6.12;29,476.68,80.43,63.29,9.61;29,71.18,99.51,161.95,9.61;29,233.13,103.86,4.79,6.12;29,240.24,99.54,3.03,9.57;29,245.09,91.67,9.09,9.57;29,258.63,99.51,281.37,9.61;29,71.42,118.59,356.40,9.61;29,429.34,117.35,18.25,6.12;29,448.09,118.59,34.45,9.61;29,482.54,117.35,57.46,10.85;29,72.00,137.67,7.01,9.61;29,79.01,142.02,3.97,6.12;29,85.66,137.67,455.86,9.61;29,72.00,156.75,22.73,9.61;29,94.73,161.10,4.71,6.12;29,101.01,156.75,24.70,9.61;29,125.79,155.51,4.08,6.12;29,132.50,154.01,231.23,12.35;29,82.96,182.26,457.04,9.61;29,72.00,201.34,97.49,9.61;29,169.49,205.70,3.97,6.12;29,173.96,201.34,66.45,9.61;29,240.41,205.70,3.97,6.12;29,244.88,201.34,295.12,9.61"><head>Proposition 6 .</head><label>6</label><figDesc xml:id="_r2kUksF">Suppose that Assumption 5 holds and there exists a constant C φ &gt; 0 such that Assumption 1 holds with φ( ) = C φ • √ . Suppose Assumption 4 holds with κ = 1. Suppose that the surrogate loss function (•, c) is Lipschitz for any given c ∈ C. Let p ← T −1/6 (ln(T )) α 1 for some α 1 &gt; 0. For a fixed δ ∈ (0, 1], consider Algorithm 1 under the same settings as Theorem 3. Then, R (h T ) − R * ≤ Õ( ln(1/δ)/T ) with probability at least 1 − δ. In Proposition 6, Assumptions 4 and 5 mean that the near-degeneracy function Ψ(b) is in the order of Θ(b), i.e., K 1 b ≤ Ψ(b) ≤ b/b 0 . Proposition 6 implies that the excess surrogate risk converges</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="30,82.96,504.17,457.05,9.61;30,72.00,523.43,468.00,9.61;30,72.00,542.69,467.99,9.61;30,72.00,561.95,468.00,9.61;30,72.00,581.21,468.00,9.61;30,72.00,600.48,468.00,9.61;30,72.00,619.74,409.53,9.61"><head>Figure 3</head><label>3</label><figDesc xml:id="_Wc7Zj4z">Figure3shows our results for this experiment. Excess SPO risks during the training process for MBAL-SPO and supervised learning are shown in the left plot of Fig.3. The x-axis shows the number of labeled samples and the y-axis shows the log-scaled excess SPO risk on the test set. The results are from 25 trials, and the error bar in Figure3is an 85% confidence interval. We observe that as more samples are labeled, the margin-based algorithm performs better than supervised learning, as expected. Compared to supervised learning, the margin-based algorithm achieves a significantly lower excess SPO risk when the number of labeled samples is around 25.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="31,125.46,257.57,33.54,7.96;31,174.95,257.57,311.59,7.96;31,72.00,272.25,468.00,9.61;31,72.00,291.25,291.44,9.61;31,82.96,310.25,382.49,9.61;31,465.45,314.61,3.01,6.12;31,472.61,310.25,26.28,9.61;31,498.90,314.61,3.01,6.12;31,502.40,310.25,37.60,9.61;31,72.00,329.25,468.00,9.61;31,72.00,348.25,467.99,9.61;31,72.00,367.25,470.12,9.61;31,72.00,92.35,234.00,156.00"><head>Figure 3</head><label>3</label><figDesc xml:id="_GV2KsB7">Figure 3 Risk on the test set during the training process in 3 × 3 grid, and 5 × 5 grid.risk level, the margin-based algorithm has a much faster learning rate than supervised learning and can achieve a lower SPO risk even after 200 labeled samples.In Appendix C.1, we further examine the impacts of the scale of parameters, r t and b t , on the number of labels and the SPO risk during the training process, which demonstrates the robustness of the SPO risk with respect to the scales of these parameters. In Appendix C.2, we include more results in which we change the noise levels and variance of the features when generating the data.</figDesc><graphic coords="31,72.00,92.35,234.00,156.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="32,72.00,138.59,4.94,9.61;32,76.94,142.95,3.01,6.12;32,84.55,138.59,26.81,9.61;32,111.37,142.95,3.01,6.12;32,118.98,138.59,421.02,9.61;32,72.00,157.98,468.00,9.61;32,72.00,177.37,468.29,9.61;32,72.00,196.76,468.22,9.61;32,72.00,216.14,468.29,9.61;32,72.00,235.53,467.99,9.61;32,72.00,254.92,96.81,9.61"><head>r</head><label></label><figDesc xml:id="_6BWpR4e">t and b t are selected by the rules discussed at the end of Section 4.4. The excess SPO risks of MBAL-SPO and supervised learning on the test set as the number of acquired labels increases are shown in Figure 4. The results are from 25 simulations, and the error bars in Figure 4 represent an 85% confidence interval. Notice that the demand function is in an exponential form but our hypothesis class is linear, so the hypothesis class is misspecified. The results in Figure 4 show that MBAL-SPO achieves a smaller excess SPO risk than supervised learning even when the hypothesis class is misspecified.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="32,136.96,483.49,33.54,7.96;32,186.44,483.49,288.60,7.96;32,142.20,287.06,327.61,187.20"><head>Figure 4</head><label>4</label><figDesc xml:id="_BTPekKJ">Figure 4Excess test set risk during the training process in personalized pricing.</figDesc><graphic coords="32,142.20,287.06,327.61,187.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="32,72.00,696.65,468.00,9.61;32,72.00,716.04,468.00,9.61;33,72.00,80.43,468.00,9.61;33,72.00,99.37,240.33,9.61;37,72.00,81.11,468.20,8.74;37,71.17,98.48,59.13,8.74;37,130.30,102.70,3.84,5.24;37,134.64,98.48,164.54,9.29;37,299.19,97.59,13.55,5.24;37,316.59,95.85,223.77,11.92"><head></head><label></label><figDesc xml:id="_ntVyDbR">to design active learning algorithms in situations where we can minimize the SPO loss function approximately. While our work focuses on stream-based active learning in the predict-then-optimize framework, it is also worthwhile to consider pool-based active learning, where all feature vectors are revealed at once before training, in the future. abuse of notation, we keep the notation the same after the change of basis, for example, now the vector ∆ equals κ • e d . Rewrite c as c = (c , ξ), where c ∈ R d−1 and ξ ∈ R. Define c := E[c ] and ξ = E[ξ]. Then by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="41,535.57,692.27,4.43,8.74;42,81.96,81.08,458.04,8.77;42,72.00,95.07,359.22,12.07;42,431.22,102.75,18.68,6.20"><head>)</head><label></label><figDesc xml:id="_9HCbKwn">Proof of Lemma 4The proof is by strong induction. For the base case of t = 1, part (a) follows since H 0 = H and part (b) follows since b 0 ≥ r 0 /η ≥ ω ( Ĉ, C)/η, and thus sup h∈H 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="43,391.08,655.54,9.69,8.74;43,400.77,654.64,3.82,5.24;43,405.09,655.54,8.84,8.74;43,413.93,659.75,2.85,5.24;43,417.28,655.54,122.72,8.74;43,72.00,672.65,345.91,9.56;43,423.44,672.19,9.53,4.98;43,435.13,672.65,62.01,8.74;43,498.35,668.85,32.62,7.15;43,498.33,677.64,32.64,6.98;43,538.62,672.65,2.73,8.74;43,255.56,704.27,15.00,8.74;43,255.23,714.91,15.65,5.24;43,285.92,697.53,4.98,8.74;43,284.81,711.10,5.82,8.74;43,299.12,694.54,4.37,5.24;43,295.36,718.45,12.86,5.24;43,310.37,704.27,6.80,8.74;43,317.88,703.31,3.18,4.98;43,317.17,709.45,4.34,5.24;43,335.29,704.27,21.48,8.74;44,72.00,85.76,69.78,8.74;44,144.91,82.14,25.29,6.98;44,154.89,91.92,4.37,5.24;44,174.71,83.25,67.75,11.26;44,253.63,80.19,117.66,8.74;44,309.79,91.92,4.37,5.24;44,374.14,85.76,7.59,8.74;44,384.58,82.14,29.25,6.98;44,396.54,91.92,4.37,5.24;44,416.96,85.76,14.03,8.74;44,430.99,89.98,4.37,5.24;44,436.83,85.76,103.17,8.74;44,72.00,102.58,174.98,8.74;44,293.95,99.56,6.98,7.05"><head></head><label></label><figDesc xml:id="_HTRnMPA">(h * ; z t )} and their differences, we have the following bound for any &gt; 2α &gt; 0 with probability at least 1− 2N 1 (α, rew • H, T ) exp − T ( −2α) 2 2ω ( Ĉ,C) 2 : Considering α = ω ( Ĉ,C) T and = ω ( Ĉ, C) 4 ln(2T N 1 (ω ( Ĉ,C)/T, rew •H,T )/δ) T + 2ω ( Ĉ,C) T = r T /2 yields that the above bound holds with probability at least 1 − δ 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="44,473.66,279.99,4.37,5.24;44,495.79,274.80,45.60,8.74;44,72.00,291.62,236.41,8.74;44,308.42,295.83,4.37,5.24;44,316.19,291.62,224.08,8.74;44,72.00,308.43,18.18,8.74;44,90.18,308.43,20.53,9.46;44,110.71,312.64,4.37,5.24;44,116.55,308.43,33.60,8.74;44,150.15,308.43,20.53,9.46;44,170.69,312.64,4.37,5.24;44,176.52,308.43,364.86,8.74"><head>T</head><label></label><figDesc xml:id="_cCxrDZb">= 0. Thus, part (b) follows immediately. Otherwise, assume that b T &gt; 0. Recall that, by Assumption 1.(1), we have that Dist H * (h T ) ≤ Dist H * (h T ). Then, by combining Assumption 1.(2) with part (a), with probability at least 1 − δ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="45,523.63,506.52,16.37,8.74;45,72.00,523.20,36.55,8.74;45,108.82,522.31,3.82,5.24;45,113.14,520.57,69.97,11.37;45,183.38,522.31,3.82,5.24;45,187.70,520.57,47.83,11.37;45,235.80,522.31,3.82,5.24;45,240.11,523.20,300.16,8.74;45,72.00,537.25,349.32,12.18;45,81.96,573.20,458.41,8.77;45,72.00,589.91,108.01,8.74"><head></head><label></label><figDesc xml:id="_wMYyVtG">and hence w * (2 h(x) − c) = w * ( h(x)) = w * (c), with probability one over (x, c) ∼ D. Therefore, we have that R SPO+ ( h) = E (x,c)∼D [ SPO+ ( h(x), c)] = 0 by the above expression for SPO+ (ĉ, c). Proof of Theorem 2 We provide the proof of part (a), as the proofs of parts (b) and (c) are completely analogous to Theorem 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15" coords="46,157.75,322.20,4.37,5.24;46,177.79,313.52,69.50,11.26;46,258.45,311.88,89.81,8.01;46,300.69,322.20,4.37,5.24;46,351.25,316.04,7.90,8.74;46,362.15,312.42,29.25,6.98;46,374.10,322.20,4.37,5.24;46,394.67,316.04,14.47,8.74;46,409.14,320.26,4.37,5.24;46,418.51,316.04,123.16,8.74;46,72.00,332.91,209.75,8.74;46,281.75,337.12,4.37,5.24;46,289.52,332.91,24.69,8.74;46,314.21,338.06,15.65,5.24;46,342.76,331.37,3.96,5.68;46,342.07,339.06,4.37,5.24;46,360.50,330.62,4.37,5.24;46,360.50,338.62,12.86,5.24;46,375.25,332.91,6.80,8.74;46,382.76,332.44,3.18,4.98;46,382.05,338.09,4.34,5.24;46,399.27,332.91,140.73,8.74;46,72.00,349.77,110.05,8.74"><head>T</head><label></label><figDesc xml:id="_Wr3qTas">and = ω ( Ĉ, C) 4 ln(2T N1 (α, SPO+ •H)/δ) T + 2ω ( Ĉ,C) T = r T and following the same reasoning as in the proof of Theorem 1 yields that r T ≥ sup h∈H</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16" coords="48,523.49,474.40,4.37,5.24;48,72.00,485.81,35.05,8.74;48,110.18,482.19,25.29,6.98;48,121.43,491.62,3.32,5.68;48,147.83,483.15,86.90,6.88;48,188.60,491.96,4.37,5.24;48,237.58,485.81,302.42,8.74;48,72.00,503.37,41.24,8.74;48,248.59,522.04,15.00,8.74;48,248.27,532.68,15.65,5.24;48,270.93,515.30,4.98,8.74;48,269.82,528.87,5.82,8.74;48,284.13,512.31,4.37,5.24;48,280.37,536.22,12.86,5.24;48,295.38,522.04,6.80,8.74;48,302.89,521.08,3.18,4.98;48,302.18,527.23,4.34,5.24;48,316.98,522.04,38.15,8.74;48,355.13,526.26,4.37,5.24;48,360.97,522.04,2.77,8.74"><head>T</head><label></label><figDesc xml:id="_KFcAUzT">and ← ω ( Ĉ,C) p 4 ln(2T N 1 (α, rew •H,T )/δ) T + 2α. Then, by considering their differences using the union bound</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17" coords="49,135.36,380.73,4.37,5.24;49,135.36,388.73,12.86,5.24;49,150.10,383.02,21.21,8.74;49,171.32,387.23,2.85,5.24;49,174.66,383.02,58.60,8.74;49,233.26,387.23,2.85,5.24;49,239.04,380.50,47.41,11.26;49,286.95,383.02,69.20,8.74;49,370.72,380.73,4.37,5.24;49,370.72,388.73,12.86,5.24;49,385.46,383.02,21.21,8.74;49,406.67,387.23,2.85,5.24;49,410.02,380.50,57.75,11.26;49,468.27,383.02,71.73,8.74;49,72.00,400.52,320.89,8.74;49,81.96,417.99,458.04,8.77;49,72.00,435.52,92.96,9.46;49,165.46,435.52,49.37,8.74;49,185.99,464.46,27.43,8.74;49,219.32,454.73,4.37,5.24;49,215.56,478.64,12.86,5.24;49,230.57,464.46,20.88,8.74;49,251.45,468.68,2.85,5.24;49,254.80,464.46,37.12,8.74;49,298.11,454.73,4.37,5.24;49,294.35,478.64,12.86,5.24;49,309.36,464.46,32.36,8.74;49,351.65,457.73,4.98,8.74;49,352.48,471.30,4.17,8.74;49,367.81,464.46,44.38,8.74;49,260.61,497.74,31.31,8.74;49,298.11,488.01,4.37,5.24;49,294.35,511.92,12.86,5.24;49,309.36,497.74,14.95,8.74;49,334.51,491.00,4.98,8.74;49,335.34,504.57,4.17,8.74;49,349.23,486.81,4.12,4.37;49,349.60,491.74,3.39,4.35;49,356.43,497.74,45.76,8.74;49,402.19,495.45,12.11,5.24;49,260.61,528.40,48.79,11.26;49,319.61,524.18,4.98,8.74;49,320.44,537.75,4.17,8.74;49,334.33,519.99,4.12,4.37;49,334.70,524.92,3.39,4.35;49,341.53,530.92,33.23,8.74;49,374.76,529.18,21.83,5.68;49,406.36,530.92,2.77,8.74;49,71.64,559.21,468.36,8.74;49,72.00,576.71,162.32,8.74;49,235.70,572.73,19.38,6.91;49,242.65,576.71,167.03,9.46;49,410.18,574.19,73.77,11.26;49,485.33,572.73,23.33,7.00;49,496.24,576.71,43.96,8.74;49,72.00,594.21,25.11,8.74;49,81.96,611.71,55.01,8.74;49,136.97,615.93,4.37,5.24;49,144.74,609.19,32.44,11.26;49,167.63,613.16,5.98,5.24;49,173.61,617.89,9.18,5.68;49,183.98,608.31,49.44,12.13;49,225.41,611.71,314.59,8.74;49,72.00,629.21,208.66,8.74;49,280.66,633.43,4.37,5.24;49,286.50,625.24,58.38,12.71;49,332.11,629.21,29.75,8.74;49,81.96,646.69,458.04,8.77;49,74.72,661.70,34.14,8.74;49,109.35,664.22,59.75,8.74;49,169.11,668.43,4.37,5.24;49,176.95,661.70,32.67,11.26;49,200.58,670.40,13.67,5.68;49,216.41,664.22,97.46,8.74;49,315.25,660.24,19.72,6.91;49,322.20,664.22,57.04,8.74;49,379.24,668.43,4.37,5.24;49,387.08,661.70,31.12,11.26;49,419.58,660.18,3.87,8.74;49,433.30,660.24,4.12,4.37;49,424.65,660.26,32.96,9.41"><head>T</head><label></label><figDesc xml:id="_nydjWuf">t=1 Ψ(2b t )). Because b t ≤ Õ(T −κ/2 ), we have that T t=1 Ψ(2b t ) ≤ Õ(T 1−κ/2 ). Then, we can obtain the label complexity in Proposition 4 depending on the value of κ.Proof of Proposition 5We first consider the label complexity. By the part (d) in Theorem 3, the totallabel complexity E[n t ] is at most pT T ) 1−κ/4 .The first inequality is because of assumptions 6 and 4. The second inequality is because of the integration. To minimize the order of T , we set p = T− k 2(k+2) . Then, the label complexity E[n t ] is at most Õ T r T ≤ Õ( 1 √ T p ) = Õ(T − 1 κ+2), we obtain the risk bounds for the surrogate loss. Since φ is a square root function. The SPO risk is at most 2Ψ(2φ(r T )) ≤ Õ(T The reason why the excess surrogate risk in Proposition 5 is larger thanÕ(T −1/2 ) is because r T ≤ Õ( 1 p√ T ). Indeed, when p ← T − κ 2(κ+2) , then r T ≤ Õ T ( κ 2(κ+2) − 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18" coords="50,411.98,137.42,4.37,5.24;50,430.42,128.98,4.37,5.24;50,430.42,136.97,12.86,5.24;50,445.16,131.26,6.80,8.74;50,452.67,130.80,3.18,4.98;50,451.96,136.45,8.48,6.39;50,462.31,131.26,2.77,8.74;50,81.96,147.98,78.57,8.74;50,231.78,170.34,27.24,9.30;50,241.74,178.00,23.48,6.28;50,265.72,170.34,9.90,8.74;50,275.62,174.55,4.52,5.24;50,280.96,170.34,9.62,8.74;50,290.58,168.95,3.82,5.24;50,294.89,170.34,38.20,8.74;50,333.09,174.55,4.37,5.24;50,338.93,170.34,32.69,9.56;50,371.62,174.55,4.37,5.24;50,377.46,170.34,2.77,8.74;50,81.96,190.96,205.35,12.71;50,274.55,194.93,38.13,8.74;50,312.68,199.15,2.85,5.24;50,318.40,192.41,36.58,11.26;50,363.51,193.39,3.96,5.68;50,364.09,200.75,3.32,5.68;50,378.62,190.97,73.22,12.70;50,439.44,194.93,100.56,8.74;50,71.64,211.65,193.70,8.74;50,265.34,210.76,3.82,5.24;50,271.60,211.65,17.00,8.74;50,288.69,210.76,3.82,5.24;50,293.00,211.65,16.86,8.74;50,309.86,210.76,3.82,5.24;50,314.18,211.65,30.38,8.74;50,344.56,215.87,4.37,5.24;50,350.40,211.65,73.35,8.74;50,423.76,215.87,4.37,5.24;50,432.92,211.65,107.35,8.74;50,71.50,228.37,284.53,8.74"><head></head><label></label><figDesc xml:id="_GYzXMWS">h * ∈H * {ν S (h * (x))} ≤ b T ) ≥ K 1 b T .Recall that in Theorem 3, when p ≤ Õ(T ) ). In the proof of Theorem 3, we also have that there exists h * ∈ H * , h * (x) − h T (x) is at most b T with probability at least 1 − δ. By the Lipschitz property and Assumption 5, we have that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19" coords="52,196.59,257.57,33.54,7.96;52,246.08,257.57,169.34,7.96;52,72.00,92.35,234.00,156.00"><head>Figure 6</head><label>6</label><figDesc xml:id="_UZEKHVp">Figure 6Performance under different settings of p</figDesc><graphic coords="52,72.00,92.35,234.00,156.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20" coords="52,116.35,710.85,33.54,7.96;52,165.84,710.85,329.81,7.96;52,142.20,475.90,327.60,225.72"><head>Figure 7</head><label>7</label><figDesc xml:id="_7f9JSV3">Figure 7Excess SPO risk during the training process under different variance of features.</figDesc><graphic coords="52,142.20,475.90,327.60,225.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21" coords="53,133.61,322.25,33.54,7.96;53,183.10,322.25,295.29,7.96"><head>Figure 8</head><label>8</label><figDesc xml:id="_pRyy8TF">Figure 8Excess SPO risk during the training process under different noise levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22" coords="53,81.96,336.90,459.42,8.74;53,72.00,354.09,468.00,8.74;53,72.00,371.27,468.61,8.74;53,142.20,92.35,327.59,220.68"><head>Figures 7</head><label>7</label><figDesc xml:id="_yn7JTCP">Figures7 and 8show that when the variance of the features and the noise level of the labels are small, both active learning and supervised learning have close performance. When the variance of features or the noise level of labels is large, our proposed active learning methods perform better than supervised learning.</figDesc><graphic coords="53,142.20,92.35,327.59,220.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23" coords="54,133.61,416.83,33.54,7.96;54,183.10,416.83,295.29,7.96;54,72.00,92.35,468.00,315.27"><head>Figure 9</head><label>9</label><figDesc xml:id="_D6RRQ8U">Figure 9Excess SPO risk during the training process under different noise levels.</figDesc><graphic coords="54,72.00,92.35,468.00,315.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24" coords="55,72.00,218.59,469.93,9.56"><head>Define a 1</head><label>1</label><figDesc xml:id="_H53UAuD">= −0.0202733, b 1 = −1.19155, a 2 = −0.0133531, b 2 = −1.45748, a 3 = −0.00540672, b 3 = −1.22819.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="20,72.00,697.73,468.00,27.93"><head></head><label></label><figDesc xml:id="_gG8nYEB">Definition 4 (Strength Property for the Feasible Region S). The feasible region S satisfies the strength property with constant µ &gt; 0 if, for all w ∈ S and ĉ ∈ R d , it holds that ĉT</figDesc><table /><note xml:id="_rRv44Nh">The strength property, as defined in El Balghiti et al. (2022), is reviewed below in Definition 4.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="42,71.62,410.09,468.58,91.16"><head></head><label></label><figDesc xml:id="_zuHYuYD">and h t−1 is the corresponding minimizer over H t−2 hence ; z i ) = ˆ t−1, * . By assumption (a) of induction, we have that h * ∈ H t−2 . The above chain of inequalities shows that ˆ t−1 (h * ) ≤ ˆ t−1, * + 1 Next, we prove (b) for t. Let h ∈ H t−1 be fixed. By Assumption 1.(2) and since h * is the unique minimizer in H * , we have that</figDesc><table coords="42,71.62,424.28,468.38,43.99"><row><cell>1 t−1</cell><cell>t−1 i=1</cell><cell>rew (h t−1 t−1</cell><cell>t−1 i=1 ηb 2 i−1 + r t−1 , hence h  *  ∈ H t−1 by definition in Line 20 of</cell></row><row><cell cols="3">Algorithm 1.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="43,72.00,81.11,468.00,61.87"><head></head><label></label><figDesc xml:id="_VKSYgbB">t−1 ⊆ H i−1 for i ∈ {1, . . . , t − 1} and the assumption that r t ≥ sup h∈H ∈ {1, . . . , T }, and the equality follows by the definition of the reweighted loss function in Algorithm 1. By assumption we have that h ∈ H t−1 and by the proof of part (a), we have that h * ∈ H t−1 . Thus, since ˆ t−1, * = min h∈H t−2 ˆ t−1 (h) and H t−1 ⊆ H t−2 , we have that</figDesc><table coords="43,226.10,95.93,74.60,13.69"><row><cell>1 t</cell><cell>t i=1 Z i h</cell><cell>for t</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="43,71.50,619.02,468.77,45.97"><head></head><label></label><figDesc xml:id="_tUUyZVQ">For each T ≥ 1, we apply Proposition 1 and plug in both h, h * ∈ H. Indeed, by considering the twosequences {E[ rew (h; z t )|F t−1 ] − rew (h; z t )} and {E[ rew (h * ; z t )|F t−1 ] − rew</figDesc><table coords="43,71.50,619.02,468.77,28.14"><row><cell>It remains to show that r T ≥ sup h∈H</cell><cell>1 T</cell><cell>T t=1 Z t h</cell><cell>simultaneously for all T ≥ 1 with probability at least</cell></row><row><cell>1 − δ.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="46,70.83,206.83,469.36,61.62"><head></head><label></label><figDesc xml:id="_XyKJN3E">To control the probability that r t ≥ sup h∈H</figDesc><table coords="46,70.83,206.83,469.36,61.62"><row><cell>1 t</cell><cell>t i=1 Z i h</cell><cell>, we now consider</cell></row><row><cell cols="3">the i.i.d. covering number instead of the sequential covering number. As pointed out by Kuznetsov and Mohri</cell></row><row><cell cols="3">(2015), in the i.i.d. case, for T ≥ 1 and for any &gt; 2α &gt; 0, we have a similar convergence result as Proposition</cell></row><row><cell>1 as follows:</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Liu, Grigas, Liu, and Shen:  Active Learning in the Predict-then-Optimize Framework: A Margin-Based Approach</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_kBZg62A">Acknowledgments</head><p xml:id="_VkG389D">PG acknowledges the support of NSF AI Institute for Advances in Optimization Award 2112533.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_FKxK3Tu"><p xml:id="_X9JT8m9">Appendix A: Examples of φ and Ψ Functions and Upper Bound for η</p><p xml:id="_q3AtDPm">The existence of non-trivial φ and Ψ depends on the distribution D and the feasible region S. In this appendix, we examine the case where we use the SPO+ loss as the surrogate loss function given the norm • as the 2 norm. We first present two special cases, polyhedral and strongly convex feasible regions, for which we can characterize the function φ. We then present sufficient conditions on the distribution D so that we can ultimately bound the label complexity. For simplicity, we use P(c|x) to denote the probability density function of c conditional on x. To study the pointwise error as needed in Assumption 1, we make a recoverability assumption. Assumption 6 holds for linear hypothesis classes when the features have nonsingular covariance and for certain decision tree hypothesis classes when the density of features is bounded below by a positive constant <ref type="bibr" coords="36,112.19,237.46,67.80,8.74" target="#b25">(Hu et al. 2022)</ref>.</p><p xml:id="_uUJkQ8Y">Assumption 6 (Recoverability). There exists κ &gt; 0 such that for all h ∈ H, h * ∈ H * , and almost all</p><p xml:id="_ty7MyKZ">x ∈ X , it holds that</p><p xml:id="_FBZD5E2">Assumption 6 provides an upper bound of the pointwise error from the bound of the expected error. It implies that the order of pointwise error is no larger than the order of the expected error.</p><p xml:id="_m8zp5mg">Polyhedral feasible region. First, we consider the case where the feasible region S is a polyhe- dron. Let P cont, symm denote the class of joint distributions D such that P(•|x) is continuous on R d and is centrally symmetric with respect to its mean for all x ∈ X . Following Theorem 2 in Liu and Grigas Lemma 5 (Example of φ). Given • as the 2 norm, suppose that Assumption 6 holds and the feasible region S is a bounded polyhedron. Define</p><p xml:id="_Uvcphff">, for all x ∈ X . When the distribution D ∈ P M,α,β , then it holds that for almost all x ∈ X ,</p><p xml:id="_GyvV2N5">Proof of Lemma 5 For given x ∈ X , let c = E[c|x] and ∆ = h(x) − h * (x). According to Theorem 1 in <ref type="bibr" coords="36,72.00,599.10,132.65,8.74" target="#b15">Elmachtoub and Grigas (2022)</ref>, it holds that</p><p xml:id="_d4fjRVk">Without loss of generality, we assume d S &gt; 0. Otherwise, the constant Ξ S will be zero and the bound will be trivial.</p><p xml:id="_KD5Na7z">Define the function ι(κ <ref type="bibr" coords="36,413.65,682.53,11.16,8.74">∞)</ref>, where M &gt; 1 is a scaler which is larger than σ. Let κ = ∆ 2 and A ∈ R d×d be an orthogonal matrix such that A T ∆ = κ • e d for e d = (0, . . . , 0, 1) T . We implement a change of basis and let the new basis be A = (a 1 , . . . , a d ). With a slight</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_jg2TWrV">Appendix C: Details for Numerical Experiments</head><p xml:id="_bvqUab3">In this appendix, we provide the sensitivity analysis of the parameters and additional results of the numerical experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_5GVWSbj">C.1. Setting Parameters in the Algorithm</head><p xml:id="_FeSKBRs">Here we discuss how to set the values of the parameters r t and b t in margin-based algorithm in practice. In general, these values depend on D, the budget of the labeled samples, and the performance that we would like to achieve. Setting r t and b t , to be large numbers makes our active learning algorithm the same as supervised learning. Setting them as smaller numbers will make our algorithms less conservative and more sensitive to the first several samples.</p><p xml:id="_v3VsK5U">To further illustrate the impact of the scale of these values, we run the following experiments by changing the scales of these parameters. In the margin-based algorithm, we set the value of r t to [d × ln(t) + ln(1/δ)]/t, where t is the number of samples, d is the dimension of cost vector, which is 12, and δ is set as 10 −7 . According to Proposition 5, we set b t = slackness × √ r t , where slackness a parameter we will tune. The plot on the left of Figure <ref type="figure" coords="51,132.98,321.24,5.06,8.74">5</ref> shows the ratio of labeled samples to total samples in the first 30 samples as the slackness value varies. Wee see that the larger slackness is, the more samples are labeled. The right plot in Figure <ref type="figure" coords="51,535.19,338.91,5.06,8.74">5</ref> further shows the value of excess SPO risk as the value of slackness changes when the number of labeled samples is seven. It shows that the excess SPO risk is quite robust to the value of slackness. In other words, the value of slackness has little impact on the excess SPO risk given the same number of labeled samples, though the value of slackness affects the ratio of the labeled samples. In practice, to find the set the scale for b t and r t , we can refer to the rules discussed at the end of Section 4.4, where we set a "burn in" period of T iterations that acquires all labels during the first T iterations. Then, we can use the distribution of values {ν S (h T (x t ))} T t=1 to inform the value of b T . For example, if we want to reduce the number of labels by 50%, compared to supervised learning, we can set the scale of b T as the median of {ν S (h T (x t ))} T t=1 .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="33,72.00,195.41,467.72,8.74;33,94.74,212.62,228.53,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_VWSUZbG">Optnet: Differentiable optimization as a layer in neural networks</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Y8DAwNF">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,72.00,234.56,468.00,8.74;33,95.40,251.76,93.43,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_JzJPjEb">Agnostic active learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8H8geXS">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="89" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,72.00,273.70,469.07,8.74;33,95.15,290.90,181.62,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_YE4bWAN">Margin based active learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_CNcTNkw">International Conference on Computational Learning Theory</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="35" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,72.00,312.84,468.00,8.74;33,95.91,330.04,336.61,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_76MdknN">Learning with differentiable pertubed optimizers</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Berthet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Teboul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Vert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cTmHAXK">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9508" to="9519" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,72.00,351.98,469.93,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_xK6Cukn">From predictive to prescriptive analytics</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kallus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EfDcZNa">Management Science</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1025" to="1044" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,72.00,373.92,468.00,8.74;33,95.15,391.12,263.53,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_z2pezRe">Importance weighted active learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_xTM4rq8">Proceedings of the 26th annual international conference on machine learning</title>
				<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,72.00,413.06,469.93,8.74;33,95.37,430.26,338.09,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_zrWqzX2">Batch mode active learning for regression with expected model change</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cxbpghg">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1668" to="1681" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,72.00,452.20,469.93,8.74" xml:id="b7">
	<monogr>
		<title level="m" type="main" xml:id="_YeUwgVB">Faster rates in regression via active learning</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="179" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,72.00,474.14,468.37,8.74;33,95.91,491.34,182.79,8.74" xml:id="b8">
	<monogr>
		<title level="m" type="main" xml:id="_NuH3DaX">Decision-aware learning for optimizing health supply chains</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rostami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bastani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.08507</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="33,72.00,513.28,469.49,8.74;33,95.66,530.48,17.71,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_vY7ZhMa">Improving generalization with active learning</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Atlas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ladner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jjuZ45A">Machine learning</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="221" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,72.00,552.42,430.36,8.74" xml:id="b10">
	<monogr>
		<title level="m" type="main" xml:id="_733WFKk">A general agnostic active learning algorithm (Citeseer)</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monteleoni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,72.00,574.35,468.00,8.74;33,95.55,591.56,444.45,8.74;33,95.37,608.76,446.01,8.74;33,95.25,625.97,330.32,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_Av8n4yE">Predict+ optimise with ranking objectives: Exhaustively learning linear functions</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Demirović</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Stuckey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Leckie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Guns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_vZQHwZ8">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019</title>
				<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-10">2019. August 10-16, 2019</date>
			<biblScope unit="page" from="1078" to="1085" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence</note>
</biblStruct>

<biblStruct coords="33,72.00,647.90,468.00,8.74;33,95.91,665.11,445.46,8.74;33,95.17,682.31,446.21,8.74;33,94.34,699.52,447.03,8.74;33,95.35,716.72,253.50,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_trK3fZR">Dynamic programming for predict+ optimise</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Demirovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Stuckey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Guns</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Leckie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_5s9ED4j">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07">2020. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1444" to="1451" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct coords="34,72.00,81.11,468.00,8.74;34,95.27,98.31,194.45,8.74" xml:id="b13">
	<monogr>
		<title level="m" type="main" xml:id="_rKUXP3b">Task-based end-to-end model learning in stochastic optimization. Advances in neural information processing systems 30</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Donti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,72.00,120.24,467.99,8.74;34,95.91,137.44,217.43,8.74" xml:id="b14">
	<monogr>
		<title level="m" type="main" xml:id="_wd7HS4h">Generalization bounds in the predict-then-optimize framework</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>El Balghiti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Elmachtoub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Grigas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Mathematics of Operations Research</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,72.00,159.37,435.01,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_6J9kMmD">Smart &quot;predict, then optimize</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Elmachtoub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Grigas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_DqeEm63">Management Science</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="26" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,72.00,181.31,469.66,8.74;34,95.91,198.51,372.22,8.74" xml:id="b16">
	<monogr>
		<title level="m" type="main" xml:id="_KeRbTJx">Estimate-then-optimize versus integrated-estimationoptimization: A stochastic dominance perspective</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Elmachtoub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.06833</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="34,72.00,220.44,469.66,8.74;34,95.91,237.64,391.20,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_NqDjyD9">Decision trees for decision-making under the predict-thenoptimize framework</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Elmachtoub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jcn</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mcnellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_B3URqcK">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2858" to="2867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,72.00,259.58,468.00,8.74;34,95.91,276.78,200.38,8.74" xml:id="b18">
	<monogr>
		<title level="m" type="main" xml:id="_9zrMVS7">Finite-sample guarantees for wasserstein distributionally robust optimization: Breaking the curse of dimensionality</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Operations Research</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,72.00,298.71,467.24,8.74;34,95.15,315.91,307.61,8.74" xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_fScs3vm">Cost-accuracy aware adaptive labeling for active learning</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saar-Tsechansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_HV5CFDD">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2569" to="2576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,72.00,337.85,468.56,8.74;34,95.91,355.05,2.77,8.74" xml:id="b20">
	<monogr>
		<title level="m" type="main" xml:id="_xzYhEfd">Integrated conditional estimation-optimization</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Grigas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.12351</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="34,72.00,376.98,468.00,8.74;34,95.27,394.18,241.44,8.74" xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_bhkyRXb">A bound on the label complexity of agnostic active learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hanneke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_HSMdUFs">Proceedings of the 24th international conference on Machine learning</title>
				<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,72.00,416.12,403.18,8.74" xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_azWVUmG">Rates of convergence in active learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hanneke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_74Azj2h">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="333" to="361" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,72.00,438.05,468.00,8.74;34,95.91,455.25,445.86,8.74" xml:id="b23">
	<monogr>
		<title level="m" type="main" xml:id="_eeNXShh">On data-driven prescriptive analytics with side information: A regularized nadaraya-watson approach</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">P</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Hanasusanto</surname></persName>
		</author>
		<ptr target="http://www.optimization-online.org/DBFILE/2019/01/7043.pdf" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,72.00,477.18,469.93,8.74;34,95.31,494.38,98.01,8.74" xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_jnpDYe9">Risk guarantees for end-to-end prediction and optimization processes</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ho-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Kılınç-Karzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6H2C5qb">Management Science</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,72.00,516.32,435.46,8.74" xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_2XFUKKC">Fast rates for contextual linear optimization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kallus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_v5PjBwT">Management Science</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,72.00,538.25,467.96,8.74;34,95.37,555.45,153.35,8.74" xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_pJUUUSU">Active learning in the non-realizable case</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kääriäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_xfc3wsH">International Conference on Algorithmic Learning Theory</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="63" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,72.00,577.39,417.06,8.74" xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_CEgQvPr">Stochastic optimization forests</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kallus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UsKC383">Management Science</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1975" to="1994" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,72.00,599.32,469.12,8.74;34,72.00,621.25,468.38,8.74;34,95.91,638.45,183.10,8.74" xml:id="b28">
	<monogr>
		<title level="m" type="main" xml:id="_6dGDkSd">End-to-end constrained optimization learning: A survey</title>
		<author>
			<persName coords=""><forename type="first">Kao</forename><surname>Yh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X ;</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kotary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Fioretto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Van Hentenryck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wilder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16378</idno>
		<imprint>
			<date type="published" when="2009">2009. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in Neural Information Processing Systems 22</note>
</biblStruct>

<biblStruct coords="34,72.00,660.39,467.99,8.74;34,95.91,677.59,359.98,8.74" xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_4XM9yPr">Active learning for cost-sensitive classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iii</forename><forename type="middle">H</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_XTHc4Wq">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1915" to="1924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,72.00,699.52,469.94,8.74;34,95.17,716.72,238.01,8.74" xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_FEV3XjU">Learning theory and algorithms for forecasting non-stationary time series</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_jY98QK6">Advances in neural information processing systems 28</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,72.00,81.11,468.00,8.74;35,95.35,98.29,188.02,8.74" xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_QWjxyuz">Risk bounds and calibration for a smart predict-then-optimize method</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Grigas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_my9ARtD">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,72.00,120.21,469.93,8.74;35,95.17,137.39,127.28,8.74" xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_AEpukcn">Decision-driven regularization: A blended model for predict-then-optimize</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Loke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_RGxwX3K">SSRN 3623006</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,72.00,159.31,467.72,8.74;35,95.37,176.50,203.38,8.74" xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_3s6kd3F">Interior point solving for lp-based prediction+ optimisation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mandi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Guns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mRCRbFQ">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7272" to="7282" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,72.00,198.42,468.00,8.74;35,95.91,215.60,418.01,8.74" xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_QPhUDxu">Smart predict-and-optimize for hard combinatorial optimization problems</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mandi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Stuckey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Guns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_hYBTG7m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1603" to="1610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,72.00,237.52,469.53,8.74;35,95.41,254.70,63.10,8.74" xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_ASudEnB">Online learning via sequential complexities</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_guJ54tH">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="186" />
			<date type="published" when="2015">2015a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,72.00,276.62,467.99,8.74;35,95.91,293.81,255.49,8.74" xml:id="b36">
	<monogr>
		<title level="m" type="main" xml:id="_aWwgVkV">Sequential complexities and uniform martingale laws of large numbers</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015b</date>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="111" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,72.00,315.73,468.20,8.74;35,95.91,332.91,42.12,8.74" xml:id="b37">
	<monogr>
		<title level="m" type="main" xml:id="_AUTMNMn">Active learning literature survey</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="35,72.00,354.83,468.00,8.74;35,95.37,372.01,105.25,8.74" xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_zypFM7v">Pool-based active learning in approximate linear regression</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4MTuX3f">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="274" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,72.00,393.93,468.00,8.74;35,95.91,411.12,247.02,8.74" xml:id="b39">
	<monogr>
		<title level="m" type="main" xml:id="_RQ7uppA">Pyepo: A pytorch-based end-to-end predict-then-optimize library for linear and integer programming</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">B</forename><surname>Khalil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.14234</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="35,72.00,433.04,467.99,8.74;35,95.91,450.22,77.57,8.74" xml:id="b40">
	<monogr>
		<title level="m" type="main" xml:id="_BtH9N2r">High-dimensional statistics: A non-asymptotic viewpoint</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,72.00,472.14,468.20,8.74;35,95.91,489.33,119.85,8.74" xml:id="b41">
	<monogr>
		<title level="m" type="main" xml:id="_2PgSF65">Weak convergence and empirical processes: with applications to statistics</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wellner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,72.00,511.24,468.20,8.74;35,95.91,528.43,445.47,8.74;35,95.41,545.61,47.60,8.74" xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_EtKXVRd">Melding the data-decisions pipeline: Decision-focused learning for combinatorial optimization</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wilder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dilkina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tambe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wTdsa8s">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1658" to="1665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,72.00,567.53,469.39,8.74;35,95.91,584.72,444.09,9.02;35,95.91,602.62,60.30,8.30" xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_AaW6yg8">CCCP is frank-wolfe in disguise</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yurtsever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=OGGQs4xFHrr" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_kVdmyK6">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,72.00,623.82,469.49,8.74;35,95.41,641.01,22.69,8.74" xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_Zw8DWzA">Joint estimation and robustness optimization</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_JGvpWt6">Management Science</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1659" to="1677" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
