<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_eEMXFbh">Speech Emotion Analysis: Exploring the Role of Context</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,164.04,121.39,63.22,9.63"><forename type="first">Ashish</forename><surname>Tawari</surname></persName>
							<email>atawari@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Robotics Research Labora-tory</orgName>
								<orgName type="institution">University of California at San Diego</orgName>
								<address>
									<addrLine>La Jolla</addrLine>
									<postCode>92093</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.24,121.39,177.04,9.63"><roleName>Fellow, IEEE</roleName><forename type="first">Mohan</forename><forename type="middle">Manubhai</forename><surname>Trivedi</surname></persName>
							<email>mtrivedi@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Robotics Research Labora-tory</orgName>
								<orgName type="institution">University of California at San Diego</orgName>
								<address>
									<addrLine>La Jolla</addrLine>
									<postCode>92093</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,39.60,682.62,72.23,7.01"><roleName>Dr</roleName><forename type="first">Hamid</forename><forename type="middle">K</forename><surname>Aghajan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Robotics Research Labora-tory</orgName>
								<orgName type="institution">University of California at San Diego</orgName>
								<address>
									<addrLine>La Jolla</addrLine>
									<postCode>92093</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_MJNCXmF">Speech Emotion Analysis: Exploring the Role of Context</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A59755626F397587B6FAAD3315735367</idno>
					<idno type="DOI">10.1109/TMM.2010.2058095</idno>
					<note type="submission">received December 15, 2009; revised April 01, 2010; accepted June 02, 2010. Date of current version September 15, 2010.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-05-12T20:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_j247pMn">Affect analysis</term>
					<term xml:id="_muH78NT">affective computing</term>
					<term xml:id="_6UGSSW4">context analysis</term>
					<term xml:id="_mnvN9Kp">emotional speech</term>
					<term xml:id="_CEs2fyp">emotion intelligence</term>
					<term xml:id="_hHazr6X">emotion recognition</term>
					<term xml:id="_BxJJN5d">vocal expression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_GBkbwZF"><p xml:id="_aR5PuDZ">Automated analysis of human affective behavior has attracted increasing attention in recent years. With the research shift toward spontaneous behavior, many challenges have come to surface ranging from database collection strategies to the use of new feature sets (e.g., lexical cues apart from prosodic features). Use of contextual information, however, is rarely addressed in the field of affect expression recognition, yet it is evident that affect recognition by human is largely influenced by the context information. Our contribution in this paper is threefold. First, we introduce a novel set of features based on cepstrum analysis of pitch and intensity contours. We evaluate the usefulness of these features on two different databases: Berlin Database of emotional speech (EMO-DB) and locally collected audiovisual database in car settings (CVRRCar-AVDB). The overall recognition accuracy achieved for seven emotions in the EMO-DB database is over 84% and over 87% for three emotion classes in CVRRCar-AVDB. This is based on tenfold stratified cross validation. Second, we introduce the collection of a new audiovisual database in an automobile setting (CVRRCar-AVDB). In this current study, we only use the audio channel of the database. Third, we systematically analyze the effects of different contexts on two different databases. We present context analysis of subject and text based on speaker/text-dependent/-independent analysis on EMO-DB. Furthermore, we perform context analysis based on gender information on EMO-DB and CVRRCar-AVDB. The results based on these analyses are promising.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_TkUkU4R">I. INTRODUCTION</head><p xml:id="_6nNGq4M">S PEECH signals convey not only words and meanings but also emotions. Besides human facial expressions, speech has been proven to be another promising modality for the recognition of human emotions. Spoken communication between humans is intricately linked with linguistic information (verbal content) and paralinguistic information such as tone, emotional states, and gestures. In such interaction, one's affective state plays a fundamental role in enriching communication. Current human-computer interaction (HCI) systems, however, ignore the user's affective states, losing a significant portion of information available in the interaction process. The human-computer paradigm suggests that user interfaces of the future need to detect subtleties and changes in the user's behavior, especially his/her affective behavior, and to initiate interactions based on this information rather than simply responding to the user's commands. The future human-centered multimodal HCI will change the ways in which we interact with computer systems. For example, an intelligent automobile system with a fatigue detector could monitor the vigilance of the driver and apply appropriate action to avoid accidents. Another important application of automated systems for human affect recognition is in affectrelated research (e.g., in psychology, psychiatry, behavior science, and neuroscience), where such systems can eliminate the tedious manual task of processing data. Research areas like social and emotional development studies <ref type="bibr" coords="1,458.46,342.55,10.58,8.76" target="#b0">[1]</ref>, mother-infant interaction <ref type="bibr" coords="1,329.70,354.49,10.58,8.76" target="#b1">[2]</ref>, and psychiatric disorders <ref type="bibr" coords="1,449.34,354.49,11.62,8.76" target="#b2">[3]</ref> would be substantially benefited. Automatic detection of fatigue, depression, and anxiety could also form an important step towards personal wellness and assistive technologies <ref type="bibr" coords="1,408.84,390.37,10.58,8.76" target="#b3">[4]</ref>. Some initial efforts toward such advanced system include <ref type="bibr" coords="1,405.36,402.31,11.51,8.76" target="#b4">[5]</ref>- <ref type="bibr" coords="1,420.71,402.31,11.51,8.76" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_YrtAm6x">II. RELATED RESEARCH AND MOTIVATION</head><p xml:id="_sDQrhc9">The auditory signal in conversation carries various kinds of information. If we disregard the manner in which it was spoken by considering only the verbal part, we might miss important aspects of the pertinent utterance and even misunderstand the spoken message. Humans are capable of recognizing subtle differences implied in an utterance. It is currently hard to imagine an artificial system reaching such a high degree of discrimination. A technical approach for classification would rely on the kind and number of emotions allowed. Research on vocal affect recognition is largely influenced by the basic emotion theory. Most of the existing efforts in this direction aim at the recognition of a subset of basic emotions from speech signals. In recent years, however, few studies have been made focusing on interpreting speech signals in terms of certain application-dependent affective states <ref type="bibr" coords="1,364.68,604.57,11.90,8.76" target="#b8">[9]</ref>- <ref type="bibr" coords="1,380.54,604.57,15.86,8.76" target="#b11">[12]</ref>.</p><p xml:id="_Zp8jvNJ">Another aspect of the vocal affect analysis is to specify the auditory features to be estimated from the input audio signal. The research in psychology and psycholinguistics provides several results on acoustic features which are correlated with emotion expressions. The popular features are prosodic features (e.g., pitch-related features, energy-related features, and speech rate) and spectral features (e.g., mel frequency cepstral coefficients (MFCCs) and cepstral features). Most of the existing approaches are trained and tested on speech data that were collected by asking actors to speak prescribed utterances with certain emotions <ref type="bibr" coords="1,437.46,736.09,15.27,8.76" target="#b9">[10]</ref>, <ref type="bibr" coords="1,459.84,736.09,15.27,8.76" target="#b12">[13]</ref>. However, the fact that deliberate behavior differs in audio profile and timing from spontaneous behavior has led research to shift towards the analysis of spontaneous human behavior in naturalistic audio recordings. In <ref type="bibr" coords="2,99.30,104.47,15.27,8.76" target="#b13">[14]</ref>, a good overview of recent advancement towards spontaneous behavior analysis in audio, visual, and audiovisual domains is presented. In spontaneous human behavior, identifying subtle changes in vocal affect expression based on only acoustic information is not sufficient <ref type="bibr" coords="2,257.64,152.29,15.27,8.76" target="#b14">[15]</ref>. In <ref type="bibr" coords="2,38.10,164.23,15.27,8.76" target="#b15">[16]</ref>, the use of lexical cues has been reported to improve the performance. However, extracting these features automatically is a challenging task. In contrast to spoken language processing, which has witnessed significant advances, the processing of "emotional speech" is still a difficult problem. Studies show that the accuracy of automated speech recognition system tends to drop to 50%-60% for emotional speech from 89%-90% for neutrally spoken words <ref type="bibr" coords="2,136.32,247.93,15.27,8.76" target="#b16">[17]</ref>, <ref type="bibr" coords="2,158.88,247.93,15.27,8.76" target="#b17">[18]</ref>. The same has been shown for speaker verification systems <ref type="bibr" coords="2,166.50,259.87,15.27,8.76" target="#b18">[19]</ref>. In such scenarios, the use of context information can help us improve the performance.</p><p xml:id="_bf3sPAV">It is evident that the ability of emotion perception in human beings is greatly influenced by the contextual information. The work in <ref type="bibr" coords="2,72.96,307.69,16.60,8.76" target="#b19">[20]</ref> discusses how contextual information influences emotion annotation as well as machine-learned classification. In particular, nine nonexpert annotators when provided with the context information by giving them utterances along with the dialogue where these were produced, marked 3.4% more nonneutral emotions. Similarly, machine-learned classfication of negative emotions (angry, doubtful, and bored) is enhanced by incorporating automatically generated context information in two steps. The first step is to classify emotions into angry and doubt-fulORbored by utilizing users' neutral speaking style, and the second step uses the dialogue context such as the total number of dialogue turns (referred to as depth) and the number of additional user turns necessary to obtain a particular piece of information (referred as to width) to distinguish between the bored and the doubtful categories. Authors claim that the classification process is substantially improved by adding both sources of contextual information.</p><p xml:id="_5VatHRg">Few studies <ref type="bibr" coords="2,99.48,510.90,16.97,8.76" target="#b20">[21]</ref>- <ref type="bibr" coords="2,120.68,510.90,16.97,8.76" target="#b22">[23]</ref> have investigated the role of context information like subject and gender. The work in <ref type="bibr" coords="2,230.46,522.84,16.60,8.76" target="#b22">[23]</ref> shows improved classification performance by incorporating a gender-detection module as a front end to overall emotion recognition system. Such contextual information is relatively easy to extract and simple to incorporate in existing classification system. We believe that enough attention is not been paid to evaluate the effectiveness of such contextual information for emotion recognition. Towards this end, in this paper, we present a systematic study on the use of context information like user (the speaker), text (the spoken content), and gender (male-female) on a publicly available Berlin Database of Emotional Speech (EMO-DB) and locally collected car database (CVRRCar-AVDB).</p><p xml:id="_h7uQw76">Despite the relatively high recognition accuracy reported on deliberate (acted) vocal expression, performance evaluation based on speaker-or text-independent analysis is still not often addressed. The well-documented EMO-DB database gives us the opportunity to perform such analyses. One of the best reported results on EMO-DB is in <ref type="bibr" coords="2,198.42,726.12,15.27,8.76" target="#b23">[24]</ref>. This presents the performance results of a series of machine-learning algorithms for emotion classification. It uses overlapping frames of 25 ms with a shift of 10 ms to extract a feature vector consisting of 15 coefficients: the log-energy, 12 MFCCs, the pitch period, and the voicing class. On the basis of the time series of these parameters, over 3800 statistical parameters were extracted to characterize semantic unit of varying lengths. Furthermore, feature selection, normalization, and discretization techniques are utilized to improve the base performance. Best classification performance is achieved using the SVM classifier. However, speaker-and text-independent performance is not analyzed, which is important for practical application of such system. In this paper, we present these studies on EMO-DB emotional speech corpus. An important related issue that should also be examined is how one can utilize information about the context (environment, observed subjects or the current task), in which the observed affective behavior was displayed. Towards this end, we also present the study of a context model based on gender information on the EMO-DB and CVRRCar-AVDB.</p><p xml:id="_DeyUfKD">Fig. <ref type="figure" coords="2,331.50,506.71,4.98,8.76" target="#fig_0">1</ref> shows a generic block diagram showing the three fundamental steps of information acquisition, extraction, and processing of parameters and classification of semantic units. The classification system has three different phases: feature extraction and selection (phase 1), to identify features to be used during classification; the model training phase (phase 2); and, finally, the testing phase (phase 3) to evaluate the performance of the system in terms of classification accuracy. The remainder of this paper is organized based on these phases. In Section III, we provide a brief overview of speech corpora used in our experiments. Section IV describes the statistical feature extracted from the speech signal. In Section V, details about the model training and evaluation are provided. Finally, in Section VI, we provide some concluding remarks and discuss future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_EDnQEvh">III. DATABASES FOR EMOTION RECOGNITION STUDIES</head><p xml:id="_fmg7wpB">Collecting emotional data is certainly useful for researchers interested in human affective expression recognition. Authentic affective expressions are difficult to collect because they are relatively rare and filled with subtle context-based changes. More-Fig. <ref type="figure" coords="3,54.90,275.76,2.99,7.01">2</ref>. Example of a "positive" utterance obtained during natural conversation between driver and passenger in a moving-car environment. Film strip shows samples of five images equally spaced in the utterance. The first half of the utterance contains the speech and later half the road noise. Notice, however, that facial features are more expressive after speech content while head dynamics are concomitant with the speech. over, manual labeling of spontaneous emotions for the ground truth is very time-consuming and error-prone <ref type="bibr" coords="3,220.20,337.50,15.27,8.76" target="#b24">[25]</ref>. Due to these difficulties, a majority of databases encompass five to six emotions based on deliberate (acted) affect expression. In this research, two databases are used: EMO-DB <ref type="bibr" coords="3,209.40,373.33,16.60,8.76" target="#b25">[26]</ref> and CVRRCar-AVDB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_zNNuFUN">A. Berlin Database of Emotional Speech</head><p xml:id="_mVH88Ye">The EMO-DB Berlin was recorded at the Technical University, Berlin. It comprises of six basic emotions (anger, boredom, disgust, anxiety, happiness, and sadness) as well as neutral speech. Ten professional German actors (five female and five male) spoke ten sentences with emotionally neutral content in the seven different emotions. In our study, we used 535 sentences available in the database. These sentences were not equally distributed between the various emotional states: 69 frightened (fea); 46 disgusted (dis); 71 happy(joy); 81 bored (bor); 79 neutral (neu); 62 sad (sad); 127 angry (ang).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_7tdbzqg">B. Audio-Visual Affect Database (CVRRCar-AVDB)</head><p xml:id="_mjUABNx">In our ongoing research on driver-assistance systems <ref type="bibr" coords="3,271.56,572.70,15.27,8.76" target="#b26">[27]</ref>, <ref type="bibr" coords="3,39.60,584.64,16.60,8.76" target="#b27">[28]</ref> and audiovisual scene understanding <ref type="bibr" coords="3,212.70,584.64,15.27,8.76" target="#b28">[29]</ref>, <ref type="bibr" coords="3,235.38,584.64,15.27,8.76" target="#b29">[30]</ref>, emotion recognition using multimodality will certainly help us improve the interface. With this motivation, we have put in significant effort on collection of audiovisual affect database in a challenging ambient of car settings <ref type="bibr" coords="3,171.00,632.46,15.27,8.76" target="#b30">[31]</ref>. The user at the driver's seat was prompted by a computer program specifying the emotion to be expressed. It also provides example of an utterance that can be used by the driver. The database is collected in both stationary and moving-car environments. We also have been collecting natural conversation between driver and passenger. Fig. <ref type="figure" coords="3,57.54,704.17,4.98,8.76">2</ref> shows one such example of a happy utterance obtained in a moving-car environment. The cockpit of the automobile does not provide the comfort of noiseless anechoic environment. In fact, moving automobile with a lot of road noise has a drastic effect on signal-to-noise ratio (SNR) for audio channel <ref type="bibr" coords="3,262.92,751.99,16.60,8.76" target="#b31">[32]</ref> as well as challenging illumination condition for the video channel.</p><p xml:id="_YRghHMq">In this study, we analyze emotional speech data from stationary car setting which gives the effect of the cockpit of the car with relatively high SNR value.</p><p xml:id="_umE2WQH">The database is collected with the use of an analog video camera facing the driver and a directional microphone beneath steering wheel. Fig. <ref type="figure" coords="3,383.82,397.27,4.98,8.76" target="#fig_1">3</ref> shows the settings of the camera and microphone. Video frames were acquired approximately 30 frames per second, and the audio signal, captured, is resampled to a 16-kHz sampling rate. A version of the software for synchronizing as well as labeling the data is developed. Fig. <ref type="figure" coords="3,520.74,445.09,4.98,8.76" target="#fig_2">4</ref> shows a snippet of the tool. The emotional speech has been labeled into three groups "pos," "neg," and "neu" for positive, negative, and neutral expressions. The data were acquired with four different subjects: two male and two female. Distribution of data for different categories is: 82 pos, 82 neg, and 60 neu. Fig. <ref type="figure" coords="3,548.70,504.85,4.98,8.76" target="#fig_3">5</ref> presents a visualization of typical utterances for three emotions: pos/happy (top row), neg/sad (bottom row) and neutral (middle row) for CVRRCar-AVDB and EMO-DB databases. It can be noticed that the strength (amplitude) and quality (SNR) of the speech signal of the EMO-DB database obtained in a controlled setting is far superior than those of CVRRCar-AVDB obtained in the car setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_JJ9CYJA">IV. FEATURE EXTRACTION, SELECTION AND TRANSFORMATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Z5FmHJt">A. Feature Extraction</head><p xml:id="_Gsz26vv">So far, a large number of different features have been proposed to recognize emotional states from speech signal. These features can be categorized as acoustic features and linguistic features. Linguistic features analyzes the spoken content and often requires recognition of various words in the utterance. We avoid using these features since these demand for robust recognition of speech at first place and also are a drawback for multilanguage emotion recognition. Hence, we only consider the acoustic features. Within the acoustic category, we focus on prosodic and spectral features to model emotional states.  It is well known that different emotional states carry different prosodic patterns. Hence, prosodic feature like speech intensity, pitch and speaking rate can model prosodic patterns in different emotions. Similarly, spectral feature like MFCCs have been used successfully in emotion recognition.</p><p xml:id="_s5FRCqY">For pitch calculation, we used the auto-correlation algorithm similar to <ref type="bibr" coords="4,81.90,644.41,15.27,8.76" target="#b32">[33]</ref>. The input signal is divided into overlapping frames with shift intervals (difference between the starting point of consecutive frames) of 10 ms. Each frame is of 60 ms long to be able to span three periods of minimum pitch value (in our case, 50 Hz). Pitch candidate over each frame is calculated and a dynamic programming technique is used to get the final pitch contour. Log-energy coefficients are calculated using 30-ms frames with shift interval of 10 ms.</p><p xml:id="_XePrMur">The value of framewise parameters extracted from a few milliseconds of audio is of little significance to determine an emo- tional state. On the contrary, it is of interest to capture the time trend of these features. In order to capture the characteristics of the contours, we perform cepstrum analysis over the contour. For this, we first interpolate the contour to obtain samples separated by sampling period of speech signal, which is then used to calculate the cepstrum coefficients as follows:</p><p xml:id="_5DQrTWc">where denotes the -point discrete Fourier transform of the windowed signal . Cesptrum analysis is a source-filter separation process commonly used in speech processing. Cepstrum coefficients to and their time derivative (first and second order), calculated from 480 samples, are utilized to obtain the spectral characteristic of the contours. For pitch contour analysis, only the voiced portion is utilized. Fig. <ref type="figure" coords="4,506.94,728.11,4.98,8.76">6</ref> shows the interpolated pitch contour and voiced segment used for the cepstrum analysis along with the spectrogram of the speech. Other Fig. <ref type="figure" coords="5,53.64,274.68,2.99,7.01">6</ref>. Pitch contour analysis of an utterance. Further, the contour is upsampled at a sampling frequency of the speech signal and used to calculate the cepstrum coefficient. We only utilize the voiced region (the red crossed marked portion) for final feature extraction.</p><p xml:id="_Gv4pb9S">features that we utilized are 13 MFCCs with their delta and acceleration components. Input signal is processed using a 30-ms Hamming window with a frame shift interval of 10 ms.</p><p xml:id="_udVZMdj">For all of these sequences, the following statistical information is calculated: mean, standard deviation, relative maximum/ minimum, position of relative maximum/minimum, first quartile, second quartile (median), and third quartile. The speaking rate is modeled as a fraction of the voiced segments. Thus, the total feature vector per segment contains attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_hebp6p3">B. Feature Selection</head><p xml:id="_ruwKWc3">Intuitively, a large number of features would improve the classification performance, however, in practice, a large feature space suffers from the phenomenon of "curse of dimensionality." Therefore, in order to improve the classification performance, a feature selection technique is utilized. This also helps to increase the speed of the system. One such method to eliminate redundant and insignificant features is to identify features with high correlation with the class but low correlation among themselves. In this paper, we used CFSSubsetEval feature selection technique provided by WEKA. To determine the best subset, we used a best-first search strategy and a stratified tenfold cross-validation procedure. Thus, we have ten different sets of selected attributes. An attribute may get selected in of times in different sets. We group the attributes which are selected at least of times and call them " " aggregate. Fig. <ref type="figure" coords="5,187.74,692.22,4.98,8.76" target="#fig_4">7</ref> shows the performance results for different aggregates with a sequential minimal optimization (SMO) classifier provided by WEKA. The "2-10" aggregate provided the best results on EMO-DB. Similarly, for the CVRRCar-AVDB database, the best aggregate set of features were selected for further analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_hWBEhqT">C. Feature Normalization</head><p xml:id="_AyfdasY">Feature normalization is a common technique to provide more appropriate attributes to the learning scheme used. In this work, we used the z-score technique. This transforms the original attributes to new attributes as where and are mean and standard deviation of , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_SYcxZWA">V. EXPERIMENTAL STUDIES AND ANALYSIS</head><p xml:id="_cSneknG">Here, we describe a series of experiments for user-and textindependent analysis for emotion recognition. In all of our experiments, we have used an SVM trained with the SMO algorithm with "2-10" aggregate features. Before these analyses, we provide performance results for randomized tenfold cross validation, that is, the database is divided into ten folds in stratified manner so that they contain approximately the same proportions of labels as the original database, and the system is trained on nine folds and tested on the left out fold. This is repeated ten times, each time leaving out a different fold. The confusion matrix obtained through the cross validation is presented in Table <ref type="table" coords="5,366.24,584.65,2.90,8.76" target="#tab_0">I</ref>. The overall recognition rate (weighted accuracy) on this seven-way classification task was 84.5%, whereas average per class accuracy (unweighted accuracy) was 83.1%. This will compare with some of the best performances reported on EMO-DB database. One of the best performances reported so far on EMO-DB is given in <ref type="bibr" coords="5,426.24,644.41,15.27,8.76" target="#b23">[24]</ref>. The work in <ref type="bibr" coords="5,499.44,644.41,16.60,8.76" target="#b23">[24]</ref> achieved close to 85% weighted accuracy using a similar feature selection technique and SMO classifier. In their experiments, however, they used 494 sentences based on a listening test of 20-30 judges as opposed to 535 used in our experiments. That work further used a discretization technique to improve the results. Our results are comparable to those obtained prior to disctretization. We did not use the discretization step to tailor the system for the database, a route that we wish to avoid for the sake of generality. We believe that, if the database size is sufficiently  large, such a step will have little effect on recognition accuracy <ref type="bibr" coords="6,38.10,405.30,15.27,8.76" target="#b33">[34]</ref>. Also of note is that our feature set based on cepstrum analysis of pitch and energy contour is quite novel and performed well, despite this being our first attempt to employ these features. Thus, we believe this approach shows great potential for improvement as we begin exploring the parameters of the technique in greater detail. Other reported results on EMO-DB is given in <ref type="bibr" coords="6,72.90,477.01,15.27,8.76" target="#b34">[35]</ref>, which achieved 83.8% recognition rate using fusion of GMM and HMM based classifiers on fivefold cross validation. However, a conclusion cannot be made about superiority of one method over other for the reason that training and test set while cross validation can be quite Either the database should provide a seperate training and test set or a deterministic method of performance evaluation such as leave-one-out cross validation should be performed.</p><p xml:id="_QfWpfEH">Next, we present a series of experiments for user-and text-independent analysis to understand the practical utility of these system in a real-world scenario. For user-independent analysis, we used leave-one-subject-out cross validation, where each time system is trained leaving one speaker out of the training set and tested performance on the speaker left out. Table <ref type="table" coords="6,65.34,644.40,6.64,8.76" target="#tab_1">II</ref> provides the confusion matrix for this seven-way classification task. Using the same SMO classifier and 2-10 aggregate feature set, the performance measure (unweighted accuracy) drops down to 72.6%. For text-independent analysis, leave-one-text-out cross validation is employed where each time the system is trained leaving all the utterances with same spoken content across speaker out of the training set and testing on the left out utterances. Table <ref type="table" coords="6,166.68,728.10,9.95,8.76" target="#tab_2">III</ref> shows the confusion matrix obtained for the analysis. Unweighted accuracy, in this case, is still over 80%. These analyses suggest that the classifier    is learning a particular manner that a speaker might express his/her emotions while ignoring the verbal content to some degree. This is a very important observation for a successful design of a practical system, which suggests a need for speaker adaptation in speech emotion recognition task.</p><p xml:id="_ANgHVMw">Other experiments include the analyses of context like gender information which can be reliably recognized automatically. Performance results based on randomized tenfold cross validation, leave-one-subject-out and leave-one-text-out cross validation on EMO-DB speech corpus for gender-dependent analysis (male-female combined performance) is summarized in Tables IV, V, and VI, respectively. Tables VII and VIII provide gender-dependent and -independent analysis on CVR-RCar-AVDB for randomized tenfold cross validation. From the analysis, it is evident that using gender information in emotion recognition leads to significant improvement in recognition rate. For EMO-DB, text-independent analysis has relative improvement of over 3%. Similarly, for CVRRCar-AVDB relative improvement of over 3% is quite encouraging.    We presented a systematic study to understand the importance of the context in emotion recognition from speech. We also introduced a novel feature set based on cepstrum analysis of pitch and speech-intensity contours. Using this feature set, we were able to achieve a high recognition rate of over 84% for the seven emotions on EMO-DB and over 87% for the three groups of emotions on CVRRCar-AVDB using tenfold stratified cross validation. We then performed user-and text-independent analysis on EMO-DB database. The results suggest that the system is robust towards the spoken containt, however, it performs significantly better when speaker information is incorporated into the training set. Furthermore, we analyzed the use of gender-based context information on recognition rate over the two databases. Experimental results suggest that a gender-specific emotion recognizer works more accurately than a genderindependent one. In future, we will extend our basic emotion recognition system by a preceeding stage of automatic genderdetection system that would determine which gender-specific emotion recognition system should be used. However, there is still much room for improvement. Our initial experiments with vision modality and audiovisual analysis have shown promising results. Although these two modalities do not couple strongly in time as also shown in Fig. <ref type="figure" coords="7,157.26,752.05,3.74,8.76">2</ref>, they seem to complement each other <ref type="bibr" coords="7,326.10,68.59,15.27,8.76" target="#b35">[36]</ref>. In some cases, similar facial expressions may have different vocal characteristics, and vocal emotions having similar properties may have different facial behaviors. Our initial observations also suggest that head dynamics carry useful information for emotion classification. We will thoroughly present such multimodal affect recognition analysis in our future efforts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,301.14,270.48,153.07,7.01;2,311.82,66.98,229.00,193.00"><head>Fig. 1 .</head><label>1</label><figDesc xml:id="_tKDgGTe">Fig. 1. Block diagram of classification system.</figDesc><graphic coords="2,311.82,66.98,229.00,193.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,38.10,282.66,514.07,7.01;4,38.10,291.61,514.10,7.01;4,38.10,300.61,100.38,7.01;4,43.14,66.58,504.00,206.00"><head>Fig. 3 .</head><label>3</label><figDesc xml:id="_rFbFeeW">Fig. 3. Visualization of typical utterances in (a) CVRRCar-AVDB database and (b) EMO-DB database for three emotions: positive/happy (top row), neutral (middle row), and negative/sad (bottom row) and. Notice that the signal strength in the realistic car setting for CVRRCar-AVDB is very poor as compared with the controlled setting in EMO-DB.</figDesc><graphic coords="4,43.14,66.58,504.00,206.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,38.10,520.56,251.04,7.01;4,38.10,529.50,251.06,7.01;4,38.10,538.44,21.03,7.01;4,40.62,330.64,246.00,179.00"><head>Fig. 4 .</head><label>4</label><figDesc xml:id="_UrdJB4T">Fig. 4. Tool for audiovisual scene synchronization, cropping, and labeling. It has capability of voice activity detection (VAD) to decide precise onset of speech signal.</figDesc><graphic coords="4,40.62,330.64,246.00,179.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,301.14,510.78,178.40,7.01;4,312.66,330.00,228.00,171.00"><head>Fig. 5 .</head><label>5</label><figDesc xml:id="_uSHhdVR">Fig. 5. Data acquisition and online recognition setting.</figDesc><graphic coords="4,312.66,330.00,228.00,171.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,302.64,252.84,251.03,7.01;5,302.64,261.78,194.53,7.01;5,499.44,257.06,54.20,13.34;5,302.64,270.78,200.98,7.01;5,506.10,266.06,47.56,13.34;5,302.64,279.72,133.95,7.01;5,319.92,67.06,216.00,176.00"><head>Fig. 7 .</head><label>7</label><figDesc xml:id="_JY253xs">Fig. 7. Correct classification accuracy over seven emotional states in EMO-DB using different aggregate of features and SMO classifier. "n 0 10" aggregate represents the group of attributes which are selected at leastn 0 number of times in 10 fold feature selection process.</figDesc><graphic coords="5,319.92,67.06,216.00,176.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,92.40,66.62,408.00,199.00"><head></head><label></label><figDesc xml:id="_gdJEgte"></figDesc><graphic coords="3,92.40,66.62,408.00,199.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,44.10,69.48,228.62,149.43"><head>TABLE I CONFUSION</head><label>I</label><figDesc xml:id="_qxYyKKE">TABLE FOR RANDOMIZED TENFOLD STRATIFIED CROSS VALIDATION</figDesc><table coords="6,44.10,196.04,105.88,22.88"><row><cell>Unweighted Accuracy = 83:1% Weighted Accuracy = 84:5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,44.10,232.92,233.41,140.43"><head>TABLE II CONFUSION</head><label>II</label><figDesc xml:id="_6G2ykdu">TABLE FOR LEAVE-ONE-SUBJECT-OUT CROSS VALIDATION Unweighted Accuracy = 72:6% Weighted Accuracy = 74:8%</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,307.08,69.48,228.25,135.15"><head>TABLE III CONFUSION</head><label>III</label><figDesc xml:id="_DVHQbHV">TABLE FOR LEAVE-ONE-TEXT-OUT CROSS VALIDATION</figDesc><table coords="6,307.08,181.76,106.08,22.88"><row><cell>Unweighted Accuracy = 80:6% Weighted Accuracy = 81:4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,303.12,217.56,247.10,144.09"><head>TABLE IV GENDER</head><label>IV</label><figDesc xml:id="_6pQRxxU">-DEPENDENT CONFUSION TABLE FOR RANDOMIZED TENFOLD CROSS VALIDATION</figDesc><table /><note xml:id="_QJmthYj">Unweighted Accuracy = 84:0% Weighted Accuracy = 84:9%</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,308.70,374.58,134.51,15.95"><head>TABLE V GENDER=DEPENDENT CONFUSION</head><label>VCONFUSION</label><figDesc xml:id="_c9hazrZ"></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,307.08,383.52,237.54,134.91"><head>TABLE FOR</head><label>FOR</label><figDesc xml:id="_sDjbqn6"></figDesc><table /><note xml:id="_xsaFhPQ">LEAVE-ONE-SUBJECT-OUT CROSS VALIDATIONUnweighted Accuracy = 72:2% Weighted Accuracy = 73:1%</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,42.18,69.48,140.83,16.01"><head>TABLE VI GENDER</head><label>VI</label><figDesc xml:id="_SwfQeTM"></figDesc><table /><note xml:id="_WPS5Pex">-DEPENDENT CONFUSION</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,45.60,78.48,242.54,135.15"><head></head><label></label><figDesc xml:id="_rxmYUfk">TABLE FOR LEAVE-ONE-TEXT-OUT CROSS VALIDATION Unweighted Accuracy = 83:21% Weighted Accuracy = 83:7%</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="7,45.60,226.56,235.22,106.71"><head></head><label></label><figDesc xml:id="_UUz4HMv">TABLE VII GENDER-INDEPENDENT CONFUSION TABLE FOR RANDOMIZED TENFOLD STRATIFIED CROSS VALIDATION Unweighted Accuracy = 87:9% Weighted Accuracy = 88:5%</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_m34xfmu">ACKNOWLEDGMENT</head><p xml:id="_HHHSad2">The authors would like to thank the reviewers and editors for their insightful comments and helpful suggestions. The authors would also like to thank their colleagues at the Computer Vision and Robotics Research Laboratory, University of California, San Diego, for useful discussions and assistance.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_KApsJEd" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,330.54,260.16,223.11,7.01;7,330.54,269.17,223.16,7.01;7,330.54,278.11,223.13,7.01;7,330.54,287.10,162.29,7.01" xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_23Efvey">The emotional integration of childhood experience: Physiological, facial expressive, and self-reported emotional response during the adult attachment interview</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Roisman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_g88PPtT">Devel. Psychol</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="776" to="789" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.54,296.05,223.13,7.01;7,330.54,305.05,223.16,7.01;7,330.54,313.99,198.41,7.01" xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_rSPUkrB">Mother-infant face-to-face interaction: Influence is bidirectional and unrelated to periodic cycles in either partner&apos;s behavior</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">Z</forename><surname>Tronick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_BQKxVqq">Devel. Psychol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="68" to="77" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.54,322.99,223.17,7.01;7,330.54,331.93,188.69,7.01" xml:id="b2">
	<monogr>
		<title level="m" type="main" xml:id="_n6umqbf">Facial expression in affective disorders</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="429" to="439" />
		</imprint>
	</monogr>
	<note>What the Face Reveals</note>
</biblStruct>

<biblStruct coords="7,330.54,340.87,223.13,7.01;7,330.54,349.87,223.13,7.01;7,330.54,358.81,121.62,7.01" xml:id="b3">
	<monogr>
		<title level="m" type="main" xml:id="_UBkaYnH">Machine analysis of facial expressions</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
		<editor>Face Recognition, K. Delac and M. Grgic</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="377" to="416" />
			<pubPlace>Vienna, Austria</pubPlace>
		</imprint>
	</monogr>
	<note>: I-Tech Education</note>
</biblStruct>

<biblStruct coords="7,330.54,367.81,223.11,7.01;7,330.54,376.75,223.14,7.01;7,330.54,385.75,29.89,7.01" xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_Z7Ew9Zk">Maui: A multimodal affective user interface</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Lisetti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nasoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_SQGJwve">Proc. 10th ACM Int. Conf. Multimedia</title>
				<meeting>10th ACM Int. Conf. Multimedia<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.54,394.69,223.12,7.01;7,330.54,403.63,223.16,7.01;7,330.54,412.63,223.17,7.01;7,330.54,421.57,151.43,7.01" xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_KEcnBgz">Integrating perceptual and cognitive modeling for adaptive and intelligent human-computer interaction</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Duric</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Heishman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schoelles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schunn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_VHCDfTD">Proc. IEEE</title>
				<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2002-07">Jul. 2002</date>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="1272" to="1289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.54,430.57,223.12,7.01;7,330.54,439.51,223.14,7.01;7,330.54,448.51,120.43,7.01" xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_2psYf5p">Gaze-x: Adaptive affective multimodal interface for single-user office scenarios</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Maat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Ew5My5V">Proc. ACM Int. Conf. Multimodal Interfaces</title>
				<meeting>ACM Int. Conf. Multimodal Interfaces</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.54,457.45,223.11,7.01;7,330.54,466.45,223.14,7.01;7,330.54,475.39,17.93,7.01" xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_fqBQkpA">Automatic prediction of frustration</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Burleson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Vxhnpct">Int. J. Human-Comput. Stud</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="724" to="736" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.54,484.33,223.16,7.01;7,330.54,493.33,221.57,7.01" xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_NhRsKS9">Children&apos;s emotion recognition in an intelligent tutoring scenario</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Levinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_X6jvfRy">Proc. ICSLP</title>
				<meeting>ICSLP</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.54,502.27,223.13,7.01;7,330.54,511.27,223.15,7.01;7,330.54,520.21,122.29,7.01" xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_ctSM7hX">Emotion recognition by speech signals</title>
		<author>
			<persName coords=""><forename type="first">O.-W</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-L</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_p6GkzPQ">Proc. 8th Eur</title>
				<meeting>8th Eur</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="172" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.54,529.21,223.11,7.01;7,330.54,538.15,223.13,7.01;7,330.54,547.15,168.42,7.01" xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_8QcGDeu">Prosodybased automatic detection of annoyance and frustration in human-computer dialog</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krupski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_49ZqKkp">Proc. ICSLP</title>
				<meeting>ICSLP</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="2037" to="2040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.54,556.09,223.12,7.01;7,330.54,565.03,223.14,7.01;7,330.54,574.03,208.57,7.01" xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_7d8GaCH">On the necessity and feasibility of detecting a driver&apos;s emotional state while driving</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kroschel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Nass</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Moosmayr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_6xqqjp2">Proc. ACII</title>
				<meeting>ACII</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="126" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.54,582.97,223.13,7.01;7,330.54,591.97,223.13,7.01;7,330.54,600.91,171.48,7.01" xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_FdAxYFw">Prosody based emotion recognition for mexi</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Austermann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Esau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kleinjohann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kleinjohann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_vUW9mQc">Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst</title>
				<meeting>IEEE/RSJ Int. Conf. Intell. Robots Syst</meeting>
		<imprint>
			<date type="published" when="2005-08">Aug. 2005</date>
			<biblScope unit="page" from="1138" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.54,609.91,223.15,7.01;7,330.54,618.85,223.13,7.01;7,330.54,627.79,223.15,7.01;7,330.54,636.79,17.93,7.01" xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_45eBGrU">A survey of affect recognition methods: Audio, visual, and spontaneous expressions</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Roisman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_JS2SdxT">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="58" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.54,645.73,223.15,7.01;7,330.54,654.73,223.17,7.01;7,330.54,663.67,62.45,7.01" xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_mmHr35c">How to find trouble in communication</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Spilker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nöth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qMD3GgD">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="117" to="143" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.54,672.67,223.10,7.01;7,330.54,681.61,223.15,7.01;7,330.54,690.61,223.14,7.01;7,330.54,699.55,29.89,7.01" xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_vQThqU6">Real-life emotions detection with lexical and paralinguistic cues on human-human call center dialogs</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Vidrascu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_4bxWpwK">Proc 9th Int. Conf. Spoken Language Processing</title>
				<meeting>9th Int. Conf. Spoken Language essing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="801" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.54,708.49,223.16,7.01;7,330.54,717.49,223.12,7.01;7,330.54,726.43,220.73,7.01" xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_hcVsdsg">Speech under stress conditions: Overview of the effect on speech production and on system performance</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J M</forename><surname>Steeneken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H L</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_9mNw5a5">Proc. ICASSP</title>
				<meeting>ICASSP<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="2079" to="2082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.54,735.43,223.11,7.01;7,330.54,744.37,223.12,7.01;7,330.54,753.37,213.88,7.01" xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_XgKM3mp">Acoustic profiles in vocal emotion expression</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Banse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<ptr target="http://view.ncbi.nlm.nih.gov/pubmed/8851745" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_6cU3qwN">J Pers. Soc. Psychol</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="614" to="636" />
			<date type="published" when="1996-03">Mar. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,66.00,69.90,223.20,7.01;8,66.00,78.84,223.15,7.01;8,66.00,87.84,223.17,7.01;8,66.00,96.78,17.93,7.01" xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_ceNSMSh">Automatic verification of emotionally stressed speakers: The problem of individual differences</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bänziger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_S3fzCPB">Proc. Int. Workshop Speech Comput</title>
				<meeting>Int. Workshop Speech Comput<address><addrLine>St. Petersburg, Russia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,66.00,105.78,223.15,7.01;8,66.00,114.72,223.21,7.01;8,66.00,123.66,109.19,7.01" xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_DJDEXAs">Influence of contextual information in emotion annotation for spoken dialogue systems</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Callejas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>López-Cózar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_k7244ca">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="416" to="433" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,66.00,132.67,223.19,7.01;8,66.00,141.61,223.19,7.01;8,66.00,150.61,22.41,7.01" xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_EyPc9u9">Predicting student emotions in computer-human tutoring dialogues</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Forbes-Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_NTN6R9D">Proc. ACL</title>
				<meeting>ACL<address><addrLine>Morristown, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">351</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,66.00,159.55,223.16,7.01;8,66.00,168.55,223.15,7.01;8,66.00,177.49,223.19,7.01;8,66.00,186.49,29.89,7.01" xml:id="b21">
	<monogr>
		<title level="m" type="main" xml:id="_gJsvQcC">Predicting emotion in spoken dialogue from multiple knowledge sources</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Forbes-Riley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Litman</surname></persName>
		</author>
		<editor>Proc. HLT-NAACL, D. M. S. Dumais and S. Roukos</editor>
		<imprint>
			<date type="published" when="2004">May 2-7, 2004</date>
			<biblScope unit="page" from="201" to="208" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,66.00,195.43,223.15,7.01;8,66.00,204.37,223.19,7.01;8,66.00,213.37,50.22,7.01" xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_Pa685NS">Improving automatic emotion recognition from speech via gender differentiation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>André</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wMB5Ht2">Proc. LREC</title>
				<meeting>LREC<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1123" to="1126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,66.00,222.31,223.18,7.01;8,66.00,231.31,223.18,7.01;8,66.00,240.25,143.47,7.01" xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_guCWUAz">Speech emotion classification using machine learning algorithms</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Casale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Scebba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Serrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_d3as6ve">Proc. Int. Conf. Semantic Computing</title>
				<meeting>Int. Conf. Semantic Computing</meeting>
		<imprint>
			<date type="published" when="2008-08">Aug. 2008</date>
			<biblScope unit="page" from="158" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,66.00,249.25,223.19,7.01;8,66.00,258.19,223.18,7.01;8,66.00,267.13,129.05,7.01" xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_FgfafCT">Challenges in real-life emotion annotation and machine learning based detection</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Vidrascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NeEukjY">Neural Network</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="407" to="422" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,66.00,276.13,223.12,7.01;8,66.00,285.07,223.16,7.01;8,66.00,294.07,173.93,7.01" xml:id="b25">
	<monogr>
		<title level="m" type="main" xml:id="_R4A2QqW">A database of german emotional speech</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paeschke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rolfes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Sendlmeier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Weiss</surname></persName>
		</author>
		<editor>Proc. Interspeech, D. M. S. Dumais and S. Roukos</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1517" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,66.00,303.01,223.19,7.01;8,66.00,312.01,223.18,7.01;8,66.00,320.95,190.91,7.01" xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_BfTpfC6">Looking-in and looking-out of a vehicle: Computer-vision-based enhanced vehicle safety</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mccall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_uq2rNKZ">IEEE Trans. Intell. Transportation Syst</title>
		<imprint>
			<biblScope unit="page" from="108" to="120" />
			<date type="published" when="2007-03">Mar. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,66.00,329.95,223.16,7.01;8,66.00,338.89,223.20,7.01;8,66.00,347.83,215.20,7.01" xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_KwCGEHB">Holistic sensing and active displays for intelligent driver support systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Y</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jnsXnpY">IEEE Computer, Special Issue on Human-Centered Computing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,66.00,356.83,223.20,7.01;8,66.00,365.77,223.18,7.01;8,66.00,374.77,136.85,7.01" xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_mKUsjHA">Role of head pose estimation in speech acquisition from distant microphones</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Shivappa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_EWJBYKV">Proc. IEEE ICASSP</title>
				<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2009-04">Apr. 2009</date>
			<biblScope unit="page" from="3557" to="3560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,66.00,383.71,223.19,7.01;8,66.00,392.71,223.16,7.01;8,66.00,401.65,143.87,7.01" xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_RbT37ZF">Dynamic context capture and distributed video arrays for intelligent spaces</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Mikic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vJvKvAf">IEEE Trans. Syst., Man, Cybern. A</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="145" to="163" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,66.00,410.65,223.17,7.01;8,66.00,419.59,223.13,7.01;8,66.00,428.53,99.19,7.01" xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_XA2A3WN">Speech based emotion classification framework for driver assistance system</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tawari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_yrwtf9u">Proc. IEEE Intell. Vehicles Symp</title>
				<meeting>IEEE Intell. Vehicles Symp</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="174" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,66.00,437.53,223.14,7.01;8,66.00,446.47,151.49,7.01" xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_vRJCtwC">Speech emotion analysis in noisy real world environment</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tawari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_zACVzyB">Proc. 20th ICPR</title>
				<meeting>20th ICPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,66.00,455.47,223.17,7.01;8,66.00,464.41,223.18,7.01;8,66.00,473.41,193.93,7.01" xml:id="b32">
	<monogr>
		<title level="m" type="main" xml:id="_7SF67p6">Accurate Short-Term Analysis of the Fundamental Frequency and the Harmonics-to-Noise Ratio of a Sampled Sound</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Paul</surname></persName>
		</author>
		<ptr target="Available:citeseer.ist.psu.edu/boersma93accurate.html" />
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct coords="8,66.00,482.36,223.17,7.01;8,66.00,491.30,223.17,7.01;8,66.00,500.29,185.82,7.01" xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_mGuxjx9">Exploring the benefits of discretization of acoustic features for speech emotion recognition</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>André</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_eSPZMhZ">Proc. 10th Conf. INTERSPEECH</title>
				<meeting>10th Conf. INTERSPEECH<address><addrLine>Brighton, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">Sep. 2009</date>
			<biblScope unit="page" from="328" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,66.00,509.23,223.20,7.01;8,66.00,518.24,223.16,7.01;8,66.00,527.25,223.15,6.84;8,66.00,536.18,115.93,7.01" xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_ZA6aVdv">Speech-driven automatic facial expression synthesis</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bozkurt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Erzin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ozkan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tekalp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_5WFykxm">Proc. 3DTV Conference: The True Vision-Capture, Transmission and Display of 3-D Video</title>
				<meeting>3DTV Conference: The True Vision-Capture, Transmission and Display of 3-D Video</meeting>
		<imprint>
			<date type="published" when="2008-05">May 2008</date>
			<biblScope unit="page" from="273" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,329.04,69.90,223.15,7.01;8,329.04,78.84,223.12,7.01;8,329.04,87.84,119.69,7.01" xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_tygP5JF">Audio-visual information fusion in human computer interfaces and intelligent environments: A survey</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shivappa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_bbZFe8b">Proc. IEEE</title>
				<meeting>IEEE</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
