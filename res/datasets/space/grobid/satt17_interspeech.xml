<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_QKs7uKT">Efficient Emotion Recognition from Speech Using Deep Learning on Spectrograms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,186.95,131.13,57.67,10.52"><forename type="first">Aharon</forename><surname>Satt</surname></persName>
							<email>aharonsa@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research-Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,254.75,131.13,75.47,10.52"><forename type="first">Shai</forename><surname>Rozenberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research-Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Technion-Israel Institute of Technology</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,348.37,131.13,52.97,10.52"><forename type="first">Ron</forename><surname>Hoory</surname></persName>
							<email>hoory@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research-Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_GeP4GtT">Efficient Emotion Recognition from Speech Using Deep Learning on Spectrograms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6842111F783A44227AA83A5C0DBCB1A5</idno>
					<idno type="DOI">10.21437/Interspeech.2017-200</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-05-12T20:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_tgcQrBD">Speech Emotion Recognition</term>
					<term xml:id="_tqe2kJn">Para-lingual</term>
					<term xml:id="_X8jPfFk">Deep Neural Network</term>
					<term xml:id="_jGCmkPc">Spectrogram</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_APAFSEA"><p xml:id="_XKBafNM">We present a new implementation of emotion recognition from the para-lingual information in the speech, based on a deep neural network, applied directly to spectrograms. This new method achieves higher recognition accuracy compared to previously published results, while also limiting the latency. It processes the speech input in smaller segments -up to 3 seconds, and splits a longer input into non-overlapping parts to reduce the prediction latency. The deep network comprises common neural network toolsconvolutional and recurrent networks -which are shown to effectively learn the information that represents emotions directly from spectrograms. Convolution-only lowercomplexity deep network achieves a prediction accuracy of 66% over four emotions (tested on IEMOCAP -a common evaluation corpus), while a combined convolution-LSTM higher-complexity model achieves 68%. The use of spectrograms in the role of speech-representing features enables effective handling of background non-speech signals such as music (excl. singing) and crowd noise, even at noise levels comparable with the speech signal levels. Using harmonic modeling to remove non-speech components from the spectrogram, we demonstrate significant improvement of the emotion recognition accuracy in the presence of unknown background non-speech signals.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1." xml:id="_yGTM7fn">Introduction</head><p xml:id="_EZtc3a6">Emotion recognition solutions are becoming one of the latest trends in the global IT market <ref type="bibr" coords="1,172.46,550.41,9.79,7.89" target="#b0">[1]</ref>. These advanced solutions capture and analyze the human emotions from multiple sources, where the human voice serves as one of the major ones. Emotion recognition capabilities are required to support natural and efficient human-computer interaction, generate marketing insights, help discovering entertainment content across large repositories and playlists, enable effective elearning through e-tutors, and many more <ref type="bibr" coords="1,208.83,622.93,6.79,7.89" target="#b0">[1]</ref><ref type="bibr" coords="1,215.62,622.93,3.39,7.89" target="#b1">[2]</ref><ref type="bibr" coords="1,219.01,622.93,6.79,7.89" target="#b2">[3]</ref>. Emotion recognition from the para-lingual information in the speech has gone through a significant improvement over the recent years, with the introduction of deep neural networks. Yet many challenges still hold, such as the need to keep improving the recognition accuracy, reducing its latency and enhancing its robustness to background sounds -from music to everyday noises. Human-computer interaction, for example, requires low-latency emotion recognition, possibly with everyday background noise, to support a dialog on the fly. Media and entertainment content discovery requires handling of speech with music and other media-related background sounds.</p><p xml:id="_qDez9Za">In this work we present our approach for dealing with these challenges. We follow the recent success of applying deep learning methods directly to spectrograms, across different areas of speech processing, to design our solution. This paper is organized as follows: the relevant related work is surveyed in section 2; the proposed solution is described in section 3, and its evaluation results are presented at section 4; section 5 describes as extension to handle effectively loud background signals; finally, section 6 presents the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2." xml:id="_XG56E5p">Related work and evaluation dataset</head><p xml:id="_hF5SC27">Emotion recognition from the para-lingual component of the speech has been an active research area for decades <ref type="bibr" coords="1,518.17,368.35,6.87,7.89" target="#b2">[3]</ref><ref type="bibr" coords="1,525.04,368.35,3.43,7.89" target="#b3">[4]</ref><ref type="bibr" coords="1,525.04,368.35,3.43,7.89" target="#b4">[5]</ref><ref type="bibr" coords="1,525.04,368.35,3.43,7.89" target="#b5">[6]</ref><ref type="bibr" coords="1,525.04,368.35,3.43,7.89" target="#b6">[7]</ref><ref type="bibr" coords="1,528.48,368.35,6.87,7.89" target="#b7">[8]</ref>. Traditional methods were based on short-time frame-level feature extraction, followed by utterance-level information extraction, and classification or regression as required <ref type="bibr" coords="1,508.69,399.43,6.82,7.89" target="#b2">[3]</ref><ref type="bibr" coords="1,515.51,399.43,3.41,7.89" target="#b3">[4]</ref><ref type="bibr" coords="1,515.51,399.43,3.41,7.89" target="#b4">[5]</ref><ref type="bibr" coords="1,515.51,399.43,3.41,7.89" target="#b5">[6]</ref><ref type="bibr" coords="1,515.51,399.43,3.41,7.89" target="#b6">[7]</ref><ref type="bibr" coords="1,518.92,399.43,6.82,7.89" target="#b7">[8]</ref>. In the recent years, deep learning methodologies and tools have been introduced to this area, used for feature extraction, classification/regression, or both <ref type="bibr" coords="1,431.52,430.54,7.08,7.89" target="#b8">[9]</ref><ref type="bibr" coords="1,438.60,430.54,3.54,7.89" target="#b9">[10]</ref><ref type="bibr" coords="1,438.60,430.54,3.54,7.89" target="#b10">[11]</ref><ref type="bibr" coords="1,438.60,430.54,3.54,7.89" target="#b11">[12]</ref><ref type="bibr" coords="1,438.60,430.54,3.54,7.89" target="#b12">[13]</ref><ref type="bibr" coords="1,442.14,430.54,10.62,7.89" target="#b13">[14]</ref>. Evaluation and comparison of different solutions is challenging, as no sufficiently comprehensive labeled public corpus of emotional speech is yet available <ref type="bibr" coords="1,475.19,464.62,6.91,7.89" target="#b2">[3]</ref><ref type="bibr" coords="1,482.10,464.62,3.45,7.89" target="#b3">[4]</ref><ref type="bibr" coords="1,482.10,464.62,3.45,7.89" target="#b4">[5]</ref><ref type="bibr" coords="1,482.10,464.62,3.45,7.89" target="#b5">[6]</ref><ref type="bibr" coords="1,482.10,464.62,3.45,7.89" target="#b6">[7]</ref><ref type="bibr" coords="1,485.55,464.62,6.91,7.89" target="#b7">[8]</ref>. One of the best available corpora is IEMOCAP <ref type="bibr" coords="1,460.13,474.94,13.90,7.89" target="#b14">[15]</ref>. It contains a relatively large amount of data, labeled down to the single sentences. State of the art results have been published using this corpus <ref type="bibr" coords="1,355.83,506.03,7.14,7.89" target="#b8">[9]</ref><ref type="bibr" coords="1,366.54,506.03,10.71,7.89" target="#b9">[10]</ref>. For this reason, we chose the IEMOCAP dataset for the evaluation of our proposed system. Deep learning has contributed to breakthroughs across multiple areas of multimedia, including speech processing <ref type="bibr" coords="1,311.89,550.43,11.01,7.89" target="#b15">[16]</ref><ref type="bibr" coords="1,326.57,550.43,11.01,7.89" target="#b16">[17]</ref>. Researchers have shown that replacing hand-crafted low-level (frame-level) features with statistical learning by the different layers of the deep network can significantly enhance the accuracy of detectors and regression solutions. In speech recognition, the shift back from MFCC representation to Melscale spectrograms <ref type="bibr" coords="1,383.73,602.19,14.44,7.89" target="#b15">[16,</ref><ref type="bibr" coords="1,401.20,602.19,11.87,7.89" target="#b17">18]</ref> led to a reduced error. Direct use of Mel-scale spectrograms for speaker recognition was proved successful as well <ref type="bibr" coords="1,378.81,622.84,13.90,7.89" target="#b18">[19]</ref>. In <ref type="bibr" coords="1,408.18,622.84,11.56,7.89" target="#b19">[20]</ref><ref type="bibr" coords="1,423.59,622.84,11.56,7.89" target="#b20">[21]</ref> a recently published state of the art robust speech recognition system is described based on linearly-spaced spectrograms. In the present work we follow this path. For speech recognition, Mel-scale spectrograms that eliminate part of the pitch information, can give rise to a superior solution. Conversely, emotions are strongly manifested in the pitch information; hence we turned to design our system using linearly-spaced spectrograms, which represent the fine harmonics structure of the speech. A recent publication <ref type="bibr" coords="1,523.66,718.98,15.08,7.89" target="#b21">[22]</ref> demonstrates the use of such spectrograms to remove background music and deliver the clean speech. Our aim is to design a high-accuracy emotion recognition system from speech, while limiting its latency and enhancing its robustness to background sound signals, in particular loud signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3." xml:id="_ffhGFx3">The proposed system</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1." xml:id="_paCTtp8">Speech input processing and spectrogram calculation</head><p xml:id="_qsEHh4X">As described above, we calculate spectrograms from the speech signal and apply deep learning directly to the spectrograms. The speech signal in the IEMOCAP corpus <ref type="bibr" coords="2,268.36,183.39,14.99,7.89" target="#b14">[15]</ref> is sampled at 16KHz and organized as single sentences with durations from less than a second to about 20 seconds. Each sentence is labeled with one emotion. As the first step, we split each sentence longer than T=3 seconds to shorter sub-sentences of approximately equal lengths, not longer than T=3 seconds. Each sub-sentence is assigned the emotion labeling of the corresponding whole sentence. These shorter sentences are used throughout the proposed system, where only during the testing phase we evaluate the prediction for the whole sentences by averaging the posterior probabilities of the respective sub-sentences. While we lose some accuracy in this process, our aim is to propose a system that limits the prediction latency. Next, we calculate a spectrogram for each (shorter) sentence. A sequence of overlapping Hamming windows is applied to the speech signal, with frame size (window shift) of 10msec, and window size of either 20msec or 40msec. For each frame we calculate a DFT of length 800 (for 20Hz grid resolution) or 1600 (for 10Hz grid resolution). We use the frequency range of 0-4KHz, ignoring the rest. Following aggregation of the short-time spectra, we obtain a matrix of size NxM, where N&lt;=300 according to the speech sentence length, and M=200 or 400 according to the selected frequency grid resolution. Next, we implement a normalization step: the DFT data is converted to log-power-spectrum, expressed in dB; it is then limited from below by the constant E noise that was determined empirically to represent a universal noise level; the resulting log-spectrum was lifted to be non-negative by subtracting the constant E noise , and then normalized to bring its non-zero data points to a unity variance. The last step in calculating the log-spectrogram is zeropadding to get 300 time points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2." xml:id="_nmHQDW5">The deep neural network</head><p xml:id="_5dHNujf">We chose to evaluate two types of neural networks: convolutional networks and recurrent networks, where the latter refers to an LSTM -Long Short Term Memory networks. Figure <ref type="figure" coords="2,119.78,593.39,4.50,7.89">1</ref> depicts an example of the deep network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_USm4uRP">Figure 1: Example of the deep network topology</head><p xml:id="_C3gM62U">We hypothesized that the convolutional networks, capable of learning spatial patterns, will learn effectively spatial spectrogram patterns that represent the emotional information. We provide visual insights for this in the next section. We also hypothesized that adding an LSTM layer will help learning the temporal behavior across the sentence being represented by the spectrogram. This hypothesis is supported by the improved accuracy as presented in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4." xml:id="_nzdBEsT">Experimental setup and evaluation</head><p xml:id="_Au2gXgM">The IEMOCAP corpus comprises five sessions; each session includes labeled emotional speech sentences from recordings of dialogs between two persons. There is no speaker overlapping between different sessions. We used this setup for running a five-fold cross validation. In each fold, the data from four sessions is used for training the deep neural network, and the data from the remaining session is split -one speaker for validation and the other for the accuracy testing. The IEMOCAP corpus contains scripted and improvised dialogs. As the script text exhibits strong correlation with the labeled emotions, it may give rise to lingual content learning, at least partially, which is an undesired side effect. Therefore we used the improvised data only. We used two common evaluation criteria:</p><p xml:id="_CqTRBCp">1. Overall accuracy -where each sentence across the dataset has an equal weight, AKA weighted accuracy; 2. Class accuracy -the accuracy is first evaluated for each emotion and then averaged, AKA unweighted accuracy. For the sake of comparison to <ref type="bibr" coords="2,442.92,388.98,13.90,7.89" target="#b9">[10]</ref>, the following four emotions were used: Anger, Happiness, Neutral and Sadness. We tested dozens of combinations of topologies and parameters. We evaluated convolution-only topologies, ranging from two to eight layers, with different combinations of time windows sizes and frequency grid resolutions. We also evaluated topologies with one to six convolution layers and with one and two LSTM layers. The following table summarizes the best topologies, convolution-only and convolution with LSTM. With respect to Table <ref type="table" coords="2,407.15,689.03,4.50,7.89" target="#tab_0">1</ref> above, we used the following parameters:</p><p xml:id="_Sq58zYV">1. The window size was set to 40msec; a 20msec window yielded similar results, lower by 0-2% across the different topologies;</p><p xml:id="_BCK8pw8">2. The bi-directional LSTM contained 128x2 nodes; using 64x2 nodes, the accuracy drops by 1-3%; 3. The frequency grid resolution was set to 10Hz; lower resolution (20Hz) yields lower accuracy by 1-3%; 4. The best topology for convolution-only network was found to include 5 layers (we tried 2-8 layers), whereas the best mixed-topology was found to include 3 convolution layers and a single LSTM layer (we tried 1-6 convolution layers and 1-2 LSTM layers); 5. The deep network was optimized to maximize the overall accuracy (this is discussed below). The published state of the art accuracy using the IEMOCAP corpus, to our knowledge, is given in <ref type="bibr" coords="3,195.88,209.31,14.02,7.89" target="#b9">[10]</ref>, based on the same evaluation setup as we used; it reports 63.9% and 62.8% for the overall accuracy and the class accuracy, respectively. It should be noted that <ref type="bibr" coords="3,137.78,240.27,15.06,7.89" target="#b9">[10]</ref> and other works present accuracy results based on the whole speech sentences; conversely, we split the sentences into shorter sub-sentences of T&lt;=3 seconds, demonstrating the accuracy under limited latency constraint. The IEMOCAP corpus is significantly unbalanced; to cope with the unbalanced data we tried the following techniques:</p><p xml:id="_GjebNT9">1. Training the network to maximize the class accuracy rather than the overall accuracy AE the penalty on the overall accuracy makes it less useful; 2. Assigning different weights to the stochastic gradient, in inverse proportion to the class size AE it improved both the overall and the class accuracies by 1-3%; 3. Applying statistical oversampling to get equal-sized training classes AE increased the smallest class accuracy (happiness), but not the overall and class-accuracies. We also tried a two-step prediction, based on:</p><p xml:id="_ABtyUwu">1. A four-class predictor as in Table <ref type="table" coords="3,201.78,423.93,4.23,7.89" target="#tab_0">1</ref>/row 3; 2. Three two-class predictors, which classify the majority class (neutral) against each of the other three classes; they use convolution layers as in Table <ref type="table" coords="3,221.37,458.01,3.53,7.89" target="#tab_0">1</ref>/ row 1. The two-step prediction process proceeds as follows: first, the test sample is run through the four-class predictor; if the higher probability is assigned to a non-majority class (nonneutral), then this class is selected; otherwise, the test sample is run through the three two-class predictors, and the predicted emotion is selected to maximize the posterior probability across the three predictors. The obtained accuracy using this two-step procedure is shown in Table <ref type="table" coords="3,198.62,543.81,3.44,7.89" target="#tab_0">1</ref>/line 5, demonstrating higher class-accuracy. A heuristic explanation for the success of this method (special emphasis on the neutral class) could be due to the fact that significant parts of a typical non-neutral sentence tend to be neutral, whereas the emotional (nonneutral) nature is typically manifested in the smaller parts.     To further enhance the recognition accuracy of the proposed solution, we tried to add a unidimensional attention mechanism to the LSTM layer. Our motivation, based on the success of bi-dimensional attention mechanisms in object recognition from images <ref type="bibr" coords="3,415.04,409.15,11.01,7.89" target="#b23">[25]</ref><ref type="bibr" coords="3,429.72,409.15,11.01,7.89" target="#b24">[26]</ref>, was to find the time segments of the speech signal that are relevant to emotion recognition. Unfortunately, we have not gained any improvement of accuracy, concluding that in our case the convolution and LSTM layers seem to detect the relevant time segments effectively from the log-spectrogram, by themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_DjGR4FP">It is informative to examine what the deep network learns, by looking at the activations of the convolution layers. The Figures below show the</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5." xml:id="_uQX4HbP">Emotion recognition in noise conditions</head><p xml:id="_VXxHVDK">As discussed in section 1, several key use cases require the capability of emotion recognition from speech in the presence of background signals, such as music or crowd noise. The magnitude of these background signals can be close to or the same as the magnitude of the speech itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1." xml:id="_tTW6Ryn">Background</head><p xml:id="_nHEpZSB">A common way to filter out the unwanted signals (noise) and retain the speech information is based on de-noising autoencoders <ref type="bibr" coords="3,350.57,597.83,14.35,7.89" target="#b25">[27,</ref><ref type="bibr" coords="3,371.53,597.83,10.74,7.89" target="#b21">22]</ref>. This method is useful if the noise characteristics are known, or it can be represented by a large enough set of samples. In the current work, however, we chose to demonstrate a different method: removing the noise from the logspectrogram itself, as a pre-processing step. Its significant advantage stems from the fact that no prior knowledge about the noise signal is needed, other than one constraint: it should not be a short-time harmonic signal, in the sense of exhibiting a single dominant short-time fundamental frequency within the pitch frequency range of humans. In addition, we chose to demonstrate a method capable of handling high noise levels, as high as 0dB signal to noise ratio. Emotion recognition from speech under lower noise levels was demonstrated at <ref type="bibr" coords="4,132.30,83.43,9.79,7.89" target="#b6">[7]</ref>: Gaussian noise with SNR = -20dB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2." xml:id="_XTajbjG">The noise filtering</head><p xml:id="_TZZZvQp">We base our method on the short-time harmonic nature of the voiced speech. We made an assumption that the emotion information is contained mainly in the voiced parts of the speech. It is an approximation, but a useful assumption as shown below. First, we use a common open-source pitch detector <ref type="bibr" coords="4,244.52,173.91,11.00,7.89" target="#b22">[23]</ref><ref type="bibr" coords="4,259.18,173.91,11.00,7.89">[24]</ref>, to evaluate the pitch frequency of the speech, per frame. Then, for each voiced frame we generate a modified log-powerspectrum, which empirically approximates the real one:</p><formula xml:id="formula_0" coords="4,56.60,220.52,170.04,9.60">(1) S(f) = E(f) -0.5•(1 -cos(2πf / F 0 ))•D(f)</formula><p xml:id="_h9PKsQg">Where S(f) is the modified short-time log-power-spectrum, E(f) is the short-time spectral envelope, F 0 is the pitch frequency, D(f) is linear from 20dB @0Hz to 12dB @4KHz, f is the frequency, 0 &lt;= f &lt;= 4KHz. Unvoiced frames are regarded as silence. Figure <ref type="figure" coords="4,238.66,304.38,4.50,7.89" target="#fig_6">4</ref> depicts an example of the modified log-power-spectrum. In Figure <ref type="figure" coords="4,101.34,480.92,4.50,7.89" target="#fig_6">4</ref> it is seen that the harmonic structure is approximately preserved, while any non-harmonic component would be largely eliminated. By replacing the normal logpower-spectra with their corresponding modified versions, we get the modified log-spectrogram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3." xml:id="_QyFje2U">Evaluation</head><p xml:id="_hwtxbWc">For evaluating the noise immunity, we used seven different noise signals:</p><p xml:id="_btmBCuf">x Three music signals: passages from ambient, pop and rock compositions; all excluding singing; x Four crowd noises: crowd speech, crowd anger sound, crowd applause and crowd laughter. For testing we kept one fold (IEMOCAP session) aside, and trained two predictors based on the remaining four sessions:</p><p xml:id="_EuBTkEz">1. The normal predictor, based on 3 convolution layers and LSTM as listed in As seen in Table <ref type="table" coords="4,384.48,273.18,3.40,7.89" target="#tab_2">2</ref>, the spectrogram modification process takes its toll on the accuracy -comparing rows 1 and 2.</p><p xml:id="_jWFgQxh">The value of the noise filtering becomes apparent when testing the noisy speech: comparing rows 3 and 4, the normal predictor's accuracy collapses, whereas the impact on the modified predictor's accuracy is relatively minor. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6." xml:id="_ZXKSzmn">Conclusions and discussion</head><p xml:id="_V5gAT6m">We demonstrated an emotion recognition system from speech, under limited latency constraint (&lt;=3 seconds), which achieves beyond the state of the art accuracy on the common benchmarking dataset IEMOACP, comparing with previous works without latency constraints: one of the tested network topologies achieves 67.3% and 62.0% vs. previous work -63.9% and 62.8%, for overall-and class-accuracy, respectively. The system is based on an end-to-end deep neural network, applied directly to raw spectrograms without a feature extraction step. Using raw spectrograms enables us to easily combine a noise reduction solution based on harmonic filtering, which can handle high noise levels such as SNR=0dB -we demonstrated robustness to this level in the case of background non-speech noise sounds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,146.50,632.64,136.77,7.89;3,56.61,642.96,213.55,7.89"><head></head><label></label><figDesc xml:id="_Bugfp9e">activations of select filters at the first convolution layer, from a speech sample labeled as neutral.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,56.61,659.28,215.91,8.21;3,56.61,669.72,210.33,7.89;3,56.61,680.04,220.70,8.21;3,56.61,690.36,219.53,7.89;3,56.61,700.68,221.82,7.89;3,56.61,711.12,203.68,7.89;3,56.61,721.44,223.70,7.89"><head>Figure 2 -</head><label>2</label><figDesc xml:id="_QvEwxE3">Figure 2 -the left side -shows the original normalized logspectrogram. The horizontal axis denotes the time, and the vertical -the frequency.Figure 2 -the right side -shows the activation of one of the filters. Reddish colors designate high activation, and blueish colors low activation. It is clearly seen that this filter tends to learn vertical and close-to vertical patterns of the fine harmonic structure in the log-spectrogram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,56.61,659.28,215.91,8.21;3,56.61,669.72,210.33,7.89;3,56.61,680.04,220.70,8.21;3,56.61,690.36,219.53,7.89;3,56.61,700.68,221.82,7.89;3,56.61,711.12,203.68,7.89;3,56.61,721.44,223.70,7.89"><head>Figure 2 -</head><label>2</label><figDesc xml:id="_7Fzbspn">Figure 2 -the left side -shows the original normalized logspectrogram. The horizontal axis denotes the time, and the vertical -the frequency.Figure 2 -the right side -shows the activation of one of the filters. Reddish colors designate high activation, and blueish colors low activation. It is clearly seen that this filter tends to learn vertical and close-to vertical patterns of the fine harmonic structure in the log-spectrogram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,333.49,256.43,183.74,8.10;3,311.89,272.94,226.81,8.21;3,311.89,283.26,226.88,7.89;3,311.89,293.58,119.22,7.89"><head>Figure 3 :Figure 3 -</head><label>33</label><figDesc xml:id="_eVk8qqn">Figure 3: Left and right -activations learn patterns Figure 3 -the left side -shows the activation of another filter, which clearly tends to learn horizontal and close-to horizontal patters of the harmonic structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="3,311.89,310.02,226.87,8.21;3,311.89,320.34,226.84,7.89;3,311.89,330.66,226.95,7.89;3,311.89,340.98,226.54,7.89;3,311.89,351.30,161.37,7.89;3,311.39,166.93,229.57,81.79"><head>Figure 3 -</head><label>3</label><figDesc xml:id="_sUdYpvK">Figure 3 -the right side -demonstrates a filter that tends to learn the less-relevant areas of the spectrogram, including silence and low-energy zones. This activation explains how the deep network is capable of separating the relevant parts of the spectrogram from the less-relevant areas.</figDesc><graphic coords="3,311.39,166.93,229.57,81.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="3,321.49,158.72,206.07,8.10;3,311.75,70.81,112.15,81.67"><head>Figure 2 :</head><label>2</label><figDesc xml:id="_SGfEehS">Figure 2: Left: original log-spectrogram; right: activation</figDesc><graphic coords="3,311.75,70.81,112.15,81.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="4,78.56,464.41,183.18,8.10;4,56.63,325.81,226.75,133.39"><head>Figure 4 :</head><label>4</label><figDesc xml:id="_kkkXDJ2">Figure 4: Original and modified log-power-spectra</figDesc><graphic coords="4,56.63,325.81,226.75,133.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="4,336.01,489.61,178.47,8.10;4,311.75,338.95,226.75,144.91"><head>Figure 5 :</head><label>5</label><figDesc xml:id="_E64csvu">Figure 5: Left: noisy spectrograms; right: cleaned</figDesc><graphic coords="4,311.75,338.95,226.75,144.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,311.89,507.57,225.44,172.56"><head>Table 1 :</head><label>1</label><figDesc xml:id="_9EyBqzZ">Summary of accuracy evaluation results based on five-fold cross validation</figDesc><table coords="2,311.89,534.92,225.44,145.20"><row><cell>Network</cell><cell>Overall</cell><cell>Class</cell></row><row><cell></cell><cell>accuracy</cell><cell>accuracy</cell></row><row><cell>1. 5 convolution layers</cell><cell>66.1%</cell><cell>56.6%</cell></row><row><cell>10Hz grid resolution</cell><cell></cell><cell></cell></row><row><cell>2. 5 convolution layers</cell><cell>63.6%</cell><cell>53.4%</cell></row><row><cell>20 Hz grid resolution</cell><cell></cell><cell></cell></row><row><cell>3. 3 convolution layers and LSTM</cell><cell>68.8%</cell><cell>59.4%</cell></row><row><cell>10 Hz grid resolution</cell><cell></cell><cell></cell></row><row><cell>4. 3 convolution layers and LSTM</cell><cell>65.8%</cell><cell>56.6%</cell></row><row><cell>20 Hz grid resolution</cell><cell></cell><cell></cell></row><row><cell>5. Two-step predictor (refer to</cell><cell>67.3%</cell><cell>62.0%</cell></row><row><cell>explanation below)</cell><cell></cell><cell></cell></row><row><cell>10 Hz grid resolution</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,56.61,73.10,481.86,667.39"><head>Table 1 /</head><label>1</label><figDesc xml:id="_Ude4BNU">row 3; 2. A modified predictor, similar to the normal one but trained based on the modified spectrograms of the clean training data. Next, we added (separately) the seven noise signals to each of the testing speech samples. The noise level was set to be at 0dB signal to noise ratio. Finally we run the noisy samples through the two predictors: x Noisy signal AE noisy spectrogram AE normal predictor, x Noisy signal AE modified spectrogram AE modified predictor.Calculating the modified spectrograms required extracting the pitch from the noisy signals, using the open-source<ref type="bibr" coords="4,497.59,124.84,11.00,7.89" target="#b22">[23]</ref>[24]. Table2below summarizes the prediction accuracy.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,311.89,154.48,225.44,106.68"><head>Table 2 :</head><label>2</label><figDesc xml:id="_nvZ5pYR">Accuracy evaluation results for noisy speech</figDesc><table coords="4,311.89,171.51,225.44,89.64"><row><cell>Test data</cell><cell>Predictor</cell><cell>Overall</cell></row><row><cell></cell><cell></cell><cell>accuracy</cell></row><row><cell>1. Clean spectrograms</cell><cell>Normal</cell><cell>68.8%</cell></row><row><cell>2. Modified spectrograms from</cell><cell>Modified</cell><cell>64.5%</cell></row><row><cell>clean speech</cell><cell></cell><cell></cell></row><row><cell>3. Noisy spectrograms</cell><cell>Normal</cell><cell>40.1%</cell></row><row><cell>4. Modified spectrograms from</cell><cell>Modified</cell><cell>59.8%</cell></row><row><cell>noisy speech</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7." xml:id="_PnpSFF4">Acknowledgements</head><p xml:id="_n4zzU32">The authors wish to thank Mr. Izhak Golan from the Technion-Israel Institute of Technology, for his contribution for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="5,74.61,92.59,208.89,7.33;5,74.61,101.70,208.66,7.05;5,74.61,110.94,208.90,7.33;5,74.61,120.18,97.58,7.05" xml:id="b0">
	<monogr>
		<title level="m" type="main" xml:id="_Hxf8Caz">Emotion Detection and Recognition Market by Technology, Software Tool, Service, Application Area, End User, and Region -Global Forecast to 2021</title>
		<imprint>
			<date type="published" when="2016-11">November 2016</date>
			<publisher>Markets and Markets</publisher>
		</imprint>
	</monogr>
	<note>Market research report</note>
</biblStruct>

<biblStruct coords="5,74.61,129.30,208.61,7.33;5,74.61,138.54,130.31,7.33" xml:id="b1">
	<monogr>
		<title level="m" type="main" xml:id="_eG93pwa">How To Measure Emotion In Customer Experience</title>
		<imprint>
			<date type="published" when="2015-11">November 2015</date>
			<pubPlace>Forrester</pubPlace>
		</imprint>
	</monogr>
	<note>Market research report</note>
</biblStruct>

<biblStruct coords="5,74.61,147.77,208.60,7.05;5,74.61,156.89,208.94,7.05;5,74.61,166.13,208.89,7.05;5,74.61,175.37,46.28,7.05" xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_yGpSCJ2">A survey on human emotion recognition approaches, databases and applications</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Vinola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Vimaladevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_wcC8WGv">ELCVIA Electronic Letters on Computer Vision and Image Analysis</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="24" to="44" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.61,184.48,208.61,7.05;5,74.61,193.72,208.63,7.05;5,74.61,202.96,208.99,7.05;5,74.61,212.08,14.11,7.05" xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_Z8t3KwN">Survey on speech emotion recognition: Features, classification schemes, and databases</title>
		<author>
			<persName coords=""><forename type="first">El</forename><surname>Ayadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohamed</forename><forename type="middle">S</forename><surname>Moataz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fakhri</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Karray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xJJk83X">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="572" to="587" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.61,221.32,208.76,7.05;5,74.61,230.55,208.65,7.05;5,74.61,239.67,208.76,7.05;5,74.61,248.94,208.93,7.05;5,74.61,258.18,39.42,7.05" xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_D2QFTgt">Automatic speech emotion recognition: A survey</title>
		<author>
			<persName coords=""><forename type="first">Purnima</forename><surname>Chandrasekar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Santosh</forename><surname>Chapaneri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deepak</forename><surname>Jayaswal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_h5R5EYN">Circuits, Systems, Communication and Information Technology Applications (CSCITA), 2014 International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.61,267.30,208.53,7.05;5,74.61,276.53,208.44,7.05;5,74.61,285.77,193.98,7.05" xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_RMGg2zj">Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge</title>
		<author>
			<persName coords=""><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ExgjeXh">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1062" to="1087" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.61,294.89,208.52,7.05;5,74.61,304.13,208.64,7.05;5,74.61,313.37,91.20,7.05" xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_esxVncZ">Speech emotion recognition using CNN</title>
		<author>
			<persName coords=""><forename type="first">Zhengwei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_AJTzAfx">Proceedings of the 22nd ACM international conference on Multimedia</title>
				<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.61,322.48,208.69,7.05;5,74.61,331.72,208.88,7.05;5,74.61,340.96,127.14,7.05" xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_4fwjdT5">Emotion recognition from speech: a review</title>
		<author>
			<persName coords=""><forename type="first">Shashidhar</forename><forename type="middle">G</forename><surname>Koolagudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sreenivasa</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qTmn4Rx">International journal of speech technology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="99" to="117" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.61,350.08,208.65,7.05;5,74.61,359.31,208.73,7.05;5,74.61,368.55,93.79,7.05" xml:id="b8">
	<monogr>
		<title level="m" type="main" xml:id="_7qpBc2R">Speech emotion recognition using deep neural network and extreme learning machine</title>
		<author>
			<persName coords=""><forename type="first">Kun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Tashev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct coords="5,74.61,377.67,208.76,7.05;5,74.61,386.91,208.83,7.05;5,74.61,396.15,74.10,7.05" xml:id="b9">
	<monogr>
		<title level="m" type="main" xml:id="_mXfryRA">High-level feature representation using recurrent neural network for speech emotion recognition</title>
		<author>
			<persName coords=""><forename type="first">Jinkyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Tashev</surname></persName>
		</author>
		<idno>INTERSPEECH. 2015</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.61,405.26,208.52,7.05;5,74.61,414.50,208.64,7.05;5,74.61,423.76,91.17,7.05" xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_NrejMYq">Speech emotion recognition using CNN</title>
		<author>
			<persName coords=""><forename type="first">Zhengwei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_GRVYpzh">Proceedings of the 22nd ACM international conference on Multimedia</title>
				<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.61,432.88,208.73,7.05;5,74.61,442.12,208.83,7.05;5,74.61,451.36,208.74,7.05;5,74.61,460.47,202.02,7.05" xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_tsWu3t2">Emotion recognition from spontaneous speech using hidden Markov models with deep belief networks</title>
		<author>
			<persName coords=""><forename type="first">Duc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emily</forename><forename type="middle">Mower</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_EzeAftA">Automatic Speech Recognition and Understanding (ASRU)</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.62,469.71,208.81,7.05;5,74.62,478.95,208.87,7.05;5,74.62,488.07,23.33,7.05" xml:id="b12">
	<monogr>
		<title level="m" type="main" xml:id="_B2c8M3D">Emotion Classification from Noisy Speech-A Deep Learning Approach</title>
		<author>
			<persName coords=""><forename type="first">Rajib</forename><surname>Rana</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05901</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,74.62,497.30,208.64,7.05;5,74.62,506.54,208.76,7.05;5,74.62,515.66,170.14,7.05" xml:id="b13">
	<monogr>
		<title level="m" type="main" xml:id="_YpveAJd">Emotion Recognition From Speech With Recurrent Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">Vladimir</forename><surname>Chernykh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grigoriy</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pavel</forename><surname>Prihodko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08071</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,74.63,524.90,208.56,7.05;5,74.63,534.14,208.77,7.05;5,74.60,543.36,55.60,7.05" xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_Jj9zJhb">IEMOCAP: Interactive emotional dyadic motion capture database</title>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_55Ceu7b">Language resources and evaluation</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">335</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.60,552.59,208.69,7.05;5,74.60,561.71,208.85,7.05;5,74.60,570.95,132.92,7.05" xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_gs6DR6R">A tutorial survey of architectures, algorithms, and applications for deep learning</title>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8PM3dxj">APSIPA Transactions on Signal and Information Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">e2</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.60,580.19,208.57,7.05;5,74.60,589.34,208.59,7.05;5,74.60,598.57,74.31,7.05" xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_zHPy6N8">Deep learning: methods and applications</title>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6qXKfmh">Foundations and Trends® in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="197" to="387" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.60,607.81,208.93,7.05;5,74.60,616.93,208.77,7.05;5,74.60,626.17,207.01,7.05" xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_xYTZNY6">Recent advances in deep learning for speech research at Microsoft</title>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_XYcCD3x">Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,74.60,635.41,208.71,7.05;5,74.60,644.52,208.80,7.05;5,74.60,653.76,208.68,7.05;5,74.60,663.00,90.29,7.05" xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_5tGpTEQ">Deep neural networks for small footprint text-dependent speaker verification</title>
		<author>
			<persName coords=""><forename type="first">Ehsan</forename><surname>Variani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_T55JnjD">Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct coords="5,74.60,672.12,208.92,7.05;5,74.60,681.35,195.34,7.05" xml:id="b19">
	<monogr>
		<title level="m" type="main" xml:id="_ksgYYHm">Deep speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName coords=""><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,74.61,690.59,208.83,7.05;5,74.61,699.71,208.85,7.05;5,74.61,708.95,83.81,7.05" xml:id="b20">
	<monogr>
		<title level="m" type="main" xml:id="_BXv3gA5">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName coords=""><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02595</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,74.61,718.19,208.40,7.05;5,74.61,727.30,208.80,7.05;5,329.90,72.73,208.86,7.05;5,329.90,81.87,208.82,7.05;5,329.90,91.11,99.13,7.05" xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_cpUZnfh">Deep karaoke: Extracting vocals from musical mixtures using a convolutional deep neural network</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">Jr</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_GFDvGTm">International Conference on Latent Variable Analysis and Signal Separation</title>
				<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,329.90,100.34,208.60,7.05;5,329.90,109.46,169.54,7.05" xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_DaM4xHf">Praat, a system for doing phonetics by computer</title>
		<author>
			<persName coords=""><forename type="first">Paulus</forename><surname>Boersma</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Petrus Gerardus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_wk3yhV7">Glot international</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,329.90,127.94,208.46,7.05;5,329.90,137.05,208.95,7.05;5,329.90,146.29,208.93,7.05;5,329.90,155.53,62.24,7.05" xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_CkZK22Q">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName coords=""><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ernst</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8h6g9VP">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,329.90,164.65,208.78,7.05;5,329.90,173.89,183.33,7.05" xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_aky7nrR">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName coords=""><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_f3p53AS">ICML</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,329.90,183.12,208.73,7.05;5,329.90,192.24,208.66,7.05;5,329.90,201.48,208.75,7.05;5,329.91,210.72,87.19,7.05" xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_qbqFvXn">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName coords=""><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MFWMRsQ">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12">Dec (2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
