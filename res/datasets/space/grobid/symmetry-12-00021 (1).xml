<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_aPQZVvZ">Emotion Classification Based on Biophysical Signals and Machine Learning Techniques</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-20">20 December 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,76.54,170.99,52.85,9.34"><forename type="first">Oana</forename><surname>Bălan</surname></persName>
							<email>oana.balan@cs.pub.ro</email>
							<idno type="ORCID">0000-0002-6822-2684</idno>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Automatic Control and Computers</orgName>
								<orgName type="institution">University POLITEHNICA of Bucharest</orgName>
								<address>
									<postCode>060042</postCode>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,157.03,170.99,70.56,9.34"><forename type="first">Gabriela</forename><surname>Moise</surname></persName>
							<email>gmoise@upg-ploiesti.ro</email>
							<idno type="ORCID">0000-0002-3842-9828</idno>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Information Technology, Mathematics and Physics (ITIMF)</orgName>
								<orgName type="institution">Petroleum-Gas University of Ploiesti</orgName>
								<address>
									<postCode>100680</postCode>
									<settlement>Ploiesti</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,248.41,170.99,63.91,9.34"><forename type="first">Livia</forename><surname>Petrescu</surname></persName>
							<email>livia.petrescu@bio.unibuc.ro</email>
							<idno type="ORCID">0000-0001-9264-1603</idno>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Biology</orgName>
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<postCode>030014</postCode>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,333.15,170.99,81.09,9.34"><forename type="first">Alin</forename><surname>Moldoveanu</surname></persName>
							<email>alin.moldoveanu@cs.pub.ro</email>
							<idno type="ORCID">0000-0002-1368-7249</idno>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Automatic Control and Computers</orgName>
								<orgName type="institution">University POLITEHNICA of Bucharest</orgName>
								<address>
									<postCode>060042</postCode>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,76.54,184.50,83.85,9.34"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
							<email>marius.leordeanu@cs.pub.ro</email>
							<idno type="ORCID">0000-0002-1368-7249</idno>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Automatic Control and Computers</orgName>
								<orgName type="institution">University POLITEHNICA of Bucharest</orgName>
								<address>
									<postCode>060042</postCode>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,189.29,184.50,91.61,9.34"><forename type="first">Florica</forename><surname>Moldoveanu</surname></persName>
							<email>florica.moldoveanu@cs.pub.ro</email>
							<idno type="ORCID">0000-0002-8357-5840</idno>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Automatic Control and Computers</orgName>
								<orgName type="institution">University POLITEHNICA of Bucharest</orgName>
								<address>
									<postCode>060042</postCode>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_mQKxmqT">Emotion Classification Based on Biophysical Signals and Machine Learning Techniques</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-20">20 December 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">0D43510C22B02AE0C7EE6F78C883B606</idno>
					<idno type="DOI">10.3390/sym12010021</idno>
					<note type="submission">Received: 12 November 2019; Accepted: 18 December 2019;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-05-12T20:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_YNPmEs9">emotion classification</term>
					<term xml:id="_v6fdw8f">machine learning</term>
					<term xml:id="_Jmb2rsh">feature selection</term>
					<term xml:id="_JdHXp6A">affective computing</term>
					<term xml:id="_T6Wdd4f">biophysical sensors</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_pburfzQ"><p xml:id="_bxny8YM">Emotions constitute an indispensable component of our everyday life. They consist of conscious mental reactions towards objects or situations and are associated with various physiological, behavioral, and cognitive changes. In this paper, we propose a comparative analysis between different machine learning and deep learning techniques, with and without feature selection, for binarily classifying the six basic emotions, namely anger, disgust, fear, joy, sadness, and surprise, into two symmetrical categorical classes (emotion and no emotion), using the physiological recordings and subjective ratings of valence, arousal, and dominance from the DEAP (Dataset for Emotion Analysis using EEG, Physiological and Video Signals) database. The results showed that the maximum classification accuracies for each emotion were: anger: 98.02%, joy:100%, surprise: 96%, disgust: 95%, fear: 90.75%, and sadness: 90.08%. In the case of four emotions (anger, disgust, fear, and sadness), the classification accuracies were higher without feature selection. Our approach to emotion classification has future applicability in the field of affective computing, which includes all the methods used for the automatic assessment of emotions and their applications in healthcare, education, marketing, website personalization, recommender systems, video games, and social media.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1." xml:id="_Bg5sGTn">Introduction</head><p xml:id="_ttjaFw3">Emotions influence our quality of life and how we interact with others. They determine the thoughts we have, the actions we take, subjective perceptions of the world, and our behavioral responses. According to Scherer's theory <ref type="bibr" coords="1,257.69,638.79,10.50,9.58" target="#b0">[1]</ref>, emotions consist of five synchronized processes, namely cognitive appraisal, bodily symptoms (physiological reactions in the central and autonomic nervous systems), action tendencies (the motivational component that determines us to react or take action), facial or vocal expressions, and feelings (inner experiences, unique for each person apart). Affective computing is the study of systems or devices that can identify, process, and simulate emotions. This domain has applicability in education, medicine, social sciences, entertainment, and so on. The purpose of affective computing is to improve user experience and quality of life, and this is why various emotion models have been proposed over the years and efficient mathematical models applied in order to extract, classify, and analyze emotions.</p><p xml:id="_S5sCFFb">In the 1970s, Paul Ekman identified six basic emotions <ref type="bibr" coords="2,331.42,91.07,10.45,9.58" target="#b1">[2]</ref>, namely anger, disgust, fear, joy, sadness, and surprise. Russell and Mehrabian proposed a dimensional approach <ref type="bibr" coords="2,407.60,104.58,11.74,9.58" target="#b2">[3]</ref> which states that any emotion is represented relative to three fundamental dimensions, namely valence (positive/pleasurable or negative/unpleasurable), arousal (engaged or not engaged), and dominance (degree of control that a person has over their affective states).</p><p xml:id="_8bfdCMQ">Joy or happiness is a pleasant emotional state, synonymous with contentment, satisfaction and well-being. Sadness is the opposite of happiness, being characterized by grief, disappointment, and distress. Fear emerges in the presence of a stressful or dangerous stimulus perceived by the sensory organs. When the fight or flight response appears, heart rate and respiration rate increase. Also, the muscles become more tense in order to contend with threats in the environment. Anger is defined by fury, frustration, and resentment towards others. Surprise is triggered by an unexpected outcome to a situation, ranging from amazement to shock, whereas disgust is synonymous with dislike, distaste, or repugnance, being the most visceral of all six emotions.</p><p xml:id="_UJT6Jx3">The DEAP database <ref type="bibr" coords="2,184.43,266.70,11.49,9.58">[4]</ref> was created with the purpose of developing a music video recommendation system based on the users' emotional responses. The biophysical signals of 32 subjects have been recorded while they were watching 40 one-minute long excerpts of music videos eliciting various emotions. The participants rated each video in terms of valence, arousal, dominance, like/dislike and familiarity on a scale from one to nine. The physiological signals were: galvanic skin response (GSR), plethysmograph (PPG), skin temperature, breathing rate, electromyogram (EMG), and electroencephalography (EEG) from 32 electrodes, decomposed into frequency ranges (theta, slow alpha, alpha, beta, and gamma) and the differences between the spectral power of all symmetrical pairs of electrodes on the left and right brain hemispheres. Other well-known databases are MAHNOB, SEED, and eNTERFACE06_EMOBRAIN. The MAHNOB database <ref type="bibr" coords="2,335.92,388.28,11.52,9.58" target="#b4">[5]</ref> contains the physiological signals of 27 subjects in response to 20 stimulus videos who rated arousal, valence, dominance and predictability on a scale from one to nine. The SEED database <ref type="bibr" coords="2,271.13,415.30,11.48,9.58" target="#b5">[6]</ref> stores facial videos and EEG data from 15 participants who watched emotional video clips and expressed their affective responses towards them by filling in a questionnaire. The multimodal eNTERFACE06_EMOBRAIN dataset <ref type="bibr" coords="2,383.73,442.32,11.51,9.58">[7]</ref> contains EEG, frontal fNIRS and physiological recordings (GSR, respiration rate, and blood volume pressure) from five subjects in response to picture stimuli.</p><p xml:id="_Uhjs7j6">We divided the recordings from de DEAP into six groups, corresponding to the basic six emotions, according to the valence-arousal-dominance ratings. Each emotion has been binary classified into two classes: 1 (emotion) and 0 (lack of emotion). For emotion classification, we have used four deep neural network models and four machine learning techniques. The machine learning techniques were: support vector machine (SVM), linear discriminant analysis (LDA), random forest (RF) and k-nearest neighbors (kNN). These algorithms have been trained and cross-validated, with and without feature selection, on four input sets, containing EEG and peripheral data:</p><p xml:id="_UENKBZv">(1) Raw 32-channel EEG values and the peripheral recordings, including hEOG (horizontal electrooculography), vEOG (vertical electrooculography), zEMG (zygomaticus electromyography), tEMG (trapezius electromyography), galvanic skin response (GSR), respiration rate, plethysmography (PPG), temperature; (2) Petrosian fractal dimensions of the 32 EEG channels and the peripheral recordings mentioned in <ref type="bibr" coords="2,110.64,655.92,10.58,9.58" target="#b0">(1)</ref>. (3) Higuchi fractal dimensions of the 32 EEG channels and the peripheral recordings mentioned in <ref type="bibr" coords="2,110.64,683.93,10.58,9.58" target="#b0">(1)</ref>. (4) Approximate entropy for each of the 32 EEG channels and the peripheral recordings mentioned in <ref type="bibr" coords="2,110.64,711.95,10.58,9.58" target="#b0">(1)</ref>.</p><p xml:id="_rJ4ePpH">Feature selection has been ensured by using a Fisher score, principal component analysis (PCA) and sequential forward selection (SFS). This work is a continuation of the research presented in <ref type="bibr" coords="3,343.94,91.07,10.57,9.58" target="#b7">[8]</ref>, wherein, using the same techniques, we classified fear by considering it to be of low valence, high arousal, and low dominance. Similarly, classification has been based on the physiological recordings and subjective ratings from the DEAP database. In the current approach, we extend our study of emotion classification by including all six basic emotions from Ekman's theory. Our research has impact in the field of affective computing, as we can understand better the physiological characteristics underlying various emotions. This could lead to the development of effective computational systems that can recognize and process emotional states in the fields of education, healthcare, psychology, and assistive therapy <ref type="bibr" coords="3,418.42,185.64,11.38,9.58" target="#b8">[9,</ref><ref type="bibr" coords="3,429.80,185.64,11.38,9.58" target="#b9">10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2." xml:id="_A2PqF6F">Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1." xml:id="_KDZAHnu">Emotion Models</head><p xml:id="_NPdczJW">Various theoretical models of emotions have been developed and most of them have been used for automatic emotion recognition.</p><p xml:id="_yacAQmR">Paul Ekman initially considered a set of 6 basic emotions, namely sadness, happiness, disgust, anger, fear and surprise <ref type="bibr" coords="3,179.75,292.08,10.44,9.58" target="#b1">[2]</ref>. This model is known as the discrete model of emotions. Later, he expanded the list to 15 emotions: amusement, anger, contempt, contentment, disgust, embarrassment, excitement, fear, guilt, pride in achievement, relief, sadness/distress, satisfaction, sensory pleasure and shame <ref type="bibr" coords="3,501.50,319.10,15.19,9.58" target="#b10">[11]</ref>. In 2005, Cohen claimed that empirical evidence does not support the framework of basic emotions and that autonomic responses and pan-cultural facial expressions provide no basis for thinking that there is a set of basic emotions <ref type="bibr" coords="3,187.24,359.63,15.27,9.58" target="#b11">[12]</ref>.</p><p xml:id="_At34vEp">In contrast to the discrete model, the dimensional model provides ways to express a wide range of emotional states. Using this model, an emotion is described using two or three fundamental features and the affective states are expressed in a multi-dimensional space <ref type="bibr" coords="3,363.48,400.16,12.66,9.58" target="#b12">[13]</ref><ref type="bibr" coords="3,376.14,400.16,4.22,9.58" target="#b14">[14]</ref><ref type="bibr" coords="3,380.36,400.16,12.66,9.58" target="#b15">[15]</ref>. Russell's circumplex model is an early model, in which an affective state is viewed as a circle in the two-dimensional bipolar space <ref type="bibr" coords="3,103.11,427.17,15.18,9.58" target="#b15">[15]</ref>. The proposed dimensions are pleasure and arousal. Pleasure (valence) reflects the positive or negative emotional states, and a value close to zero means a neutral emotion. Arousal expresses the active or passive emotion component. In this space, 28 affective states are represented: happy, delighted, excited, astonished, aroused, tense, alarmed, angry, afraid, annoyed, distressed, frustrated, miserable, sad, gloomy, depressed, bored, droopy, tired, sleepy, calm, relaxed, satisfied, at ease, content, serene, glad, and pleased.</p><p xml:id="_W67mkja">Whissell also used a bi-dimensional space with activation and evaluation as dimensions <ref type="bibr" coords="3,501.21,508.23,15.42,9.58" target="#b14">[14]</ref>. Later, he refined his model and proposed the wheel of emotions as follows: quadrant I (positive valence, positive arousal), quadrant II (negative valence, positive arousal), the third quadrant (negative valence, negative arousal) and quadrant IV (positive valence, negative arousal). Examples of emotional states and their positions in the wheel are as follows: joy, happiness, love, surprised, contentment in QI; anger, disgust, fear in QII; sadness, boredom, depression in QIII and relaxation, calm in QIV <ref type="bibr" coords="3,499.41,575.78,15.27,9.58" target="#b16">[16]</ref>.</p><p xml:id="_mWtapds">Plutchik developed a componential model in which a complex emotion is a mixture of fundamental emotions. The fundamental emotions considered by Plutchik are joy, trust, fear, surprise, sadness, anticipation, anger, and disgust <ref type="bibr" coords="3,217.47,616.31,15.27,9.58" target="#b12">[13]</ref>.</p><p xml:id="_CmMevym">A three-dimensional model, called the pleasure-arousal-dominance (PAD) model or Valence-Arousal-Dominance (VAD), was introduced by Mehrabian and Russell in <ref type="bibr" coords="3,427.40,643.33,12.35,9.58" target="#b2">[3,</ref><ref type="bibr" coords="3,439.76,643.33,8.24,9.58" target="#b17">[17]</ref><ref type="bibr" coords="3,447.99,643.33,4.12,9.58" target="#b18">[18]</ref><ref type="bibr" coords="3,452.11,643.33,12.35,9.58" target="#b19">[19]</ref>. In the PAD model, there are three independent dimensions: pleasure (valence), which ranges from unhappiness to happiness and expresses the pleasant or unpleasant feeling about something, arousal, the level of affective activation, ranging from sleep to excitement, and dominance, which reflects the level of control of the emotional state, from submissive to dominant. Figure <ref type="figure" coords="3,387.96,697.36,5.08,9.58" target="#fig_0">1</ref> presents the distribution of Ekman's basic emotions within the dimensional emotional space, spanned by the valence, arousal, and dominance axis of the VAD model <ref type="bibr" coords="3,229.84,724.38,15.27,9.58" target="#b20">[20]</ref>, with ratings taken from Russell and Mehrabian <ref type="bibr" coords="3,462.03,724.38,10.58,9.58" target="#b2">[3]</ref>. Russell and Mehrabian provided in <ref type="bibr" coords="4,263.89,272.21,11.69,10.17" target="#b2">[3]</ref> a correspondence between the VAD model and the discrete model of emotions. The values for the six basic emotions, in terms of emotion dimensions, are presented in Table <ref type="table" coords="4,175.53,299.21,3.73,10.17" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2." xml:id="_q7fcc96">Emotions and the nervous system</head><p xml:id="_GsEMrJP">In everyday life, each of us is trapped in the chain of emotions, an important component of behavior. Attempts to define and characterize emotions date back to ancient times, but from the 19th century, research has begun to be scientifically documented. It is well known that there is a close correlation between brain functions and emotions. In particular, the limbic system (hypothalamus, thalamus, amygdala, and hippocampus), the paralimbic system, the vegetative nervous system, and the reticular activating system are involved in processing and controlling emotional reactions. Particular importance is given to the prefrontal cortex, anterior cingulate cortex (ACC), nucleus accumbens, and insula.</p><p xml:id="_EPdT5QG">The limbic system categorizes our emotions into pleasant and unpleasant (valence). Depending on this, chemical neuro-mediators (noradrenaline and serotonin) increase or decrease, influencing the activity of different regions of the body (posture, mimicry, gestures), in response to different emotional states.</p><p xml:id="_FUMDE3j">The amygdala, a structure that gives an emotional connotation of events and memories, is located deep within the right and left anterior temporal lobes of the brain <ref type="bibr" coords="4,411.73,650.03,15.35,10.17" target="#b21">[21]</ref>. The amygdala is a neural switch for fear, anxiety and panic.</p><p xml:id="_v5y6tuB">The hypothalamus is responsible for processing the incoming signals in response to internal mental events such as pain or anger. Hypothalamus triggers corresponding visceral physiological effects like a raised heart rate, blood pressure, or galvanic skin response <ref type="bibr" coords="4,394.14,703.97,15.28,10.17" target="#b22">[22]</ref>.</p><p xml:id="_q6275QC">The insula, the part of the limbic system located deep in the lateral sulcus (Sylvius), is part of the primary gustatory cortex. Regarding the perception of emotions, in this region is perceived the feeling of disgust, which comes as a variant of an unpleasant taste. The experience of disgust protects us from the consumption of spoiled or poisonous foods <ref type="bibr" coords="4,309.52,757.91,15.83,10.17" target="#b23">[23,</ref><ref type="bibr" coords="4,327.81,757.91,11.87,10.17" target="#b24">24]</ref>. Russell and Mehrabian provided in <ref type="bibr" coords="4,252.02,262.42,11.48,9.58" target="#b2">[3]</ref> a correspondence between the VAD model and the discrete model of emotions. The values for the six basic emotions, in terms of emotion dimensions, are presented in Table <ref type="table" coords="4,113.57,289.43,3.74,9.58" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2." xml:id="_4eXbB6B">Emotions and the Nervous System</head><p xml:id="_9QHfGQW">In everyday life, each of us is trapped in the chain of emotions, an important component of behavior. Attempts to define and characterize emotions date back to ancient times, but from the 19th century, research has begun to be scientifically documented. It is well known that there is a close correlation between brain functions and emotions. In particular, the limbic system (hypothalamus, thalamus, amygdala, and hippocampus), the paralimbic system, the vegetative nervous system, and the reticular activating system are involved in processing and controlling emotional reactions. Particular importance is given to the prefrontal cortex, anterior cingulate cortex (ACC), nucleus accumbens, and insula.</p><p xml:id="_zuevCQC">The limbic system categorizes our emotions into pleasant and unpleasant (valence). Depending on this, chemical neuro-mediators (noradrenaline and serotonin) increase or decrease, influencing the activity of different regions of the body (posture, mimicry, gestures), in response to different emotional states.</p><p xml:id="_SvXuRsN">The amygdala, a structure that gives an emotional connotation of events and memories, is located deep within the right and left anterior temporal lobes of the brain <ref type="bibr" coords="4,379.01,653.01,15.42,9.58" target="#b21">[21]</ref>. The amygdala is a neural switch for fear, anxiety and panic.</p><p xml:id="_RB8MbnM">The hypothalamus is responsible for processing the incoming signals in response to internal mental events such as pain or anger. Hypothalamus triggers corresponding visceral physiological effects like a raised heart rate, blood pressure, or galvanic skin response <ref type="bibr" coords="4,393.40,707.04,15.27,9.58" target="#b22">[22]</ref>.</p><p xml:id="_X3jSbTZ">The insula, the part of the limbic system located deep in the lateral sulcus (Sylvius), is part of the primary gustatory cortex. Regarding the perception of emotions, in this region is perceived the feeling of disgust, which comes as a variant of an unpleasant taste. The experience of disgust protects us from the consumption of spoiled or poisonous foods <ref type="bibr" coords="4,286.26,761.08,15.77,9.58" target="#b23">[23,</ref><ref type="bibr" coords="4,302.03,761.08,11.83,9.58" target="#b24">24]</ref>.</p><p xml:id="_gvW9UdU">The hippocampus reminds us of the actions responsible for certain emotional states. Hippocampus abnormalities are associated with mood and anxiety disorders <ref type="bibr" coords="5,352.32,104.58,15.27,9.58" target="#b25">[25]</ref>.</p><p xml:id="_hzsPGBv">The reticular activating system controls arousal, attention, sleep, wakefulness, and reflexes <ref type="bibr" coords="5,498.01,118.09,15.27,9.58" target="#b26">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3." xml:id="_WCWFVSJ">The Six Basic Emotions and Their Corresponding Physiological Reactions</head><p xml:id="_evUb57t">Happiness is an emotional state associated with well-being, pleasure, joy, and full satisfaction. This state is characterized by a facial expression in which the mouth corners are raised <ref type="bibr" coords="5,450.15,174.30,15.12,9.58" target="#b27">[27]</ref>. Happiness activates the right frontal cortex, the left amygdala, the precuneus and the left insula, involving connections between awareness centers -frontal cortex, the insula and the center of feeling-the amygdala <ref type="bibr" coords="5,122.83,214.83,15.27,9.58" target="#b28">[28]</ref>.</p><p xml:id="_2avvVp6">Sadness, the opposite of happiness and different from depression, is an emotion associated with the feelings of regret, weakness, mental pain and melancholy. This state is characterized by a facial expression that causes lowering the mouth's corners, lifting the inner corner of the upper eyelid, raising and nearing the eyebrows. The angle with the tip upward between the inner corners of the eyebrows is a relevant sign of sadness <ref type="bibr" coords="5,198.82,282.38,15.13,9.58" target="#b27">[27]</ref>. At the brain level, sadness is associated with increased activity of the hippocampus, amygdala, right occipital lobe, left insula, and left thalamus <ref type="bibr" coords="5,406.07,295.89,15.27,9.58" target="#b28">[28]</ref>.</p><p xml:id="_EFc8RRZ">Fear is an innate emotion, considered as an evolutionary mechanism of adaptation to survival, that appears in response to a concrete or anticipated danger. This emotion is controlled by the autonomic nervous system which brings the body into a fight-or-flight state. Fear is characterized by an increasing heart rate and respiratory frequency, peripheral vasoconstriction, perspiration, hyperglycemia, etc. At the brain level, fear activates the bilateral amygdala that communicates with the hypothalamus, the left frontal cortex and other parts of limbic system <ref type="bibr" coords="5,315.32,376.94,15.27,9.58" target="#b28">[28]</ref>.</p><p xml:id="_vkk4hHE">Anger is an intense primary emotional state that is part of the fight or flight mechanism, manifested in response to threats or provocations. During the anger state, as a result of the stimulation of the sympathetic vegetative system, a rising of the adrenaline and noradrenaline discharges occurs, followed by an elevation of blood pressure, increasing heart rate and respiratory frequency <ref type="bibr" coords="5,429.13,430.98,15.13,9.58" target="#b27">[27]</ref>. Anger activates the right hippocampus, the amygdala, the left and right part of the prefrontal cortex and the insular cortex <ref type="bibr" coords="5,105.98,458.00,15.27,9.58" target="#b28">[28]</ref>.</p><p xml:id="_7gdXfA6">Disgust is often associated with avoidance. Unlike other emotions, in the case of disgust the heart rate decreases. At the facial level disgust is characterized by raising the upper lip, wrinkling the nose bridge and raising the cheeks <ref type="bibr" coords="5,212.67,498.53,15.42,9.58" target="#b27">[27]</ref>. Disgust implies an activation of the left amygdala, left inferior frontal cortex and insular cortex <ref type="bibr" coords="5,219.96,512.04,15.27,9.58" target="#b28">[28]</ref>.</p><p xml:id="_jCxQKvs">Surprise is the hardest emotion to immortalize, being an unexpected and short-lived experience. After the surprise passes, it turns into fear, anger, disgust, or amusement. When someone experiences surprise, the bilateral inferior frontal gyrus and the bilateral hippocampus are activated. The person tends to arch their brows, open the eyes widely and drop their jaw. The hippocampus is also activated, as it is strongly associated with memory and experiences one had or did not have before <ref type="bibr" coords="5,467.04,579.58,15.77,9.58" target="#b28">[28,</ref><ref type="bibr" coords="5,482.81,579.58,11.83,9.58" target="#b29">29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4." xml:id="_SM2uhJu">Biophysical Data</head><p xml:id="_tHW84A6">Electroencephalography (EEG) is a method of exploring the electrical potentials of the brain. The encephalogram is the graph obtained from the registration of electric fields at the scalp level. EEG is efficient for detecting affective states, with good temporal resolution. There are four types of waves commonly recorded in humans.</p><p xml:id="_Wvd2Vhw">Delta waves have high amplitude and low frequency (0.5-3 Hz). They are characteristic of psychosomatic relaxation states, being recorded in deep sleep phases. They can also be encountered in anesthetic states, following the blocking of nerve transmission through the reticular activating system. They can appear in any cortical region, but predominate in the frontal area <ref type="bibr" coords="5,402.22,716.85,15.17,9.58" target="#b30">[30]</ref>. The theta waves have a frequency of 3-8 Hz. This rhythm occurs during low brain activities, sleep, drowsiness or deep meditation. An excess of theta waves is related to artistic, creative, or meditative states. The alpha waves, so-called 'basic rhythm', are oscillations of small amplitude with average frequencies around 8-12 Hz. Under normal conditions, their amplitude increases and decreases regularly, the waves being grouped into characteristic spindle. They appear in the occipital cortex and indicate a normal wakeful state when the human subjects are relaxed or have their eyes closed. The beta waves are characterized by a frequency of 12-30 Hz. Unlike the alpha rhythm, beta waves are highly irregular and signify a desynchronization of cortical neural activity. Their maximum incidence is in the anterior parietal and posterior frontal regions of the brain. This wave is associated with active thinking or concentration and is related to consciousness, brain activities and motor behaviors. The gamma waves are the fastest brainwaves (30-42 Hz), usually found during conscious perception and related to the emotions of happiness and sadness <ref type="bibr" coords="6,178.88,199.15,15.20,9.58" target="#b31">[31]</ref>. During memorization tasks, the activation of the gamma waves is visible in the temporal lobe. The predominance of these waves has been associated with the installation of anxiety, stress, or arousal states <ref type="bibr" coords="6,216.05,226.17,15.27,9.58" target="#b32">[32]</ref>.</p><p xml:id="_XKf7cuz">Galvanic skin response (GSR) or electrodermal activity (EDA) is a handy and relatively noninvasive tool used to study body reactions to various stimuli, being a successful indicator of physiological and psychological arousal. The autonomic control regulates the internal environment and ensures the body's homeostasis <ref type="bibr" coords="6,163.98,280.21,15.17,9.58" target="#b34">[33]</ref>. It is considered that the skin is an organ that responds preponderantly to the action of the sympathetic nervous system through the eccrine sweat gland <ref type="bibr" coords="6,410.73,293.71,15.41,9.58" target="#b35">[34]</ref>. For this reason, the data acquisition made from the skin can offer information about the attitude of the body's "fight or flee" reactions. Skin conductance is quantified by applying an electrical potential between two contact points on the skin and measuring the current flow between them. EDA has a background component, namely skin conductance level (SCL), resulting from the interaction between the tonic discharges of the sympathetic innervations and local factors <ref type="bibr" coords="6,277.24,361.26,15.12,9.58" target="#b36">[35]</ref>, and a fast component -skin conductance responses (SCR), which results from the phasic sympathetic neuronal activity <ref type="bibr" coords="6,369.79,374.77,15.19,9.58" target="#b37">[36]</ref>. A high level of SCL indicates a high degree of anxiety <ref type="bibr" coords="6,184.84,388.28,15.27,9.58" target="#b38">[37]</ref>.</p><p xml:id="_2JT5hUZ">Facial electromyography (EMG) uses the corrugator supercilii ("frowning muscle") activity to track emotional valence.</p><p xml:id="_2msnzXc">Heart rate (HR) and HR variability (HRV) are other parameters used to assess human emotions. They have good temporal resolution and can monitor variations or trends of emotions. HRV is associated with cerebral blood flow in the amygdala and in the ventromedial prefrontal cortex <ref type="bibr" coords="6,501.21,455.83,15.42,9.58" target="#b39">[38]</ref>. Individuals with high HRV tend to better regulate their emotions <ref type="bibr" coords="6,365.51,469.34,15.27,9.58" target="#b40">[39]</ref>.</p><p xml:id="_Yh5nYCs">Respiration is an important function for maintaining the homeostasis of the internal environment. Respiratory regulation is achieved by correlating the respiratory centers and the brainstem, the limbic system, and the cerebral cortex. Breathing rate also changes according to emotional responses <ref type="bibr" coords="6,490.79,509.87,15.27,9.58" target="#b41">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5." xml:id="_b32bUU7">Machine Learning Techniques for Emotions Classification</head><p xml:id="_wgF9dgu">The interest in the field of automatic recognition of emotions is constantly increasing. The data used in emotion recognition systems is primarily extracted from voice, face, text, biophysical signals and body motion <ref type="bibr" coords="6,152.74,579.58,15.12,9.58" target="#b42">[41]</ref>. In this section, we performed a brief analysis of the machine learning techniques involved in automatic emotion recognition systems using biophysical data.</p><p xml:id="_9fxJvG5">Three binary classifications have been performed in [4]: low or high arousal, low or high valence, and low or high liking. The authors used the Gaussian naïve Bayes algorithm for classification, alongside Fisher's linear discriminant for feature selection and leave-one-out cross validation for classification assessment. To measure the performance of the proposed scheme, F1 and average accuracies (ACC) were used. To draw a final conclusion, a decision fusion method was adopted.</p><p xml:id="_57c95Kt">Atkinson and Campos <ref type="bibr" coords="6,196.62,674.15,16.47,9.58" target="#b43">[42]</ref> used the minimum-redundancy maximum-relevance (mRMR) method for feature selection and Support Vector Machine for binary classification into low/high valence and arousal. The reported accuracy rates were: 73.14% for valence and 73.06% for arousal. The study was performed by extracting and processing the EEG features from the DEAP database. Yoon and Chung <ref type="bibr" coords="6,109.10,728.19,16.57,9.58" target="#b44">[43]</ref> used the Pearson correlation coefficient for feature extraction and a probabilistic classifier based on the Bayes theorem for resolving the binary classification problem of low/high valence and arousal discrimination, with an accuracy of about 70% for both: 70.9% for valence and 70.1% for arousal. For the three-level classification (low/medium/high), the accuracy for high arousal was 55.2% and for valence, 55.4%. Similarly, emotion recognition has been performed based on the EEG data from the DEAP dataset.</p><p xml:id="_4SqMD8Z">A similar approach is presented in Naser and Saha <ref type="bibr" coords="7,314.95,131.60,15.12,9.58" target="#b45">[44]</ref>, where the SVM algorithm led to accuracies of 66.20%, 64.30%, and 28.90% for classifying arousal, valence, and dominance into low and high groups. Dual-tree complex wavelet packet transform (DT-CWPT) was used for feature selection.</p><p xml:id="_anr3zNx">In <ref type="bibr" coords="7,109.04,172.13,15.13,9.58" target="#b46">[45]</ref>, two classifiers, linear discriminant analysis and SVM were used for two-level classification of valence and arousal. The results showed that SVM produces higher accuracies for arousal and the LDA classifier is better in the case of valence. By applying the SVM technique on the EEG features, classification accuracies of 62.4% and 69.40% were achieved during a music-induced affective state evaluation experiment whereby the users were required to rate their currently perceived emotion in terms of valence and arousal. In the case of the LDA classifiers, the accuracies were 65.6% for valence and 62.4% for arousal. Liu et al <ref type="bibr" coords="7,219.76,253.19,16.73,9.58" target="#b47">[46]</ref> conducted two experiments in which visual and audio stimuli were used to evoke emotions. The SVM algorithm, having as input fractal dimension features (FD), statistical and higher order crossings (HOC) extracted from the EEG signals provided the best accuracy for recognizing two emotions -87.02%, in the case of the audio database and 76.53% in the case of the visual database. The authors provided a comparison between the performances of the proposed strategies applied on their databases and a benchmark database, DEAP. Having DEAP as data source, the mean accuracy for two emotions recognition was 83.73% and 53.7% for recognizing 8 emotions, namely happy, surprised, satisfied, protected, angry, frightened, unconcerned, and sad. A comparative study of four machine learning methods (k-nearest neighbor, SVM, regression tree, Bayesian networks (BNT)) showed that SVM offered the best average accuracy at 85.8%, followed by regression tree with 83.5% for the classification of five types of emotions, namely anxiety, boredom, engagement, frustration, and anger into 3 categories, namely low, medium, and high <ref type="bibr" coords="7,340.19,401.79,15.27,9.58" target="#b48">[47]</ref>.</p><p xml:id="_AXe335e">In the case of the two-class classification for arousal, valence and like/dislike ratings, for EEG signals, the average accuracy rates were 55.7%, 58.8%, and 49.4% with SVM. Having as input features the peripheral physiological responses, the classification average accuracies recorded 58.9%, 54.2%, and 57.9% <ref type="bibr" coords="7,124.19,455.83,15.27,9.58" target="#b49">[48]</ref>.</p><p xml:id="_vcTwh65">Based on the MAHNOB dataset and using the SVM algorithm with various kernels, Wiem et al. <ref type="bibr" coords="7,502.27,469.34,16.47,9.58" target="#b50">[49]</ref> reached a classification accuracy between 57.34% and 68.75% for valence and between 60.83% and 63.63% for arousal when discriminating into low/high groups and between 46.36% and 56.83%, respectively, 50.52% and 54.73% for classification into 3 groups. The features were normalized and a level feature fusion (LFF) algorithm was used. The most relevant features were the electrocardiogram and the respiration volume.</p><p xml:id="_FyXzy3n">In <ref type="bibr" coords="7,109.74,550.39,15.42,9.58" target="#b51">[50]</ref>, a deep learning method based on the long-short term memory algorithm was used for classifying low/high valence, arousal and liking based on the EEG raw data from the DEAP dataset [4], with accuracies of 85.45%, 85.65% and 87.99%. Jirayucharoensak et al. <ref type="bibr" coords="7,402.28,577.41,16.73,9.58" target="#b52">[51]</ref> used a deep learning network implemented with a stacked autoencoder based on the hierarchical feature learning approach. The input features were the power spectral densities of the EEG signals from the DEAP database, which were selected using the principal component analysis (PCA) algorithm. Covariate Shift Adaptation (CSA) was applied to reduce the non-stationarity in EEG signals. The ratings from 1 to 9 have been divided into 3 levels and mapped into "negative", "neutral", and "positive" for valence and into "passive" "neutral", and "active" for arousal. A leave-one-out cross validation scheme was used to evaluate the performance. They were finally classified with an accuracy of 49.52% for valence and 46.03% for arousal.</p><p xml:id="_ZtVHyDN">A 3D convolutional neural network-based schema has been applied on the DEAP data set in <ref type="bibr" coords="7,502.24,699.00,16.50,9.58" target="#b53">[52]</ref> for a two-level classification of valence and arousal. The authors increased the training samples through an augmentation process adding noise signals to the original EEG signals. The schema consisted of 6 layers: input layer, middle layers (two pairs of convolution and max-pooling layers) and a fully-connected output layer. In both convolution layers, rectified linear unit (RELU) is used as activation function. The recorded accuracies for the proposed method were: 87.44% for valence and 88.49% for arousal. Random forest is not a very common technique used for emotion recognition. In <ref type="bibr" coords="8,88.28,118.09,15.35,9.58" target="#b54">[53]</ref>, the authors reported a 74% overall accuracy rate for emotions classification into amusement, grief, anger, fear and a baseline state using the random forest classifier. Leave-one-subject-out cross validation was used for evaluating the classifier.</p><p xml:id="_ps4eVST">In Table <ref type="table" coords="8,135.28,158.62,3.74,9.58" target="#tab_2">2</ref>, we present the performance of the ML techniques used for emotion recognition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6." xml:id="_NuYPCNJ">Our Paradigm for Emotions Classification</head><p xml:id="_S28B8UP">In Mehrabian and Russell's model provided in <ref type="bibr" coords="9,302.89,336.76,10.49,9.58" target="#b2">[3]</ref>, the emotion dimensions (valence, arousal and dominance) are spanned across the interval [-1; 1]. The valence, arousal and dominance ratings from the DEAP database are continuous values in the interval <ref type="bibr" coords="9,330.58,363.78,20.75,9.58">[1; 9]</ref>. In order to obtain a correspondence, they have been mapped as follows (Figure <ref type="figure" coords="9,265.54,377.29,3.60,9.58" target="#fig_3">2</ref>):  Table <ref type="table" coords="9,123.99,559.68,5.01,10.17" target="#tab_4">3</ref> presents the intervals of valence, arousal and dominance assigned to each of the six basic emotions, inspired from the values of Mehrabian and Russell's model from Table <ref type="table" coords="9,445.19,573.18,3.76,10.17" target="#tab_0">1</ref>. The ratings of valence and arousal from the DEAP database have been assigned to a larger interval: low ([1;5)) or high <ref type="bibr" coords="9,99.71,600.18,21.26,10.17">([5;9]</ref>). Dominance was the emotion dimension that fluctuated in a smaller interval. Thus, an emotion is characterized by low or high valence/arousal and some degree of dominance spanned across a narrower interval.  Table <ref type="table" coords="9,123.45,546.48,4.94,9.58" target="#tab_4">3</ref> presents the intervals of valence, arousal and dominance assigned to each of the six basic emotions, inspired from the values of Mehrabian and Russell's model from Table <ref type="table" coords="9,444.24,559.99,3.81,9.58" target="#tab_0">1</ref>. The ratings of valence and arousal from the DEAP database have been assigned to a larger interval: low ([1;5)) or high ([5;9]). Dominance was the emotion dimension that fluctuated in a smaller interval. Thus, an emotion is characterized by low or high valence/arousal and some degree of dominance spanned across a narrower interval.</p><p xml:id="_4tEVnxS">Table <ref type="table" coords="9,123.61,627.53,4.98,9.58" target="#tab_6">4</ref> presents the intervals corresponding to Condition 0, no emotion (or the lack of emotion), and Condition 1, the existence of a certain degree of emotion.</p><p xml:id="_EnYAQh4">Four input features, sets have been generated after extracting and labelling the data from DEAP: (1) 32-channel raw EEG values and the peripheral recordings: hEOG, vEOG, zEMG, tEMG, GSR, Respiration, PPG, and temperature; (2) Petrosian fractal dimensions of the 32 EEG channels and the peripheral recordings mentioned at (1); (3) Higuchi fractal dimension of the 32 EEG channels and the peripheral recordings mentioned at (1); (4) Approximate entropy of the 32 EEG channels and the peripheral recordings mentioned at (1).   Higuchi fractal dimension (HFD) is a non-linear method highly used in the analysis of biological signals. It originates from chaos theory and has been used for 30 years as a modality of measuring signals dynamics and complexity. It has been used for detecting hidden information contained in biophysical time series with the help of fractals, which, despite scaling, preserve the structure and shape of complex signals. There are many methods for calculating fractal dimensions, such as Katz's, Petrosian's or Higuchi's <ref type="bibr" coords="11,184.65,382.83,12.78,9.58" target="#b55">[54]</ref><ref type="bibr" coords="11,197.43,382.83,4.26,9.58">[55]</ref><ref type="bibr" coords="11,201.69,382.83,12.78,9.58" target="#b57">[56]</ref>. Approximate Entropy (ApEn) is a measure of regularity in the time domain which determines the predictability of a signal by comparing the number of matching sequences of a given length with the number of matching sequences one increment longer <ref type="bibr" coords="11,430.28,409.84,15.33,9.58" target="#b58">[57]</ref>. In regular data series, knowing the previous values enables the prediction of the subsequent ones. A high value of ApEn is associated with random and unpredictable variation, while a low value correlates with regularity and predictability in a time series <ref type="bibr" coords="11,271.78,450.37,15.27,9.58" target="#b59">[58]</ref>.</p><p xml:id="_JQShRF8">DNN1 has one input layer, three hidden layers with 300 neurons per layer, and one output layer. The input layer contains 40 neurons, corresponding to the 32 EEG data (raw values/Petrosian fractal dimensions/Higuchi fractal dimensions/approximate entropy) and 8 peripheral data (hEOG, vEOG, zEMG, tEMG, GSR, respiration rate, PPG and temperature). Petrosian fractal dimensions, Higuchi fractal dimensions and approximate entropy have been computed using the functions from the PyEEG library <ref type="bibr" coords="11,109.01,531.43,15.35,9.58" target="#b60">[59]</ref>. The output layer generates two possible results: 0 or 1. In the output layer, we used the binary crossentropy loss function and sigmoid activation function. Also, the model uses the Adam gradient descent optimization algorithm and the rectified linear unit (RELU) activation function on each layer. The network is organized as a multi-layer perceptron network. The input data has been standardized to zero mean and unit variance. The Keras classifier <ref type="bibr" coords="11,359.48,585.47,16.46,9.58" target="#b61">[60]</ref> had 1000 epochs for training and a batch size of 20. Cross-validation has been performed by using the k-fold method with k = 10 splits and the leave-one-out method, which takes each sample as test set and keeps the remaining samples in the training set. However, the leave-one-out method is more computationally demanding than k-fold. The model has been trained and cross-validated for 10 times and we calculated the average accuracy and F1 score across these 10 iterations. Each time, the input data has been shuffled before being divided into the training and test datasets.</p><p xml:id="_64naTFQ">DNN2 has 3 hidden layers and 150 neurons/layer, DNN3 has 6 hidden layers with 300 neurons/layer, and DNN4 has 6 hidden layers with 150 neurons/layer. Their configuration and method of training and cross-validating is similar to DNN1. Feature selection was not necessary for the DNNs, as the dropout regularization technique prevents overfitting.</p><p xml:id="_KYzwAcT">For the SVM method, we used the radial basis function kernel (rbf). For Random Forest, the number of trees in the forest has been set to 10 (default value for the n_estimators parameter in the RandomForestClassifier method from the scikit learn library [61]). The function that measures the quality of the split has been "entropy", that divides based on information gain. For kNN, the number of neighbors has been set to 7. For SVM, LDA, RF, and KNN, the input data has been divided into 70% training and 30% test using the train_test_split method from the scikit learn library. This function makes sure that each time, the data is shuffled before dividing into the training and test datasets. The input data has been also standardized in order to reduce it to zero mean and unit variance.</p><p xml:id="_bjxdKbf">These classification methods have been trained and cross-validated 10 times, without feature selection and with the Fisher, PCA, and SFS feature selection methods. In a similar way to the DNNs, we calculated the average accuracy and F1 score across these 10 iterations. The Fisher score has been calculated on the training dataset and then the first, most relevant 20 features have been selected. Consequently, a machine learning model (SVM/RF/LDA/kNN) has been constructed and cross-validated based on these relevant features. The PCA algorithm retains 99% of the data variance (the n_components parameter of the PCA method from scikit learn has been set to 0.99). The SFS classifier selects the best feature combination containing between 3 and 20 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3." xml:id="_34XkKWh">Results</head><p xml:id="_RpcqJ2e">The cross-validation results obtained after training and testing on the data using the machine and deep learning methods, with k-fold cross validation, for each basic emotion, are presented in Tables <ref type="table" coords="12,107.77,322.91,18.29,9.58" target="#tab_14">6-11</ref>. The numbers written in bold correspond to the maximum F1 scores and accuracies. Table <ref type="table" coords="12,102.37,336.41,10.08,9.58" target="#tab_15">12</ref> presents the most important features for each of the six basic emotions, based on the Fisher score and SFS algorithm. The accuracies obtained using the leave-one-out method for cross-validation are with 5%-10% lower, but the hierarchy of results is preserved, not affecting the classification ranking.</p><p xml:id="_QKSnm3R">Figure <ref type="figure" coords="12,128.13,376.94,4.88,9.58" target="#fig_6">3</ref> presents the decision tree obtained for classifying anger using RF with raw EEG data and peripheral features, without feature selection.        </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4." xml:id="_xRP3baF">Discussion</head><p xml:id="_rKkr9nG">Table <ref type="table" coords="16,124.73,639.86,10.16,9.58" target="#tab_16">13</ref> presents the best classification F1 scores for each emotion, with and without feature selection. Without feature selection, kNN has been selected in 13 cases, followed by Random Forest (seven times) and SVM (four times). For anger, the highest classification accuracy has been obtained for Petrosian and Higuchi fractal dimension, using SVM (98.02%). For joy, the highest classification accuracy has been achieved by kNN using Petrosian values (87.9%). For surprise, kNN with raw EEG values (85.01%), disgust-kNN with Petrosian values (95%), fear-kNN with raw EEG values (90.75%), sadness-SVM with Higuchi fractal dimensions (90.8%).</p><p xml:id="_ZdxHRue">With feature selection, kNN has been selected in 12 cases, random forest seven times, SVM five times and LDA one time. SFS has been selected two times and Fisher score 14 times. For anger, the highest classification accuracy has been obtained for raw data using kNN and Fisher (97.52%). For Joy, the highest classification accuracy has been achieved by LDA and SFS using raw values (100%). For surprise, SVM and SFS with raw EEG values (96%), disgust, random forest and Fisher with Higuchi fractal dimensions (90.23%), fear, kNN and Fisher with Higuchi fractal dimensions (83.39%), and sadness, SVM and Fisher with Higuchi fractal dimensions (86.43%). For anger, disgust, fear and sadness, the classification accuracies have been higher without feature selection. The SFS feature selection algorithm lead to higher accuracies for joy and surprise.</p><p xml:id="_cn3pWNW">According to Table <ref type="table" coords="17,179.34,453.73,8.14,9.58" target="#tab_15">12</ref>, the most important features for anger were tEMG and respiration. This result is consistent with reality, because in conditions of anger, anxiety and stress, besides intensifying the breathing, there is also an accumulation of tension in the muscles located between the forehead and the shoulders (tension triangles). Thus, corrugator muscles are responsible for forehead frowning, the masseter and the temporalis muscles are responsible for jaw clenches, while the trapezius muscles are responsible for the neck tightening and the shoulders rising.</p><p xml:id="_x5GBVN8">The most important features for joy were GSR and zEMG. Dynamic facial expressions of joy determine an intense activity of the zygomatic muscle, which pulls up and laterally the corners of the lips to sketch a smile. High skin conductance entropy indicates body arousal.</p><p xml:id="_cMyChVJ">The most important features for surprise are GSR and FC1. Although surprise is an emotion with neutral valence, it is frequently associated with increased GSR.</p><p xml:id="_XBthdRq">According to existing studies, disgust suppresses attention, in order to minimize the visual contact with the threatening agent. This explains the movement of the eyeballs horizontally and vertically (vEOG and hEOG), which are the most important features for disgust (Table <ref type="table" coords="17,413.82,629.36,7.89,9.58" target="#tab_15">12</ref>).</p><p xml:id="_MJC5V9w">Fear is characterized by opening the eyes and rotating the eyeballs horizontally and vertically, for danger identification (vEOG, hEOG), stretching the mouth (zEMG), and opening the nostrils for better tissue oxygenation. Activation of the frontal cortex (FC1, F4) aims to stimulate motor areas and prepare the body for escape or fight.</p><p xml:id="_huzaPYq">Sadness involves an increasing activity, mostly in the left prefrontal cortex and in the structures of the limbic system, as we can see from our most selected traits: FC1 and FP1 (Table <ref type="table" coords="17,439.46,710.41,7.89,9.58" target="#tab_15">12</ref>).</p><p xml:id="_Z259e4z">The maximum accuracies obtained for classifying the six basic emotions into two classes (1-emotion and 0-no emotion) were higher than those obtained for classifying into low/high valence/arousal-62%/56% [4], 73% <ref type="bibr" coords="17,234.80,750.94,15.31,9.58" target="#b43">[42]</ref>, 70% <ref type="bibr" coords="17,277.33,750.94,15.31,9.58" target="#b44">[43]</ref>, 85% using the long-short term memory algorithm (all using the data from the DEAP database), 66%/64% <ref type="bibr" coords="18,314.15,91.07,15.17,9.58" target="#b45">[44]</ref>, 62%/69% <ref type="bibr" coords="18,377.08,91.07,15.17,9.58" target="#b46">[45]</ref>, 55%/58% <ref type="bibr" coords="18,440.02,91.07,15.17,9.58" target="#b62">[62]</ref>, and 68%/54% using the data from the MAHNOB database <ref type="bibr" coords="18,273.13,104.58,15.27,9.58" target="#b50">[49]</ref>.</p><p xml:id="_WjRzswK">Liu <ref type="bibr" coords="18,115.66,118.09,16.73,9.58" target="#b47">[46]</ref> achieved a classification accuracy of 53% for recognizing eight emotions using Fractal Dimension Features with SVM, while we obtained accuracies of over 83% using Higuchi Fractal Dimensions and kNN. Our results are comparable to those obtained by Liu <ref type="bibr" coords="18,398.75,145.11,16.47,9.58" target="#b48">[47]</ref> who reached accuracies of 85% with SVM and 83% using a regression tree for classifying anxiety, boredom, engagement, frustration, and anger into three categories, namely low, medium, and high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5." xml:id="_cpCZXBv">Conclusions</head><p xml:id="_9zJ2fCU">This paper presented a comparative analysis between various machine learning and deep learning techniques for classifying the six basic emotions from Ekman's model <ref type="bibr" coords="18,371.57,228.34,10.45,9.58" target="#b1">[2]</ref>, namely anger, disgust, fear, joy, sadness, and surprise, using physiological recordings and the valence/arousal/dominance ratings from the DEAP database. DEAP is the most well-known and exhaustive multimodal dataset for analyzing human affective states, containing data from 32 subjects who watched 40 one-minute long excerpts of music videos. Using the three-dimensional VAD model of Mehrabian and Russell <ref type="bibr" coords="18,439.78,282.38,10.62,9.58" target="#b2">[3]</ref>, each of the six basic emotions has been defined as a combination of valence/arousal/dominance intervals <ref type="bibr" coords="18,472.48,295.89,15.26,9.58" target="#b63">[63]</ref>. Then, we classified them into two classes: 0-lack of emotion and 1-emotion by training and cross-validating using various machine learning and deep learning techniques, with and without feature selection. For anger, the highest classification accuracy has been obtained with Petrosian and Higuchi fractal dimensions, using SVM and no feature selection (98.02%). For joy, the highest classification accuracy has been achieved by LDA and SFS using raw EEG values (100%). For surprise-SVM and SFS with raw EEG values (96%), disgust-kNN with Petrosian values and no feature selection (95%), fear-kNN with raw EEG values and no feature selection (90.75%), and sadness-SVM with Higuchi fractal dimensions and no feature selection (90.8%). In the case of four emotions (anger, disgust, fear and sadness), the classification accuracies were higher without feature selection.</p><p xml:id="_hcsjch7">Our approach to emotion classification has applicability in the field of affective computing <ref type="bibr" coords="18,501.28,430.98,15.36,9.58" target="#b65">[64]</ref>. The domain includes all the techniques and methods used for the automatic recognition of emotions and their applications in healthcare, education, marketing, website personalization, recommendation systems, video games, and social media. Basically, human feelings are translated to the computers, which can understand and express them. The identification of the six basic emotions can be used for developing assistive robots, as the ones which detect and processes the affective states of children suffering from autism spectrum disorder <ref type="bibr" coords="18,256.28,512.04,15.19,9.58" target="#b66">[65]</ref>, intelligent tutoring systems that use automatic emotion recognition to improve learning efficiency and adapt learning contents and interfaces in order to engage students <ref type="bibr" coords="18,117.58,539.06,15.41,9.58" target="#b67">[66]</ref>, virtual reality games or immersive virtual environments that act as real therapists in anxiety disorder treatment <ref type="bibr" coords="18,196.46,552.57,11.39,9.58" target="#b8">[9,</ref><ref type="bibr" coords="18,207.85,552.57,11.39,9.58" target="#b9">10]</ref>, recommender systems which know the users' mood and adapt the recommended items accordingly <ref type="bibr" coords="18,222.79,566.07,15.75,9.58" target="#b68">[67,</ref><ref type="bibr" coords="18,238.55,566.07,11.81,9.58" target="#b69">68]</ref>, public sentiments analysis about different events, economic, or political decisions <ref type="bibr" coords="18,169.82,579.58,15.27,9.58" target="#b70">[69]</ref>, and assistive technology <ref type="bibr" coords="18,301.74,579.58,12.76,9.58" target="#b71">[70]</ref><ref type="bibr" coords="18,314.50,579.58,4.25,9.58" target="#b72">[71]</ref><ref type="bibr" coords="18,318.76,579.58,12.76,9.58" target="#b73">[72]</ref>.</p><p xml:id="_E5NG5xG">Emotions play a central role in explainable artificial intelligence (AI), where there is so much need for human-AI interaction and human-AI interfaces <ref type="bibr" coords="18,307.67,606.60,15.34,9.58" target="#b74">[73]</ref>. As future research directions, we intend to classify the six basic emotions into three classes, namely negative, neutral, and positive and to develop emotion-based applications starting from the results presented in this paper, in the emerging field of explainable AI.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,105.57,247.81,380.85,9.13;4,188.43,81.89,215.28,149.28"><head>Figure 1 .</head><label>1</label><figDesc xml:id="_4HcyjFN">Figure 1. The VAD (Valence-Arousal-Dominance) model spanned across the six basic emotions.</figDesc><graphic coords="4,188.43,81.89,215.28,149.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,108.54,239.76,378.20,8.63"><head>Figure 1 .</head><label>1</label><figDesc xml:id="_vkP8XBW">Figure 1. The VAD (Valence-Arousal-Dominance) model spanned across the six basic emotions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,76.21,361.91,145.43,8.10;9,488.23,361.91,30.15,8.10"><head>Symmetry 2019 ,</head><label>2019</label><figDesc xml:id="_SCEbyHU">11, x FOR PEER REVIEW 10 of 24</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,97.45,522.26,399.74,9.13;9,97.45,535.28,100.89,9.13;9,139.09,388.14,316.14,118.50"><head>Figure 2 .</head><label>2</label><figDesc xml:id="_xc5cP2d">Figure 2. Correspondence between the ratings from Mehrabian and Russell's model and the ratings from the DEAP database.</figDesc><graphic coords="9,139.09,388.14,316.14,118.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,97.80,511.00,399.69,8.63;9,97.80,523.82,100.30,8.63"><head>Figure 2 .</head><label>2</label><figDesc xml:id="_PApagSZ">Figure 2. Correspondence between the ratings from Mehrabian and Russell's model and the ratings from the DEAP database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="12,119.05,728.69,339.79,7.76;12,119.05,739.76,31.48,7.76"><head>Symmetry 2019 , 24 Figure 3 .</head><label>2019243</label><figDesc xml:id="_rrwjvJy">Figure 3. Decision tree for Anger using RF with raw EEG data and peripheral features, without feature selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="12,97.80,730.30,399.69,8.63;12,97.80,743.12,66.88,8.63"><head>Figure 3 .</head><label>3</label><figDesc xml:id="_jEPN4mV">Figure 3. Decision tree for Anger using RF with raw EEG data and peripheral features, without feature selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,148.23,325.27,295.53,113.53"><head>Table 1 .</head><label>1</label><figDesc xml:id="_u3DkGms">Values for the six basic emotions in terms of emotion dimensions.</figDesc><table coords="4,193.05,344.69,205.32,94.11"><row><cell></cell><cell cols="2">Valence Arousal</cell><cell>Dominance</cell></row><row><cell>Anger</cell><cell>-0.43</cell><cell>0.67</cell><cell>0.34</cell></row><row><cell>Joy</cell><cell>0.76</cell><cell>0.48</cell><cell>0.35</cell></row><row><cell>Surprise</cell><cell>0.4</cell><cell>0.67</cell><cell>-0.13</cell></row><row><cell>Disgust</cell><cell>-0.6</cell><cell>0.35</cell><cell>0.11</cell></row><row><cell>Fear</cell><cell>-0.64</cell><cell>0.6</cell><cell>-0.43</cell></row><row><cell>Sadness</cell><cell>-0.63</cell><cell>0.27</cell><cell>-0.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,150.74,312.25,293.50,125.70"><head>Table 1 .</head><label>1</label><figDesc xml:id="_b2FuNJQ">Values for the six basic emotions in terms of emotion dimensions.</figDesc><table coords="4,175.58,333.22,243.84,104.73"><row><cell></cell><cell>Valence</cell><cell>Arousal</cell><cell>Dominance</cell></row><row><cell>Anger</cell><cell>-0.43</cell><cell>0.67</cell><cell>0.34</cell></row><row><cell>Joy</cell><cell>0.76</cell><cell>0.48</cell><cell>0.35</cell></row><row><cell>Surprise</cell><cell>0.4</cell><cell>0.67</cell><cell>-0.13</cell></row><row><cell>Disgust</cell><cell>-0.6</cell><cell>0.35</cell><cell>0.11</cell></row><row><cell>Fear</cell><cell>-0.64</cell><cell>0.6</cell><cell>-0.43</cell></row><row><cell>Sadness</cell><cell>-0.63</cell><cell>0.27</cell><cell>-0.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,83.18,184.04,421.07,544.88"><head>Table 2 .</head><label>2</label><figDesc xml:id="_jQCV4Ty">Emotions classification performance.</figDesc><table coords="8,83.18,203.62,421.07,525.30"><row><cell cols="2">Reference Open Data Source</cell><cell>Classifiers</cell><cell>Classification</cell><cell>Feature Selection/Processing</cell><cell>Measure of Performance (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F1-scores</cell></row><row><cell>[4] 2012</cell><cell>DEAP</cell><cell>Gaussian Naïve Bayes</cell><cell>Two-level class: arousal, valence, liking</cell><cell>Fischer's linear discriminant</cell><cell>62.9-arousal 65.2-valence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64.2-liking</cell></row><row><cell>[42] 2016</cell><cell>DEAP</cell><cell>SVM</cell><cell>Two-level class: arousal, valence</cell><cell>Minimum redundancy Maximum relevance</cell><cell>Accuracy 73.06-arousal 73.14-valence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F1-scores</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>74.9-high arousal</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>62.8-low arousal</cell></row><row><cell>[42] 2013</cell><cell>DEAP</cell><cell>Probabilistic classifier based on Bayes' theorem</cell><cell>Two-level class: arousal, valence</cell><cell>Pearson correlation coefficient</cell><cell>74.7-high valence 65.9-low valence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70.1-high arousal</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70.9-high valence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F1-scores</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>63.3 -high arousal</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>43.3-medium arousal</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>53.9-low arousal</cell></row><row><cell>[43] 2013</cell><cell>DEAP</cell><cell>Probabilistic classifier based on Bayes' theorem</cell><cell>Three-level class: arousal, valence</cell><cell>Pearson correlation coefficient</cell><cell>66.1-high valence 40.9-medium valence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>51.8-low valence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>55.2-high arousal</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>55.4-high valence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell></row><row><cell>[44] 2013</cell><cell>-</cell><cell>SVM</cell><cell>Two-level class</cell><cell>DT-CWPT</cell><cell>66.20-arousal 64.30-valence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>68.90-dominance</cell></row><row><cell>[45] 2015</cell><cell>-</cell><cell>SVM</cell><cell>Two-level class</cell><cell>Stepwise Linear Regression</cell><cell>Accuracy (%) 62.4-valence 69.4-arousal</cell></row><row><cell>[45] 2015</cell><cell>-</cell><cell>LDA</cell><cell>Two-level class</cell><cell>Stepwise Linear Regression</cell><cell>Accuracy (%) 65.6-valence 62.4-arousal</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Audio database</cell></row><row><cell>[46] 2013</cell><cell>-</cell><cell>SVM</cell><cell>Discrete emotion (presence or not)</cell><cell>HOC+6 statistical +FD</cell><cell>87.02-2 emotions 76.53-2 emotions Visual database</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>61.67%-5 emotions</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>56.6-5 emotions</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell></row><row><cell>[46] 2013</cell><cell>DEAP</cell><cell>SVM</cell><cell>Discrete emotion (presence or not)</cell><cell>HOC+6 statistical +FD</cell><cell>83.73-2 emotions 53.7-8 emotions</cell></row><row><cell>[47] 2005</cell><cell>-</cell><cell>kNN RT BNT SVM</cell><cell>Three-level class</cell><cell>Entire feature set</cell><cell>Average accuracy 75.12 83.50 74.03 85.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average accuracy</cell></row><row><cell>[48] 2010</cell><cell>-</cell><cell>SVM</cell><cell>Two-level class</cell><cell>Fast correlation based filter (FCBF)</cell><cell>54.2-valence 58.9-arousal</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>57.9-like/dislike</cell></row><row><cell>[49] 2017</cell><cell>MAHNOB-HCI</cell><cell>SVM Gaussian kernel</cell><cell>Two-level class</cell><cell>Feature fusion</cell><cell>Accuracy 63.63-arousal 68.75-valence</cell></row><row><cell>[49] 2017</cell><cell>MAHNOB-HCI</cell><cell>SVM Gaussian kernel</cell><cell>Three-level class</cell><cell>Feature fusion</cell><cell>Accuracy 59.57-arousal 57.44-valence</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,84.05,90.65,421.07,207.90"><head>Table 2 .</head><label>2</label><figDesc xml:id="_eanU7NA">Cont.    </figDesc><table coords="9,84.05,110.35,421.07,188.20"><row><cell cols="2">Reference Open Data Source</cell><cell>Classifiers</cell><cell>Classification</cell><cell>Feature Selection/Processing</cell><cell>Measure of Performance (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average accuracy</cell></row><row><cell>[50] 2017</cell><cell>DEAP</cell><cell>End-to-end deep learning neural networks</cell><cell>Two-level class</cell><cell>Raw EEG signals</cell><cell>85.65-arousal 85.45-valence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>87.99-liking</cell></row><row><cell>[51] 2014</cell><cell>DEAP</cell><cell>Deep learning network</cell><cell>Three-level class</cell><cell>PCA PCA CSA CSA</cell><cell>Accuracy 50.88-valence 48.64-arousal 53.42-valence 52.03-arousal</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F1 score</cell></row><row><cell>[52] 2018</cell><cell>DEAP</cell><cell>3D Convolutional Neural Networks</cell><cell>Two-level class</cell><cell>Spatiotemporal features are obtained from EEG signals</cell><cell>86-valence 86-arousal Accuracy 87.44-valence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>88.49-arousal</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Correct rate</cell></row><row><cell>[53] 2014</cell><cell>-</cell><cell>RF</cell><cell>Quinary classification: amusement, anger, grief, fear, baseline</cell><cell>Correlation Analysis and t-test</cell><cell>25.6-amusement 36.4-anger 74.8-grief 80.1-fear</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>88.1-baseline</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,68.29,653.24,452.98,180.00"><head>Table 3 .</head><label>3</label><figDesc xml:id="_eExY4Ha">Valence, arousal, and dominance intervals for the six basic emotions.</figDesc><table coords="9,68.29,672.66,452.98,160.59"><row><cell></cell><cell>Valence</cell><cell></cell><cell>Arousal</cell><cell></cell><cell cols="2">Dominance</cell></row><row><cell></cell><cell></cell><cell>Rating</cell><cell></cell><cell>Rating</cell><cell></cell><cell>Rating</cell></row><row><cell></cell><cell></cell><cell>adapted</cell><cell></cell><cell>adapted</cell><cell></cell><cell>adapted</cell></row><row><cell></cell><cell>Rating from [3]</cell><cell>from the</cell><cell>Rating from [3]</cell><cell>from the</cell><cell>Rating from [3]</cell><cell>from the</cell></row><row><cell></cell><cell></cell><cell>DEAP</cell><cell></cell><cell>DEAP</cell><cell></cell><cell>DEAP</cell></row><row><cell></cell><cell></cell><cell>database</cell><cell></cell><cell>database</cell><cell></cell><cell>database</cell></row><row><cell>Anger</cell><cell>-0.43</cell><cell>Low [1; 5)</cell><cell>0.67</cell><cell>High [5; 9]</cell><cell>0.34</cell><cell>[6;7]</cell></row><row><cell>Joy</cell><cell>0.76</cell><cell>High [5;9]</cell><cell>0.48</cell><cell>High [5;9]</cell><cell>0.35</cell><cell>[6;7]</cell></row><row><cell>Surprise</cell><cell>0.4</cell><cell>High [5;9]</cell><cell>0.67</cell><cell>High [5;9]</cell><cell>-0.13</cell><cell>[4;5]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,106.73,90.76,381.80,156.66"><head>Table 3 .</head><label>3</label><figDesc xml:id="_usVnbZ2">Valence, arousal, and dominance intervals for the six basic emotions.</figDesc><table coords="10,106.73,110.35,381.80,137.08"><row><cell></cell><cell></cell><cell>Valence</cell><cell></cell><cell>Arousal</cell><cell cols="2">Dominance</cell></row><row><cell cols="2">Rating from [3]</cell><cell>Rating Adapted from the DEAP Database</cell><cell>Rating from [3]</cell><cell>Rating Adapted from the DEAP Database</cell><cell>Rating from [3]</cell><cell>Rating Adapted from the DEAP Database</cell></row><row><cell>Anger</cell><cell>-0.43</cell><cell>Low [1; 5)</cell><cell>0.67</cell><cell>High [5; 9]</cell><cell>0.34</cell><cell>[6;7]</cell></row><row><cell>Joy</cell><cell>0.76</cell><cell>High [5;9]</cell><cell>0.48</cell><cell>High [5;9]</cell><cell>0.35</cell><cell>[6;7]</cell></row><row><cell>Surprise</cell><cell>0.4</cell><cell>High [5;9]</cell><cell>0.67</cell><cell>High [5;9]</cell><cell>-0.13</cell><cell>[4;5]</cell></row><row><cell>Disgust</cell><cell>-0.6</cell><cell>Low [1; 5)</cell><cell>0.35</cell><cell>High [5; 9]</cell><cell>0.11</cell><cell>[5;6]</cell></row><row><cell>Fear</cell><cell>-0.64</cell><cell>Low [1; 5)</cell><cell>0.6</cell><cell>High [5; 9]</cell><cell>-0.43</cell><cell>[3;4]</cell></row><row><cell>Sadness</cell><cell>-0.63</cell><cell>Low [1; 5)</cell><cell>0.27</cell><cell>Low [1; 5)</cell><cell>-0.33</cell><cell>[3;4]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,168.75,265.54,257.48,69.89"><head>Table 4 .</head><label>4</label><figDesc xml:id="_wCzKNgK">Intervals corresponding to Condition 0 and Condition 1. layers and neurons per layer. The machine learning techniques employed were SVM, RF, LDA and kNN. As feature selection algorithms, we used Fisher selection, PCA and SFS.</figDesc><table coords="10,175.36,285.50,245.26,49.94"><row><cell></cell><cell></cell><cell>Valence</cell><cell>Arousal</cell><cell>Dominance</cell></row><row><cell>Anger</cell><cell>No anger (0) Anger (1)</cell><cell>High [5; 9] Low [1; 5)</cell><cell>Low [1; 5) High [5; 9]</cell><cell>[1;6) or (7;9] [6;7]</cell></row></table><note xml:id="_caDJV2j">hidden</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,152.62,130.00,300.17,163.97"><head>Table 5 .</head><label>5</label><figDesc xml:id="_upeaC38">Number of entries for each emotion.</figDesc><table coords="11,152.62,150.05,300.17,143.92"><row><cell></cell><cell>Number of entries</cell><cell>Number of entries</cell><cell>Total number of entries</cell></row><row><cell></cell><cell>Condition 1</cell><cell>Condition 0</cell><cell>(5 s long)</cell></row><row><cell>Anger</cell><cell>Anger 28</cell><cell>No anger 239</cell><cell>672</cell></row><row><cell>Joy</cell><cell>Joy 117</cell><cell>No joy 249</cell><cell>2808</cell></row><row><cell>Surprise</cell><cell>Surprise 201</cell><cell>No surprise 233</cell><cell>4824</cell></row><row><cell>Disgust</cell><cell>Disgust 61</cell><cell>No disgust 186</cell><cell>1464</cell></row><row><cell>Fear</cell><cell>Fear 81</cell><cell>No fear 160</cell><cell>1944</cell></row><row><cell>Sadness</cell><cell>Sadness 89</cell><cell>No sadness 337</cell><cell>2136</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="12,107.43,760.97,363.23,82.98"><head>Table 7 .</head><label>7</label><figDesc xml:id="_2W6SWNg">Classification</figDesc><table coords="12,107.43,788.60,363.23,55.36"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Joy</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Type of Feature</cell><cell>Classifie</cell><cell></cell><cell>Raw</cell><cell cols="2">Petrosian</cell><cell cols="2">Higuchi Fractal Dimension</cell><cell cols="2">Approximate Entropy</cell></row><row><cell>Selectio n</cell><cell>r</cell><cell>F1 Scor</cell><cell>Accurac</cell><cell>F1 Scor</cell><cell>Accurac</cell><cell>F1 Scor</cell><cell>Accurac</cell><cell>F1 Scor</cell><cell>Accurac</cell></row></table><note xml:id="_xh6n2FH">F1 scores and accuracies for Joy. (The numbers written in bold correspond to the maximum F1 scores and accuracies.)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="13,91.61,90.76,408.92,299.78"><head>Table 6 .</head><label>6</label><figDesc xml:id="_nNRmJnU">Classification F1 scores and accuracies for anger. (The numbers written in bold correspond to the maximum F1 scores and accuracies.)</figDesc><table coords="13,91.61,123.17,408.92,267.38"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Anger</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Type of Feature Selection</cell><cell>Classifier</cell><cell>Raw</cell><cell></cell><cell cols="2">Petrosian</cell><cell cols="2">Higuchi Fractal Dimension</cell><cell cols="2">Approximate Entropy</cell></row><row><cell></cell><cell></cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>F1 Score</cell><cell>Accuracy</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell></cell><cell>DNN1</cell><cell>91.22</cell><cell>91.22</cell><cell>90.03</cell><cell>90.03</cell><cell>90.03</cell><cell>90.03</cell><cell>74.46</cell><cell>74.55</cell></row><row><cell></cell><cell>DNN2</cell><cell>87.80</cell><cell>87.76</cell><cell>87.05</cell><cell>87.04</cell><cell>81.24</cell><cell>81.25</cell><cell>73.06</cell><cell>73.07</cell></row><row><cell>No feature</cell><cell>DNN3</cell><cell>93.30</cell><cell>93.30</cell><cell>87.50</cell><cell>87.50</cell><cell>89.73</cell><cell>89.73</cell><cell>75.15</cell><cell>75.15</cell></row><row><cell>selection</cell><cell>DNN4</cell><cell>88.39</cell><cell>88.39</cell><cell>84.97</cell><cell>84.96</cell><cell>80.08</cell><cell>80.21</cell><cell>71.20</cell><cell>71.28</cell></row><row><cell></cell><cell>SVM</cell><cell>92.57</cell><cell>92.58</cell><cell>98.02</cell><cell>98.02</cell><cell>98.02</cell><cell>98.02</cell><cell>68.28</cell><cell>68.32</cell></row><row><cell></cell><cell>RF</cell><cell>96.04</cell><cell>96.04</cell><cell>95.05</cell><cell>95.04</cell><cell>97.52</cell><cell>97.52</cell><cell>92.55</cell><cell>92.57</cell></row><row><cell></cell><cell>LDA</cell><cell>85.64</cell><cell>85.63</cell><cell>92.08</cell><cell>92.08</cell><cell>96.04</cell><cell>96.04</cell><cell>68.81</cell><cell>68.81</cell></row><row><cell></cell><cell>kNN</cell><cell>93.56</cell><cell>93.53</cell><cell>95.05</cell><cell>95.04</cell><cell>97.03</cell><cell>97.03</cell><cell>85.15</cell><cell>85.15</cell></row><row><cell></cell><cell>SVM</cell><cell>86.14</cell><cell>86.04</cell><cell>95.05</cell><cell>95.05</cell><cell>94.05</cell><cell>94.06</cell><cell>69.70</cell><cell>69.80</cell></row><row><cell>Fisher</cell><cell>RF</cell><cell>95.54</cell><cell>95.54</cell><cell>92.57</cell><cell>92.56</cell><cell>88.15</cell><cell>88.12</cell><cell>92.08</cell><cell>92.08</cell></row><row><cell></cell><cell>LDA</cell><cell>80.69</cell><cell>80.64</cell><cell>93.07</cell><cell>93.07</cell><cell>87.12</cell><cell>87.13</cell><cell>61.74</cell><cell>62.38</cell></row><row><cell></cell><cell>kNN</cell><cell>97.52</cell><cell>97.52</cell><cell>93.56</cell><cell>93.55</cell><cell>93.56</cell><cell>93.56</cell><cell>88.09</cell><cell>88.12</cell></row><row><cell></cell><cell>SVM</cell><cell>93.42</cell><cell>93.42</cell><cell>97.67</cell><cell>97.67</cell><cell>98.32</cell><cell>98.32</cell><cell>81.93</cell><cell>82.08</cell></row><row><cell>PCA</cell><cell>RF</cell><cell>92.43</cell><cell>92.42</cell><cell>92.28</cell><cell>92.26</cell><cell>93.62</cell><cell>93.61</cell><cell>86.42</cell><cell>86.44</cell></row><row><cell></cell><cell>LDA</cell><cell>81.24</cell><cell>81.15</cell><cell>91.73</cell><cell>91.73</cell><cell>94.75</cell><cell>94.75</cell><cell>65.93</cell><cell>66.09</cell></row><row><cell></cell><cell>kNN</cell><cell>93.96</cell><cell>93.95</cell><cell>95.05</cell><cell>95.05</cell><cell>95.59</cell><cell>95.59</cell><cell>87.37</cell><cell>87.38</cell></row><row><cell></cell><cell>SVM</cell><cell>91</cell><cell>91</cell><cell>91</cell><cell>91</cell><cell>84</cell><cell>84</cell><cell>71</cell><cell>71</cell></row><row><cell>SFS</cell><cell>RF</cell><cell>86</cell><cell>86</cell><cell>86</cell><cell>86</cell><cell>83</cell><cell>83</cell><cell>78</cell><cell>78</cell></row><row><cell></cell><cell>LDA</cell><cell>91</cell><cell>91</cell><cell>91</cell><cell>91</cell><cell>85</cell><cell>85</cell><cell>66</cell><cell>66</cell></row><row><cell></cell><cell>kNN</cell><cell>91</cell><cell>91</cell><cell>91</cell><cell>91</cell><cell>83</cell><cell>83</cell><cell>79</cell><cell>79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="13,93.19,408.83,407.34,299.78"><head>Table 7 .</head><label>7</label><figDesc xml:id="_ejxPVHM">Classification F1 scores and accuracies for Joy. (The numbers written in bold correspond to the maximum F1 scores and accuracies.)</figDesc><table coords="13,93.19,441.23,407.34,267.37"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Joy</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Type of Feature Selection</cell><cell>Classifier</cell><cell>Raw</cell><cell></cell><cell cols="2">Petrosian</cell><cell cols="2">Higuchi Fractal Dimension</cell><cell cols="2">Approximate Entropy</cell></row><row><cell></cell><cell></cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>F1 Score</cell><cell>Accuracy</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell></cell><cell>DNN1</cell><cell>82.29</cell><cell>82.30</cell><cell>80.62</cell><cell>80.63</cell><cell>79.46</cell><cell>79.49</cell><cell>69.96</cell><cell>70.16</cell></row><row><cell></cell><cell>DNN2</cell><cell>80.30</cell><cell>80.34</cell><cell>78.10</cell><cell>78.10</cell><cell>76.14</cell><cell>76.18</cell><cell>67.24</cell><cell>67.38</cell></row><row><cell>No feature</cell><cell>DNN3</cell><cell>83.62</cell><cell>83.65</cell><cell>80.91</cell><cell>80.91</cell><cell>80.51</cell><cell>80.52</cell><cell>72.14</cell><cell>72.15</cell></row><row><cell>selection</cell><cell>DNN4</cell><cell>81.58</cell><cell>81.59</cell><cell>79.02</cell><cell>79.02</cell><cell>76.80</cell><cell>76.85</cell><cell>67.17</cell><cell>67.45</cell></row><row><cell></cell><cell>SVM</cell><cell>83.60</cell><cell>83.63</cell><cell>86.47</cell><cell>86.48</cell><cell>84.45</cell><cell>84.46</cell><cell>65.15</cell><cell>65.95</cell></row><row><cell></cell><cell>RF</cell><cell>90.25</cell><cell>90.27</cell><cell>86.31</cell><cell>86.36</cell><cell>87.57</cell><cell>87.66</cell><cell>86.40</cell><cell>86.48</cell></row><row><cell></cell><cell>LDA</cell><cell>71.63</cell><cell>71.65</cell><cell>70.94</cell><cell>70.94</cell><cell>72.12</cell><cell>72.12</cell><cell>65.02</cell><cell>65.12</cell></row><row><cell></cell><cell>kNN</cell><cell>91.22</cell><cell>91.22</cell><cell>87.90</cell><cell>87.90</cell><cell>87.60</cell><cell>87.60</cell><cell>83.35</cell><cell>83.39</cell></row><row><cell></cell><cell>SVM</cell><cell>78.48</cell><cell>78.65</cell><cell>83.98</cell><cell>83.99</cell><cell>79.58</cell><cell>79.60</cell><cell>68.65</cell><cell>69.16</cell></row><row><cell>Fisher</cell><cell>RF</cell><cell>89.55</cell><cell>89.56</cell><cell>80.64</cell><cell>80.78</cell><cell>81.09</cell><cell>81.14</cell><cell>80.37</cell><cell>80.43</cell></row><row><cell></cell><cell>LDA</cell><cell>64.03</cell><cell>64.29</cell><cell>68.82</cell><cell>68.92</cell><cell>67.03</cell><cell>67.02</cell><cell>64.44</cell><cell>64.53</cell></row><row><cell></cell><cell>kNN</cell><cell>89.92</cell><cell>89.92</cell><cell>85.76</cell><cell>85.77</cell><cell>83.75</cell><cell>83.75</cell><cell>74.01</cell><cell>74.02</cell></row><row><cell></cell><cell>SVM</cell><cell>83.17</cell><cell>83.21</cell><cell>86.39</cell><cell>86.39</cell><cell>84.95</cell><cell>84.96</cell><cell>72.01</cell><cell>72.41</cell></row><row><cell>PCA</cell><cell>RF</cell><cell>88.48</cell><cell>88.51</cell><cell>81.83</cell><cell>81.91</cell><cell>84.91</cell><cell>84.95</cell><cell>82.20</cell><cell>82.20</cell></row><row><cell></cell><cell>LDA</cell><cell>70.62</cell><cell>70.77</cell><cell>71.71</cell><cell>71.71</cell><cell>72.32</cell><cell>72.33</cell><cell>63.54</cell><cell>63.74</cell></row><row><cell></cell><cell>kNN</cell><cell>89.83</cell><cell>89.83</cell><cell>88.08</cell><cell>88.08</cell><cell>87.55</cell><cell>87.56</cell><cell>82.25</cell><cell>82.27</cell></row><row><cell></cell><cell>SVM</cell><cell>98</cell><cell>98</cell><cell>67</cell><cell>67</cell><cell>67</cell><cell>67</cell><cell>66</cell><cell>66</cell></row><row><cell>SFS</cell><cell>RF</cell><cell>96</cell><cell>96</cell><cell>70</cell><cell>70</cell><cell>70</cell><cell>70</cell><cell>70</cell><cell>70</cell></row><row><cell></cell><cell>LDA</cell><cell>100</cell><cell>100</cell><cell>65</cell><cell>65</cell><cell>65</cell><cell>65</cell><cell>61</cell><cell>61</cell></row><row><cell></cell><cell>kNN</cell><cell>98</cell><cell>98</cell><cell>66</cell><cell>66</cell><cell>66</cell><cell>66</cell><cell>66</cell><cell>66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="14,91.61,90.76,408.92,299.78"><head>Table 8 .</head><label>8</label><figDesc xml:id="_bjB3KDk">Classification F1 scores and accuracies for surprise. (The numbers written in bold correspond to the maximum F1 scores and accuracies.)</figDesc><table coords="14,91.61,123.17,408.92,267.38"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Surprise</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Type of Feature Selection</cell><cell>Classifier</cell><cell>Raw</cell><cell></cell><cell cols="2">Petrosian</cell><cell cols="2">Higuchi Fractal Dimension</cell><cell cols="2">Approximate Entropy</cell></row><row><cell></cell><cell></cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>F1 Score</cell><cell>Accuracy</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell></cell><cell>DNN1</cell><cell>70.89</cell><cell>70.94</cell><cell>78.04</cell><cell>78.05</cell><cell>76.41</cell><cell>76.43</cell><cell>71.50</cell><cell>71.54</cell></row><row><cell></cell><cell>DNN2</cell><cell>69.74</cell><cell>69.88</cell><cell>74.94</cell><cell>74.94</cell><cell>74.19</cell><cell>74.23</cell><cell>69.55</cell><cell>69.57</cell></row><row><cell>No feature</cell><cell>DNN3</cell><cell>71.41</cell><cell>71.41</cell><cell>77.67</cell><cell>77.67</cell><cell>78.05</cell><cell>78.11</cell><cell>70.47</cell><cell>71.02</cell></row><row><cell>selection</cell><cell>DNN4</cell><cell>68.92</cell><cell>68.93</cell><cell>76.12</cell><cell>76.12</cell><cell>74.42</cell><cell>74.42</cell><cell>69.61</cell><cell>69.94</cell></row><row><cell></cell><cell>SVM</cell><cell>71.52</cell><cell>71.75</cell><cell>80.92</cell><cell>80.94</cell><cell>81.84</cell><cell>81.84</cell><cell>63.25</cell><cell>65.06</cell></row><row><cell></cell><cell>RF</cell><cell>83.91</cell><cell>83.98</cell><cell>82.12</cell><cell>82.25</cell><cell>80.51</cell><cell>80.80</cell><cell>81.22</cell><cell>81.49</cell></row><row><cell></cell><cell>LDA</cell><cell>59.79</cell><cell>59.88</cell><cell>63.73</cell><cell>63.74</cell><cell>67.73</cell><cell>67.75</cell><cell>59.74</cell><cell>59.74</cell></row><row><cell></cell><cell>kNN</cell><cell>85.01</cell><cell>85.01</cell><cell>84.74</cell><cell>84.74</cell><cell>83.64</cell><cell>83.63</cell><cell>81.30</cell><cell>81.30</cell></row><row><cell></cell><cell>SVM</cell><cell>70.37</cell><cell>70.86</cell><cell>75.76</cell><cell>75.76</cell><cell>72.50</cell><cell>72.51</cell><cell>62.21</cell><cell>63.54</cell></row><row><cell>Fisher</cell><cell>RF</cell><cell>81.85</cell><cell>81.91</cell><cell>76.97</cell><cell>77.07</cell><cell>80.69</cell><cell>80.80</cell><cell>82.59</cell><cell>82.73</cell></row><row><cell></cell><cell>LDA</cell><cell>62.52</cell><cell>62.71</cell><cell>58.48</cell><cell>58.49</cell><cell>60.43</cell><cell>60.43</cell><cell>59.58</cell><cell>59.60</cell></row><row><cell></cell><cell>kNN</cell><cell>80.94</cell><cell>80.94</cell><cell>80.59</cell><cell>80.59</cell><cell>79.14</cell><cell>79.14</cell><cell>79.42</cell><cell>79.42</cell></row><row><cell></cell><cell>SVM</cell><cell>73.57</cell><cell>73.71</cell><cell>80.74</cell><cell>80.74</cell><cell>80.20</cell><cell>80.20</cell><cell>70.46</cell><cell>70.50</cell></row><row><cell>PCA</cell><cell>RF</cell><cell>81.34</cell><cell>81.40</cell><cell>79.21</cell><cell>79.29</cell><cell>81.47</cell><cell>81.52</cell><cell>78.36</cell><cell>78.42</cell></row><row><cell></cell><cell>LDA</cell><cell>60.65</cell><cell>60.71</cell><cell>65.60</cell><cell>65.62</cell><cell>66.81</cell><cell>66.84</cell><cell>60.02</cell><cell>60.12</cell></row><row><cell></cell><cell>kNN</cell><cell>83.60</cell><cell>83.60</cell><cell>84.81</cell><cell>84.81</cell><cell>82.94</cell><cell>82.94</cell><cell>79.71</cell><cell>79.72</cell></row><row><cell></cell><cell>SVM</cell><cell>96</cell><cell>96</cell><cell>61</cell><cell>61</cell><cell>61</cell><cell>61</cell><cell>61</cell><cell>61</cell></row><row><cell>SFS</cell><cell>RF</cell><cell>90</cell><cell>90</cell><cell>66</cell><cell>66</cell><cell>64</cell><cell>64</cell><cell>65</cell><cell>65</cell></row><row><cell></cell><cell>LDA</cell><cell>93</cell><cell>93</cell><cell>58</cell><cell>58</cell><cell>58</cell><cell>58</cell><cell>61</cell><cell>61</cell></row><row><cell></cell><cell>kNN</cell><cell>92</cell><cell>92</cell><cell>62</cell><cell>62</cell><cell>62</cell><cell>62</cell><cell>63</cell><cell>63</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="14,91.61,408.83,408.92,299.78"><head>Table 9 .</head><label>9</label><figDesc xml:id="_CadhEZT">Classification F1 scores and accuracies for Disgust. (The numbers written in bold correspond to the maximum F1 scores and accuracies.).</figDesc><table coords="14,91.61,441.23,408.92,267.37"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Disgust</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Type of Feature Selection</cell><cell>Classifier</cell><cell>Raw</cell><cell></cell><cell cols="2">Petrosian</cell><cell cols="2">Higouchi Fractal Dimension</cell><cell cols="2">Approximate Entropy</cell></row><row><cell></cell><cell></cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>F1 Score</cell><cell>Accuracy</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell></cell><cell>DNN1</cell><cell>84.65</cell><cell>84.70</cell><cell>85.04</cell><cell>85.04</cell><cell>87.08</cell><cell>87.09</cell><cell>68.90</cell><cell>68.99</cell></row><row><cell></cell><cell>DNN2</cell><cell>80.80</cell><cell>80.81</cell><cell>84.02</cell><cell>84.02</cell><cell>82.79</cell><cell>82.79</cell><cell>65.38</cell><cell>65.71</cell></row><row><cell>No feature</cell><cell>DNN3</cell><cell>87.07</cell><cell>87.09</cell><cell>85.92</cell><cell>85.93</cell><cell>87.70</cell><cell>87.70</cell><cell>67.84</cell><cell>68.44</cell></row><row><cell>selection</cell><cell>DNN4</cell><cell>82.57</cell><cell>82.65</cell><cell>83.54</cell><cell>83.54</cell><cell>81.56</cell><cell>81.56</cell><cell>65.96</cell><cell>66.80</cell></row><row><cell></cell><cell>SVM</cell><cell>86.82</cell><cell>86.82</cell><cell>91.13</cell><cell>91.14</cell><cell>91.59</cell><cell>91.59</cell><cell>64.27</cell><cell>65</cell></row><row><cell></cell><cell>RF</cell><cell>93.63</cell><cell>93.64</cell><cell>91.36</cell><cell>91.36</cell><cell>90.19</cell><cell>90.23</cell><cell>83.14</cell><cell>83.18</cell></row><row><cell></cell><cell>LDA</cell><cell>74.72</cell><cell>74.77</cell><cell>84.32</cell><cell>84.32</cell><cell>85.91</cell><cell>85.91</cell><cell>58.72</cell><cell>59.09</cell></row><row><cell></cell><cell>kNN</cell><cell>92.03</cell><cell>92.05</cell><cell>95</cell><cell>95</cell><cell>91.36</cell><cell>91.36</cell><cell>82.25</cell><cell>82.27</cell></row><row><cell></cell><cell>SVM</cell><cell>83.20</cell><cell>83.18</cell><cell>90</cell><cell>90</cell><cell>90.22</cell><cell>90.23</cell><cell>64.77</cell><cell>65.45</cell></row><row><cell>Fisher</cell><cell>RF</cell><cell>89.32</cell><cell>89.32</cell><cell>84.52</cell><cell>84.55</cell><cell>90.23</cell><cell>90.23</cell><cell>75.26</cell><cell>75.23</cell></row><row><cell></cell><cell>LDA</cell><cell>72.27</cell><cell>72.27</cell><cell>80.45</cell><cell>80.45</cell><cell>83.39</cell><cell>83.41</cell><cell>61.97</cell><cell>62.73</cell></row><row><cell></cell><cell>kNN</cell><cell>89.74</cell><cell>89.77</cell><cell>88.64</cell><cell>88.64</cell><cell>89.31</cell><cell>89.32</cell><cell>66.60</cell><cell>66.59</cell></row><row><cell></cell><cell>SVM</cell><cell>86.40</cell><cell>86.41</cell><cell>92.93</cell><cell>92.93</cell><cell>93.55</cell><cell>93.55</cell><cell>74.77</cell><cell>74.95</cell></row><row><cell>PCA</cell><cell>RF</cell><cell>87.42</cell><cell>87.43</cell><cell>87.64</cell><cell>87.66</cell><cell>90.11</cell><cell>90.11</cell><cell>78.74</cell><cell>78.80</cell></row><row><cell></cell><cell>LDA</cell><cell>72.37</cell><cell>72.43</cell><cell>85.28</cell><cell>85.30</cell><cell>87.52</cell><cell>87.52</cell><cell>61.61</cell><cell>62</cell></row><row><cell></cell><cell>kNN</cell><cell>90.84</cell><cell>90.84</cell><cell>93.89</cell><cell>93.89</cell><cell>92.43</cell><cell>92.43</cell><cell>82.05</cell><cell>82.05</cell></row><row><cell></cell><cell>SVM</cell><cell>72</cell><cell>72</cell><cell>73</cell><cell>73</cell><cell>75</cell><cell>75</cell><cell>64</cell><cell>64</cell></row><row><cell>SFS</cell><cell>RF</cell><cell>65</cell><cell>65</cell><cell>65</cell><cell>65</cell><cell>66</cell><cell>66</cell><cell>62</cell><cell>62</cell></row><row><cell></cell><cell>LDA</cell><cell>76</cell><cell>76</cell><cell>77</cell><cell>77</cell><cell>74</cell><cell>74</cell><cell>62</cell><cell>62</cell></row><row><cell></cell><cell>kNN</cell><cell>74</cell><cell>74</cell><cell>73</cell><cell>73</cell><cell>67</cell><cell>67</cell><cell>57</cell><cell>57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="15,93.19,90.76,407.34,299.78"><head>Table 10 .</head><label>10</label><figDesc xml:id="_wg398Md">Classification F1 scores and accuracies for fear. (The numbers written in bold correspond to the maximum F1 scores and accuracies.).</figDesc><table coords="15,93.19,123.17,407.34,267.38"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fear</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Type of Feature Selection</cell><cell>Classifier</cell><cell>Raw</cell><cell></cell><cell cols="2">Petrosian</cell><cell cols="2">Higuchi Fractal Dimension</cell><cell cols="2">Approximate Entropy</cell></row><row><cell></cell><cell></cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>F1 Score</cell><cell>Accuracy</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell></cell><cell>DNN1</cell><cell>82.86</cell><cell>82.87</cell><cell>78.54</cell><cell>78.55</cell><cell>81.22</cell><cell>81.22</cell><cell>66.96</cell><cell>67.03</cell></row><row><cell></cell><cell>DNN2</cell><cell>79.45</cell><cell>79.53</cell><cell>75.31</cell><cell>75.31</cell><cell>79.83</cell><cell>79.84</cell><cell>63.69</cell><cell>63.84</cell></row><row><cell>No feature</cell><cell>DNN3</cell><cell>84.88</cell><cell>84.93</cell><cell>78.61</cell><cell>78.65</cell><cell>80.97</cell><cell>81.02</cell><cell>67.25</cell><cell>67.34</cell></row><row><cell>selection</cell><cell>DNN4</cell><cell>82.33</cell><cell>82.46</cell><cell>75.87</cell><cell>75.87</cell><cell>78.65</cell><cell>78.65</cell><cell>63.26</cell><cell>63.32</cell></row><row><cell></cell><cell>SVM</cell><cell>80.21</cell><cell>80.48</cell><cell>86.82</cell><cell>86.82</cell><cell>87.15</cell><cell>87.16</cell><cell>66.02</cell><cell>66.95</cell></row><row><cell></cell><cell>RF</cell><cell>89.52</cell><cell>89.55</cell><cell>88.20</cell><cell>88.18</cell><cell>84.41</cell><cell>84.42</cell><cell>79.26</cell><cell>79.28</cell></row><row><cell></cell><cell>LDA</cell><cell>68.64</cell><cell>68.66</cell><cell>70.93</cell><cell>71.23</cell><cell>77.86</cell><cell>77.91</cell><cell>57.15</cell><cell>57.36</cell></row><row><cell></cell><cell>kNN</cell><cell>90.75</cell><cell>90.75</cell><cell>89.72</cell><cell>89.73</cell><cell>89.04</cell><cell>89.04</cell><cell>80.66</cell><cell>80.65</cell></row><row><cell></cell><cell>SVM</cell><cell>74.37</cell><cell>74.49</cell><cell>78.50</cell><cell>78.60</cell><cell>82.36</cell><cell>82.36</cell><cell>67.72</cell><cell>68.84</cell></row><row><cell>Fisher</cell><cell>RF</cell><cell>88.85</cell><cell>88.87</cell><cell>78.18</cell><cell>78.25</cell><cell>80.39</cell><cell>80.48</cell><cell>79.27</cell><cell>79.28</cell></row><row><cell></cell><cell>LDA</cell><cell>65.24</cell><cell>65.24</cell><cell>69.39</cell><cell>69.52</cell><cell>72.43</cell><cell>72.43</cell><cell>59.32</cell><cell>59.76</cell></row><row><cell></cell><cell>kNN</cell><cell>86.98</cell><cell>86.99</cell><cell>80.82</cell><cell>80.82</cell><cell>83.39</cell><cell>83.39</cell><cell>79.42</cell><cell>79.45</cell></row><row><cell></cell><cell>SVM</cell><cell>80.53</cell><cell>80.77</cell><cell>87.25</cell><cell>87.26</cell><cell>89.77</cell><cell>89.78</cell><cell>72.73</cell><cell>73.39</cell></row><row><cell>PCA</cell><cell>RF</cell><cell>86.98</cell><cell>87.02</cell><cell>82.71</cell><cell>82.77</cell><cell>86.74</cell><cell>86.76</cell><cell>76.75</cell><cell>76.78</cell></row><row><cell></cell><cell>LDA</cell><cell>62.09</cell><cell>62.14</cell><cell>73.18</cell><cell>73.20</cell><cell>77.19</cell><cell>77.19</cell><cell>57.62</cell><cell>57.69</cell></row><row><cell></cell><cell>kNN</cell><cell>89.21</cell><cell>89.23</cell><cell>89.95</cell><cell>89.95</cell><cell>89.38</cell><cell>89.38</cell><cell>82.25</cell><cell>82.26</cell></row><row><cell></cell><cell>SVM</cell><cell>65</cell><cell>65</cell><cell>66</cell><cell>66</cell><cell>65</cell><cell>65</cell><cell>60</cell><cell>60</cell></row><row><cell>SFS</cell><cell>RF</cell><cell>61</cell><cell>61</cell><cell>61</cell><cell>61</cell><cell>62</cell><cell>62</cell><cell>61</cell><cell>61</cell></row><row><cell></cell><cell>LDA</cell><cell>69</cell><cell>69</cell><cell>69</cell><cell>69</cell><cell>73</cell><cell>73</cell><cell>59</cell><cell>59</cell></row><row><cell></cell><cell>kNN</cell><cell>61</cell><cell>61</cell><cell>61</cell><cell>61</cell><cell>65</cell><cell>65</cell><cell>59</cell><cell>59</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="15,91.61,408.83,408.92,299.78"><head>Table 11 .</head><label>11</label><figDesc xml:id="_HzBQzDR">Classification F1 scores and accuracies for sadness. (The numbers written in bold correspond to the maximum F1 scores and accuracies.).</figDesc><table coords="15,91.61,441.23,408.92,267.37"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Sadness</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Type of Feature Selection</cell><cell>Classifier</cell><cell>Raw</cell><cell></cell><cell cols="2">Petrosian</cell><cell cols="2">Higuchi Fractal Dimension</cell><cell cols="2">Approximate Entropy</cell></row><row><cell></cell><cell></cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>F1 Score</cell><cell>Accuracy</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell></cell><cell>DNN1</cell><cell>80.17</cell><cell>80.20</cell><cell>81.79</cell><cell>81.79</cell><cell>83.70</cell><cell>83.71</cell><cell>68.11</cell><cell>68.12</cell></row><row><cell></cell><cell>DNN2</cell><cell>78.52</cell><cell>78.56</cell><cell>79.07</cell><cell>79.07</cell><cell>82.49</cell><cell>82.49</cell><cell>67.39</cell><cell>67.46</cell></row><row><cell>No feature</cell><cell>DNN3</cell><cell>81.72</cell><cell>81.74</cell><cell>81.95</cell><cell>81.98</cell><cell>83.31</cell><cell>83.33</cell><cell>69.71</cell><cell>69.76</cell></row><row><cell>selection</cell><cell>DNN4</cell><cell>79.56</cell><cell>79.59</cell><cell>79.19</cell><cell>79.21</cell><cell>83.13</cell><cell>83.15</cell><cell>65.90</cell><cell>66.10</cell></row><row><cell></cell><cell>SVM</cell><cell>76.26</cell><cell>76.91</cell><cell>86.90</cell><cell>86.90</cell><cell>90.80</cell><cell>90.80</cell><cell>65.21</cell><cell>65.68</cell></row><row><cell></cell><cell>RF</cell><cell>87.49</cell><cell>87.52</cell><cell>84.18</cell><cell>84.24</cell><cell>84.52</cell><cell>84.56</cell><cell>81.86</cell><cell>81.90</cell></row><row><cell></cell><cell>LDA</cell><cell>69.37</cell><cell>69.42</cell><cell>75.82</cell><cell>75.82</cell><cell>82.37</cell><cell>82.37</cell><cell>47.12</cell><cell>51.17</cell></row><row><cell></cell><cell>kNN</cell><cell>86.81</cell><cell>86.90</cell><cell>90.17</cell><cell>90.17</cell><cell>89.06</cell><cell>89.08</cell><cell>80.50</cell><cell>80.50</cell></row><row><cell></cell><cell>SVM</cell><cell>73.96</cell><cell>74.26</cell><cell>80.97</cell><cell>80.97</cell><cell>86.43</cell><cell>86.43</cell><cell>59</cell><cell>60.22</cell></row><row><cell>Fisher</cell><cell>RF</cell><cell>84.24</cell><cell>84.24</cell><cell>81.37</cell><cell>81.44</cell><cell>84.83</cell><cell>84.87</cell><cell>64.43</cell><cell>64.43</cell></row><row><cell></cell><cell>LDA</cell><cell>66.07</cell><cell>66.61</cell><cell>69.61</cell><cell>69.58</cell><cell>78.78</cell><cell>78.78</cell><cell>50.08</cell><cell>50.08</cell></row><row><cell></cell><cell>kNN</cell><cell>85.76</cell><cell>85.80</cell><cell>83.29</cell><cell>83.31</cell><cell>84.71</cell><cell>84.71</cell><cell>76.45</cell><cell>76.44</cell></row><row><cell></cell><cell>SVM</cell><cell>79.63</cell><cell>79.92</cell><cell>87.21</cell><cell>87.21</cell><cell>89.31</cell><cell>89.31</cell><cell>74.56</cell><cell>74.85</cell></row><row><cell>PCA</cell><cell>RF</cell><cell>85.55</cell><cell>85.62</cell><cell>82.38</cell><cell>82.45</cell><cell>86.49</cell><cell>86.52</cell><cell>80.11</cell><cell>80.14</cell></row><row><cell></cell><cell>LDA</cell><cell>58.91</cell><cell>58.99</cell><cell>74.47</cell><cell>74.46</cell><cell>80.31</cell><cell>80.31</cell><cell>52.36</cell><cell>53.29</cell></row><row><cell></cell><cell>kNN</cell><cell>88.14</cell><cell>88.17</cell><cell>88.53</cell><cell>88.53</cell><cell>88.12</cell><cell>88.13</cell><cell>82.56</cell><cell>82.56</cell></row><row><cell></cell><cell>SVM</cell><cell>63</cell><cell>63</cell><cell>68</cell><cell>68</cell><cell>69</cell><cell>69</cell><cell>63</cell><cell>63</cell></row><row><cell>SFS</cell><cell>RF</cell><cell>65</cell><cell>65</cell><cell>66</cell><cell>66</cell><cell>66</cell><cell>66</cell><cell>58</cell><cell>58</cell></row><row><cell></cell><cell>LDA</cell><cell>65</cell><cell>65</cell><cell>67</cell><cell>67</cell><cell>71</cell><cell>71</cell><cell>64</cell><cell>64</cell></row><row><cell></cell><cell>kNN</cell><cell>61</cell><cell>61</cell><cell>63</cell><cell>63</cell><cell>61</cell><cell>61</cell><cell>58</cell><cell>58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="16,115.98,90.76,362.25,509.66"><head>Table 12 .</head><label>12</label><figDesc xml:id="_bn8nX5K">Important features for each emotion.</figDesc><table coords="16,115.98,111.94,362.25,488.48"><row><cell></cell><cell>Raw</cell><cell>Petrosian</cell><cell>Higuchi</cell><cell>Approximate Entropy</cell></row><row><cell></cell><cell>tEMG</cell><cell>F3</cell><cell>FC1</cell><cell>tEMG</cell></row><row><cell></cell><cell>Respiration</cell><cell>AF3</cell><cell>AF3</cell><cell>Respiration</cell></row><row><cell>Anger</cell><cell>O2</cell><cell>tEMG</cell><cell>F3</cell><cell>GSR</cell></row><row><cell></cell><cell>P3</cell><cell>Respiration</cell><cell>CP5</cell><cell>PPG</cell></row><row><cell></cell><cell>C3</cell><cell>FC1</cell><cell>tEMG</cell><cell>vEOG</cell></row><row><cell></cell><cell>GSR</cell><cell>GSR</cell><cell>Cz</cell><cell>GSR</cell></row><row><cell></cell><cell>FC1</cell><cell>Oz</cell><cell>GSR</cell><cell>Respiration</cell></row><row><cell>Joy</cell><cell>PO3</cell><cell>zEMG</cell><cell>P8</cell><cell>zEMG</cell></row><row><cell></cell><cell>C3</cell><cell>O1</cell><cell>P3</cell><cell>hEOG</cell></row><row><cell></cell><cell>Cz</cell><cell>PO3</cell><cell>T7</cell><cell>vEOG</cell></row><row><cell></cell><cell>GSR</cell><cell>GSR</cell><cell>GSR</cell><cell>GSR</cell></row><row><cell></cell><cell>Cz</cell><cell>FC1</cell><cell>FC1</cell><cell>PPG</cell></row><row><cell>Surprise</cell><cell>C3</cell><cell>FC2</cell><cell>Cz</cell><cell>vEOG</cell></row><row><cell></cell><cell>Oz</cell><cell>Cz</cell><cell>P3</cell><cell>Respiration</cell></row><row><cell></cell><cell>C4</cell><cell>CP2</cell><cell>Pz</cell><cell>zEMG</cell></row><row><cell></cell><cell>vEOG</cell><cell>FC2</cell><cell>vEOG</cell><cell>vEOG</cell></row><row><cell></cell><cell>FC5</cell><cell>vEOG</cell><cell>T7</cell><cell>hEOG</cell></row><row><cell>Disgust</cell><cell>C3</cell><cell>Oz</cell><cell>AF3</cell><cell>GSR</cell></row><row><cell></cell><cell>P7</cell><cell>PO3</cell><cell>hEOG</cell><cell>CP5</cell></row><row><cell></cell><cell>Respiration</cell><cell>FP1</cell><cell>CP5</cell><cell>Oz</cell></row><row><cell></cell><cell>tEMG</cell><cell>FC1</cell><cell>FC1</cell><cell>vEOG</cell></row><row><cell></cell><cell>hEOG</cell><cell>F4</cell><cell>F4</cell><cell>zEMG</cell></row><row><cell>Fear</cell><cell>vEOG</cell><cell>T8</cell><cell>FC2</cell><cell>Respiration</cell></row><row><cell></cell><cell>zEMG</cell><cell>Cz</cell><cell>AF4</cell><cell>hEOG</cell></row><row><cell></cell><cell>Cz</cell><cell>FC2</cell><cell>Pz</cell><cell>GSR</cell></row><row><cell></cell><cell>CP1</cell><cell>FC1</cell><cell>FC1</cell><cell>PPG</cell></row><row><cell></cell><cell>F8</cell><cell>FP1</cell><cell>P3</cell><cell>Temperature</cell></row><row><cell>Sadness</cell><cell></cell><cell>AF3</cell><cell>O1</cell><cell>tEMG</cell></row><row><cell></cell><cell>Cz</cell><cell>FC2</cell><cell>FP1</cell><cell>Oz</cell></row><row><cell></cell><cell>Respiration</cell><cell>Oz</cell><cell>AF3</cell><cell>zEMG</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="17,107.54,157.02,379.83,248.72"><head>Table 13 .</head><label>13</label><figDesc xml:id="_R2XBHEB">Best classification F1 scores for each emotion.</figDesc><table coords="17,107.54,176.60,379.83,229.13"><row><cell></cell><cell cols="2">Raw</cell><cell cols="2">Petrosian</cell><cell cols="2">Higuchi Fractal Dimension</cell><cell cols="2">Approximate Entropy</cell></row><row><cell></cell><cell>No Feature Selection</cell><cell>With Feature Selection</cell><cell>No Feature Selection</cell><cell>With Feature Selection</cell><cell>No Feature Selection</cell><cell>With Feature Selection</cell><cell>No Feature Selection</cell><cell>With Feature Selection</cell></row><row><cell>Anger</cell><cell>Random Forest 96.04%</cell><cell>kNN Fisher 97.52%</cell><cell>SVM 98.02%</cell><cell>SVM Fisher 95.05%</cell><cell>SVM 98.02%</cell><cell>SVM Fisher 94.05%</cell><cell>Random Forest 92.55%</cell><cell>Random Forest Fisher 92.08%</cell></row><row><cell>Joy</cell><cell>kNN 91.22%</cell><cell>LDA SFS 100%</cell><cell>kNN 87.9%</cell><cell>kNN Fisher 85.76%</cell><cell>kNN 87.60%</cell><cell>kNN Fisher 83.75%</cell><cell>Random Forest 86.40%</cell><cell>Random Forest Fisher 80.37%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Random</cell><cell></cell><cell></cell></row><row><cell>Surprise</cell><cell>kNN 85.01%</cell><cell>SVM SFS 96%</cell><cell>kNN 84.75%</cell><cell>kNN Fisher 80.59%</cell><cell>kNN 83.64%</cell><cell>Forest Fisher</cell><cell>kNN 81.30%</cell><cell>kNN Fisher 82.59%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80.69%</cell><cell></cell><cell></cell></row><row><cell>Disgust</cell><cell>Random Forest 93.63%</cell><cell>kNN Fisher 89.74%</cell><cell>kNN 95%</cell><cell>SVM Fisher 90%</cell><cell>SVM 91.59%</cell><cell>Random Forest Fisher 90.23%</cell><cell>Random Forest 83.14%</cell><cell>Random Forest Fisher 75.26%</cell></row><row><cell></cell><cell></cell><cell>Random</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fear</cell><cell>kNN 90.75%</cell><cell>Forest Fisher</cell><cell>kNN 89.72%</cell><cell>kNN Fisher 80.82%</cell><cell>kNN 89.04%</cell><cell>kNN Fisher 83.39%</cell><cell>kNN 80.66%</cell><cell>kNN Fisher 79.45%</cell></row><row><cell></cell><cell></cell><cell>80.85%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sadness</cell><cell>Random Forest 87.49%</cell><cell>kNN Fisher 85.76%</cell><cell>kNN 90.17%</cell><cell>kNN Fisher 83.29%</cell><cell>SVM 90.8%</cell><cell>SVM Fisher 86.43%</cell><cell>Random Forest 81.86%</cell><cell>kNN Fisher 76.45%</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_XwRnphC"><p xml:id="_cgD5aEv">The work has been funded by the Operational Programme Human Capital of the Ministry of European Funds through the Financial Agreement 51675/09.07.2019, SMIS code 125125, UEFISCDI project 1/2018 and UPB CRC Research Grant 2017. This work has been funded in part through UEFISCDI, from EEA Grants 2014-2021, project number EEA-RO-NO-2018-0496.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_vB4wgXJ"><p xml:id="_k38WvJP">The DEAP database contains 40 valence/arousal/dominance ratings for each of the 32 subjects. For the emotion of anger, there were 28 ratings in the database for Condition 1-Anger and 239 ratings for the Condition 0-No anger. In order to have a balanced distribution of responses for classification, we used 28 ratings for Condition 1 and 28 ratings for Condition 0, so we took the minimum between both. Every physiological recording had a duration of 60 s. Thus, in order to obtain a larger training database, we have divided the 60 s long recordings into 12 segments, each being 5 s long. Thus, for anger we obtained a training dataset of 672 entries that was fed to the classification algorithms. Table <ref type="table" coords="10,513.66,651.55,5.08,9.58">5</ref> presents, for each emotion, the number of entries for Conditions 0 and 1 and the total number of 5-s long segments that have been fed as input data to the classification algorithms.</p><p xml:id="_TyGRmk6">For binary classifying the emotion ratings into Condition 1 (emotion) and Condition 0 (lack of emotion), we applied four machine and deep learning algorithms, with and without feature selection, similarly to the experiment described in <ref type="bibr" coords="10,250.05,719.10,10.44,9.58" target="#b7">[8]</ref>, where we classified the emotion of fear. Our input features were: EEG (raw values/approximate entropy/Petrosian fractal dimension/Higuchi fractal dimension) and peripheral signals, hEOG, vEOG, zEMG, tEMG, GSR, respiration rate, PPG and temperature. We constructed models based on four deep neural networks (DNN1-DNN4) with various numbers of </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="19,79.90,110.60,438.84,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_9HagEMa">What are emotions? And how can they be measured</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<idno type="DOI">10.1177/0539018405058216</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xFNdMCM">Soc. Sci. Inf</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="693" to="727" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,79.90,123.52,439.96,8.74;19,97.60,136.43,129.46,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_pJT4bRD">Pan-cultural elements in facial displays of emotions</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">R</forename><surname>Sorenson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.164.3875.86</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_gVnxg3h">Science</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="86" to="88" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,79.90,149.34,440.41,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_H5TzqAs">Evidence for a three-factor theory of emotions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mehrabian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5JEf9rt">J. Res. Personal</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="273" to="294" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,79.90,175.28,440.41,8.63;19,98.05,188.07,421.81,8.74;19,97.60,201.10,69.27,8.63" xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_N5PJJ9c">DEAP: A Database for Emotion Analysis using Physiological Signals</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koelstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Muehl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nijholt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<idno type="DOI">10.1109/T-AFFC.2011.15</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hyxyRXT">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,79.90,214.01,438.84,8.63;19,98.05,226.81,245.10,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_rTpKNGf">A multimodal database for affect recognition and implicit tagging</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lichtenauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="DOI">10.1109/T-AFFC.2011.25</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vun5RFe">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="42" to="55" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,79.90,239.84,438.85,8.63;19,98.05,252.63,422.25,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_HGVfq4t">Unsupervised domain adaptation techniques based on auto-encoder for non-stationary EEG-based emotion recognition</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NaJ4KAT">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="205" to="214" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,79.90,278.57,440.41,8.63;19,97.77,291.37,250.46,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_zGFybTs">Dynamical analysis of emotional states from electroencephalogram signals</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Goshvarpour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Abbasi</surname></persName>
		</author>
		<idno type="DOI">10.4015/S1016237216500150</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PARkSM7">Biomed. Eng. Appl. Basis Commun</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2016">2016. 1650015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,79.90,304.39,438.84,8.63;19,98.05,317.19,365.54,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_g9zhXhX">Fear Level Classification Based on Emotional Dimensions and Machine Learning Techniques</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bălan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Moise</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moldoveanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Moldoveanu</surname></persName>
		</author>
		<idno type="DOI">10.3390/s19071738</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_C2FKnmf">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="1738">2019. 1738</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,79.90,330.22,439.19,8.63;19,98.05,343.13,420.69,8.63;19,98.05,356.04,409.49,8.63" xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_hj8jEue">Does automatic game difficulty level adjustment improve acrophobia therapy? Differences from baseline</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bălan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Moise</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moldoveanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Moldoveanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_peUvQGc">Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology</title>
				<meeting>the 24th ACM Symposium on Virtual Reality Software and Technology<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-01">28 November-1 December 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,84.01,368.95,434.74,8.63;19,98.05,381.86,420.69,8.63;19,98.05,394.77,412.07,8.63" xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_Af5SZWY">Automatic adaptation of exposure intensity in VR acrophobia therapy, based on deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bălan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Moise</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moldoveanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Moldoveanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_BmCDWvF">Proceedings of the Twenty-Seventh European Conference on Information Systems (ECIS2019)</title>
				<meeting>the Twenty-Seventh European Conference on Information Systems (ECIS2019)<address><addrLine>Stockholm-Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-14">8-14 June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,84.01,407.57,434.73,8.74;19,97.60,420.60,171.74,8.63" xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_aB29tuj">Basic emotions</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_EHgjZKx">Handbook of Cognition and Emotion</title>
				<editor>
			<persName><forename type="first">T</forename><surname>Dalgleish</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Power</surname></persName>
		</editor>
		<meeting><address><addrLine>Hoboken, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley&amp;Sons Ltd</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,84.01,433.39,436.30,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_Gta6kfp">Against Basic Emotions, and Toward a Comprehensive Theory</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_62m7Twd">J. Mind Behav</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="229" to="254" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,84.01,446.31,217.87,8.74" xml:id="b12">
	<monogr>
		<title level="m" type="main" xml:id="_mJBTa2R">Emotion: A Psychoevolutionary Synthesis</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Plutchik</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="19,304.12,446.42,168.68,8.63" xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Row</forename><surname>Harper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,84.01,459.22,434.73,8.74;19,98.05,472.13,291.14,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_GEaZJtR">The dictionary of affect in language, Emotion: Theory, Research and Experience</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Whissell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_fRTaeym">The Measurement of Emotions</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,84.01,485.04,404.68,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_d34hVYV">A circumplex model of Affect</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0077714</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Pfu2FdK">J. Personal. Soc. Psychol</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1161" to="1178" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,84.01,498.07,434.74,8.63;19,98.05,510.86,331.02,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_2NgyQU2">Using the revised dictionary of affect in language to quantify the emotional undertones of samples of natural language</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Whissell</surname></persName>
		</author>
		<idno type="DOI">10.2466/PR0.105.2.509-521</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_DCUX8fN">Psychol. Rep</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="509" to="521" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,84.01,523.78,434.74,8.74;19,97.77,536.80,163.50,8.63" xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_gxZXrgp">An Approach to Environmental Psychology</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mehrabian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_XyGmfev">The Massachusetts Institute of Technology</title>
				<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,84.01,549.60,435.85,8.74;19,97.80,562.51,222.98,8.74" xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_8Gy5t2a">Framework for a comprehensive description and measurement of emotional states</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mehrabian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_tTEKJzh">Genet. Soc. Gen. Psychol. Monogr</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="339" to="361" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,84.01,575.54,434.73,8.63;19,98.05,588.33,283.89,8.74" xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_EeCZu5e">Pleasure-Arousal-Dominance: A general framework for describing and measuring individual differences in temperament</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mehrabian</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02686918</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TjSBpKR">Curr. Psychol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="261" to="292" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,84.01,601.36,434.73,8.63;19,98.05,614.27,420.69,8.63;19,97.70,627.18,354.06,8.63" xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_5uFvsJV">Emotion analysis as a regression problem-Dimensional models and their implications on emotion representation and metrical evaluation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Buechel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_NAhKppT">Proceedings of the 22nd European Conference on Artificial Intelligence (ECAI)</title>
				<meeting>the 22nd European Conference on Artificial Intelligence (ECAI)<address><addrLine>The Hague, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09">August-2 September 2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,84.01,640.10,434.73,8.63;19,98.05,652.89,365.77,8.74" xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_6E7HD32">Abnormal structure or function of the amygdala is a common component of neurodevelopmental disorders</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Bauman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Amaral</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuropsychologia.2010.09.028</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_sBgabXW">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="745" to="759" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,84.01,665.80,337.73,8.74" xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_XC7e7bA">EEG-based emotion recognition</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">O</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2CYk2Xs">Influ. Vis. Audit. Stimuli</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,84.01,678.71,319.73,8.74" xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_HNV5sbT">Emotional functions of the insula</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZYddqsc">Brain Nerve</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="1103" to="1112" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,84.01,691.63,435.85,8.74;19,98.05,704.65,69.27,8.63" xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_MD57hra">How do you feel now? The anterior insula and human awareness</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">D</forename><surname>Craig</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrn2555</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_bpEr6xj">Nat. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="59" to="70" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,84.01,717.57,434.74,8.63;19,98.05,730.36,420.69,8.74" xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_VTqawAZ">Abnormal hippocampal structure and function in clinical anxiety and comorbid depression</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Blair Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">R</forename><surname>Mujica-Parodi</surname></persName>
		</author>
		<idno type="DOI">10.1002/hipo.22566</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6aEtPmm">Hippocampus</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="545" to="553" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,91.75,434.73,8.63;20,98.05,104.55,188.21,8.74" xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_8wg6BQ2">Arousal and the control of perception and movement</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Garcia-Rill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Virmani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Hyde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>; D'onofrio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mahaffey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8MYSsmG">Curr. Trends Neurol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="53" to="64" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,117.46,435.86,8.74;20,98.05,130.49,131.12,8.63" xml:id="b27">
	<monogr>
		<title level="m" type="main" xml:id="_6YtgbCm">Unmasking the Face: A Guide to Recognizing Emotions from Facial Clues</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,143.40,435.86,8.63;20,98.05,156.31,420.69,8.63;20,98.05,169.10,373.57,8.74" xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_YfbtJxt">Functional atlas of emotional faces processing: A voxel-based meta-analysis of 105 functional magnetic resonance imaging studies</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fusar-Poli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Placentino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Carletti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Landi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Surguladze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Benedetti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abbamonte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gasparotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Barale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_M5XnMkd">J. Psychiatry Neurosci</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="418" to="432" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,182.13,434.74,8.63;20,98.05,194.93,253.49,8.74" xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_bdNnpMg">Emotional intelligence is associated with reduced insula responses to masked angry faces</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alkozei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">D</forename><surname>Killgore</surname></persName>
		</author>
		<idno type="DOI">10.1097/WNR.0000000000000389</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vVMAUrs">Neuroreport</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="567" to="571" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,207.96,434.74,8.63;20,98.05,220.87,417.84,8.63" xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_Z9ncsWf">The personal characteristics of happiness: An EEG study</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_effkjEw">Proceedings of the 2015 International Conference on Orange Technologies (ICOT)</title>
				<meeting>the 2015 International Conference on Orange Technologies (ICOT)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-22">19-22 December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,233.78,434.74,8.63;20,98.05,246.69,421.81,8.63;20,98.05,259.60,80.43,8.63" xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_EUxpzXJ">Emotion classification based on gamma-band EEG</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Q7dmSAv">Proceedings of the 2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
				<meeting>the 2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
			<biblScope unit="page" from="3" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,272.51,434.74,8.63;20,98.05,285.31,301.32,8.74" xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_HEwQEa6">Chapter 3-Technical Aspects of Brain Rhythms and Speech Parameters</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">A</forename><surname>Abhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">W</forename><surname>Gawali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">C</forename><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_6XdqX5G">Introduction to EEG and Speech-Based Emotion Recognition</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="20,403.59,285.42,116.27,8.63;20,98.05,298.34,263.62,8.63" xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">A</forename><surname>Abhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">W</forename><surname>Gawali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">C</forename><surname>Mehrotra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Academic Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,311.13,435.85,8.74;20,98.05,324.04,147.39,8.74" xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_8WKrfdf">Electrical autonomic correlates of emotion</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sequeira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Silvert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Delplanque</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijpsycho.2008.07.009</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TJwYqCu">Int. J. Psychophysiol</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="50" to="56" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,336.96,435.85,8.74;20,97.45,349.87,103.92,8.74" xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_urtwF5F">Mechanisms and controllers of eccrine sweating in humans</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shibasaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G</forename><surname>Crandall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_52MaM3Z">Front. Biosci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="685" to="696" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,362.89,434.74,8.63;20,98.05,375.69,252.19,8.74" xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_b9NyKX6">The discharge behaviour of single sympathetic neurones supplying human sweat glands</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">G</forename><surname>Macefield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">G</forename><surname>Wallin</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0165-1838(96)00095-1</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_tgej8Ac">J. Auton. Nerv. Syst</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="277" to="286" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,388.72,434.74,8.63;20,98.05,401.51,225.02,8.74" xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_VVqjth6">Sympathetic skin nerve discharges in relation to amplitude of skin resistance responses</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lidberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">G</forename><surname>Wallin</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1469-8986.1981.tb03033.x</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_fVb4uCN">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="268" to="270" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,414.42,387.55,8.74" xml:id="b38">
	<monogr>
		<title level="m" type="main" xml:id="_79jhXqA">Electrodermal Activity</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Boucsein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,427.45,435.08,8.63;20,98.05,440.25,421.81,8.74;20,97.77,453.16,170.37,8.74" xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_YGXpk2U">A meta-analysis of heart rate variability and neuroimaging studies: Implications for heart rate variability as a marker of stress and health</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Thayer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Åhs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Sollers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">D</forename><surname>Iii; Wager</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neubiorev.2011.11.009</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5DCAphm">Neurosci. Biobehav. Rev</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="747" to="756" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,466.07,435.86,8.74;20,97.77,478.98,152.93,8.74" xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_nKt6R8S">How heart rate variability affects emotion regulation brain networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mather</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Thayer</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cobeha.2017.12.017</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_uzvNaTA">Curr. Opin. Behav. Sci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="98" to="104" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,491.89,421.56,8.74" xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_pwKSU4U">Breathing rhythms and emotions</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Homma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Masaoka</surname></persName>
		</author>
		<idno type="DOI">10.1113/expphysiol.2008.042424</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UbW7BNV">Exp. Physiol</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="1011" to="1021" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,504.92,435.86,8.63;20,97.73,517.83,421.01,8.63;20,98.05,530.63,233.88,8.74" xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_gm8PEDB">Affective computing in virtual reality: Emotion recognition from brain and heartbeat dynamics using wearable sensors</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Marín-Morales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Higuera-Trujillo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Greco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guixeres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Llinares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">P</forename><surname>Scilingo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Alcañiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Valenza</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-018-32063-4</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kDcy4YH">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2018">2018. 13657</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,543.66,434.73,8.63;20,98.05,556.45,247.34,8.74" xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_b8RAKCr">Improving BCI-based emotion recognition by combining EEG feature selection and kernel classifiers</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2015.10.049</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_sHeFDb5">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="35" to="41" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,569.48,434.73,8.63;20,97.78,582.28,383.54,8.74" xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_hG4nYBs">EEG-based emotion estimation using Bayesian weighted-log-posterior function and perceptron convergence algorithm</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Y</forename><surname>Chung</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compbiomed.2013.10.017</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PfBCVmN">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="2230" to="2237" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,595.30,434.74,8.63;20,98.05,608.21,421.81,8.63;20,98.05,621.13,72.62,8.63" xml:id="b45">
	<analytic>
		<title level="a" type="main" xml:id="_Aawx4gQ">Recognition of emotions induced by music videos using dt-cwpt</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Naser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_VTEQwcH">Proceedings of the 2013 Indian Conference on Medical Informatics and Telemedicine (ICMIT)</title>
				<meeting>the 2013 Indian Conference on Medical Informatics and Telemedicine (ICMIT)<address><addrLine>Kharagpur, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-03">March 2013</date>
			<biblScope unit="page" from="28" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,634.04,434.74,8.63;20,98.05,646.95,420.69,8.63;20,97.70,659.86,336.78,8.63" xml:id="b46">
	<analytic>
		<title level="a" type="main" xml:id="_DumrUWD">Identifying music-induced emotions from EEG for use in brain computer music interfacing</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nasuto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kirke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Miranda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_eq5s2Qa">Proceedings of the 6th Affective Computing and Intelligent Interaction</title>
				<meeting>the 6th Affective Computing and Intelligent Interaction<address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09">September 2015</date>
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,672.77,434.74,8.63;20,98.05,685.68,288.75,8.63" xml:id="b47">
	<analytic>
		<title level="a" type="main" xml:id="_HudypVh">EEG databases for emotion recognition</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Sourina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_zeFDHmn">Proceedings of the 2013 International Conference on Cyberworlds (CW)</title>
				<meeting>the 2013 International Conference on Cyberworlds (CW)<address><addrLine>Okohama, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10">October 2013</date>
			<biblScope unit="page" from="21" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,84.01,698.60,434.73,8.63;20,98.05,711.51,421.81,8.63;20,98.05,724.42,189.98,8.63" xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_KcHDZaD">An empirical study of machine learning techniques for affect recognition in human-robot interaction</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_k2hdhq7">Proceedings of the International Conference on Intelligent Robots and Systems</title>
				<meeting>the International Conference on Intelligent Robots and Systems<address><addrLine>Edmonton, AB, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005-08">August 2005</date>
			<biblScope unit="page" from="2" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,91.75,434.73,8.63;21,98.05,104.66,420.69,8.63;21,97.80,117.46,270.06,8.74" xml:id="b49">
	<analytic>
		<title level="a" type="main" xml:id="_kyZYbU3">Single trial classification of EEG and peripheral physiological signals for recognition of emotions induced by music videos</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koelstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Muhl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nijholt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hAD7e5T">Brain Inform. Ser. Lect. Notes Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">6334</biblScope>
			<biblScope unit="page" from="89" to="100" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,130.49,436.30,8.63;21,98.05,143.28,183.73,8.74" xml:id="b50">
	<analytic>
		<title level="a" type="main" xml:id="_bbrVnrG">Emotion Classification in Arousal Valence Model using MAHNOB-HCI Database</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B H</forename><surname>Wiem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lachiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mDx9aDu">Int. J. Adv. Comput. Sci. Appl</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="318" to="323" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,156.31,436.30,8.63;21,98.05,169.10,228.35,8.74" xml:id="b51">
	<analytic>
		<title level="a" type="main" xml:id="_q3WWGcS">Emotion Recognition based on EEG using LSTM Recurrent Neural Network</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Alhagry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Reda</surname></persName>
		</author>
		<idno type="DOI">10.14569/IJACSA.2017.081046</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YJbGEHp">Int. J. Adv. Comput. Sci. Appl</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="355" to="358" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,182.13,434.99,8.63;21,97.67,194.93,405.62,8.74" xml:id="b52">
	<analytic>
		<title level="a" type="main" xml:id="_tnTSfx8">EEG-based emotion recognition using deep learning network with principal component-based covariate shift adaptation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jirayucharoensak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pan-Ngum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Israsena</surname></persName>
		</author>
		<idno type="DOI">10.1155/2014/627892</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xzJttmy">Sci. World J</title>
		<imprint>
			<biblScope unit="page">627892</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,207.96,434.73,8.63;21,98.05,220.75,317.94,8.74" xml:id="b53">
	<analytic>
		<title level="a" type="main" xml:id="_9C4sXVW">EEG-Based Emotion Recognition using 3D Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">S</forename><surname>Salama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>El-Khoribi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Shoman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A W</forename><surname>Shalaby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Xhcemyy">Int. J. Adv. Comput. Sci. Appl</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="329" to="337" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,233.78,434.73,8.63;21,98.05,246.57,365.21,8.74" xml:id="b54">
	<analytic>
		<title level="a" type="main" xml:id="_aX8EsKD">Emotion Recognition Based on Multi-Variant Correlation of Physiological Signals</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAFFC.2014.2327617</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mx7WavG">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="126" to="140" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,259.49,436.30,8.74" xml:id="b55">
	<analytic>
		<title level="a" type="main" xml:id="_kPfcJcN">Approach to an irregular time series on the basis of the fractal theory</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Higuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9vekJV8">Physica</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="277" to="283" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,285.31,399.76,8.74" xml:id="b56">
	<analytic>
		<title level="a" type="main" xml:id="_REVT9ea">Fractals and the analysis of waveforms</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Katz</surname></persName>
		</author>
		<idno type="DOI">10.1016/0010-4825(88)90041-8</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_wHhBeBN">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="145" to="156" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,298.34,436.30,8.63;21,98.05,311.25,421.81,8.63;21,98.05,324.16,60.50,8.63" xml:id="b57">
	<analytic>
		<title level="a" type="main" xml:id="_NHxpQF4">Kolmogorov complexity of finite sequences and recognition of different preictal EEG patterns</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Petrosian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_RPxmb3Z">Proceedings of the 8th IEEE Symposium on Computer-Based Medical Systems</title>
				<meeting>the 8th IEEE Symposium on Computer-Based Medical Systems<address><addrLine>Lubbock, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="9" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,337.07,434.74,8.63;21,98.05,349.87,334.54,8.74" xml:id="b58">
	<monogr>
		<title level="m" type="main" xml:id="_5rBXdGN">Electroencephalogram approximate entropy influenced by both age and sleep</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fattinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Mouthon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Noirhomme</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Huber</surname></persName>
		</author>
		<idno type="DOI">10.3389/fninf.2013.00033</idno>
		<imprint/>
	</monogr>
	<note>Front. Neuroinform. 2013, 7, 33. [CrossRef</note>
</biblStruct>

<biblStruct coords="21,84.01,362.89,435.85,8.63;21,98.05,375.81,422.26,8.63;21,97.76,388.60,226.16,8.74" xml:id="b59">
	<analytic>
		<title level="a" type="main" xml:id="_JaKXGwW">Approximate entropy in the electroencephalogram during wake and sleep</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Burioka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Miyata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cornélissen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Halberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Takeshima</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Maegaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nomura</surname></persName>
		</author>
		<idno type="DOI">10.1177/155005940503600106</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mn7Rq7a">Clin. EEG Neurosci</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="21" to="24" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,401.63,358.71,8.63" xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Pyeeg</surname></persName>
		</author>
		<ptr target="http://pyeeg.sourceforge.net/" />
		<imprint>
			<date type="published" when="2019-11-01">1 November 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,414.54,393.60,8.63" xml:id="b61">
	<monogr>
		<title level="m" type="main" xml:id="_EQJBYcf">Keras Deep Learning Library</title>
		<ptr target="https://keras.io/" />
		<imprint>
			<date type="published" when="2019-11-01">1 November 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,440.36,434.73,8.63;21,98.05,453.28,420.69,8.63;21,97.60,466.19,294.45,8.63" xml:id="b62">
	<analytic>
		<title level="a" type="main" xml:id="_gQpCu2f">A Bayesian framework for video affective representation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kierkels</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_sKUdZy2">Proceedings of the 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops</title>
				<meeting>the 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,479.10,434.74,8.63;21,98.05,491.89,420.69,8.74;21,97.73,504.81,422.14,8.74" xml:id="b63">
	<analytic>
		<title level="a" type="main" xml:id="_3YtHaZX">Emotion detection: Application of the valence arousal space for rapid biological usability testing to enhance universal access</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Stickel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Steinbach-Nordmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Searle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_n5m6xd2">Universal Access in Human-Computer Interaction; Addressing Diversity</title>
		<title level="s" xml:id="_MSa64A8">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">5614</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,98.05,517.83,308.89,8.63" xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Stephanidis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ed</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="615" to="624" />
			<pubPlace>Berlin/Heidelberg, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,530.63,291.83,8.74" xml:id="b65">
	<monogr>
		<title level="m" type="main" xml:id="_4XZb8m6">Affective Computing</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,543.66,434.73,8.63;21,97.77,556.45,420.97,8.74;21,97.76,569.36,311.24,8.74" xml:id="b66">
	<analytic>
		<title level="a" type="main" xml:id="_NGDCaqD">Towards Affect-sensitive Assistive Intervention Technologies for Children with Autism</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Conn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Warren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_EA6z26U">Affective Computing Focus on Emotion Expression, Synthesis and Recognition</title>
				<meeting><address><addrLine>Or, J., Ed; Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>I-Tech Education and Publishing</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,582.39,435.08,8.63;21,98.05,595.19,385.32,8.74" xml:id="b67">
	<analytic>
		<title level="a" type="main" xml:id="_t9Pc5VE">Learning Emotions EEG-based Recognition and Brain Activity: A Survey Study on BCI for Intelligent Tutoring System</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2018.04.056</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_uXVBkT6">Procedia Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="376" to="382" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,608.21,434.91,8.63;21,98.05,621.13,421.81,8.63;21,98.05,634.04,114.84,8.63" xml:id="b68">
	<analytic>
		<title level="a" type="main" xml:id="_v7jXyY3">Affective recommender systems: The role of emotions in recommender systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tkalčič</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Košir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Juij Tasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_6dAjdDN">Proceedings of the RecSys Workshop Hum. Decision Making Recommender System</title>
				<meeting>the RecSys Workshop Hum. Decision Making Recommender System<address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-10">October 2011</date>
			<biblScope unit="page" from="23" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,646.95,434.73,8.63;21,98.05,659.86,421.07,8.63;21,97.76,672.77,83.30,8.63" xml:id="b69">
	<analytic>
		<title level="a" type="main" xml:id="_SY6Ujte">Emotions and personality in recommender systems: Tutorial</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tkalčič</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_74Rnaaa">Proceedings of the 12th ACM Conference on Recommender Systems (RecSys &apos;18)</title>
				<meeting>the 12th ACM Conference on Recommender Systems (RecSys &apos;18)<address><addrLine>Vancouver, BC, Canada; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-10-07">2-7 October 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,685.57,430.15,8.74" xml:id="b70">
	<analytic>
		<title level="a" type="main" xml:id="_B397Xqm">Affective Computing and Sentiment Analysis</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<idno type="DOI">10.1109/MIS.2016.31</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_JAjp9ba">IEEE Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="102" to="107" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,84.01,698.60,435.85,8.63;21,98.05,711.51,421.04,8.63;21,98.05,724.42,420.69,8.63;21,98.05,737.33,404.73,8.63" xml:id="b71">
	<analytic>
		<title level="a" type="main" xml:id="_5SQZAFz">Mastering an Advanced Sensory Substitution Device for Visually Impaired through Innovative Virtual Training</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moldoveanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ivascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Stanica</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dascalu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ivanica</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bălan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Caraiman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ungureanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Moldoveanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_fDeA9Hz">Proceedings of the 7th IEEE International Conference on Consumer Electronics</title>
				<meeting>the 7th IEEE International Conference on Consumer Electronics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">September 2017</date>
			<biblScope unit="page" from="3" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,84.01,91.75,434.99,8.63;22,98.05,104.66,420.69,8.63;22,98.05,117.57,345.93,8.63" xml:id="b72">
	<analytic>
		<title level="a" type="main" xml:id="_XY3HJkr">Haptic-Auditory Perceptual Feedback Based Training for Improving the Spatial Acoustic Resolution of the Visually Impaired People</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bălan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moldoveanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wersenyi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lupu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_pKWZZNJ">Proceedings of the 21st International Conference on Auditory Display</title>
				<meeting>the 21st International Conference on Auditory Display<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,84.01,130.49,434.91,8.63;22,98.05,143.28,420.69,8.74;22,98.05,156.31,420.69,8.63;22,98.05,169.22,249.33,8.63" xml:id="b73">
	<analytic>
		<title level="a" type="main" xml:id="_yYUB86r">On interactive data visualization of physiological low-cost-sensor data with focus on mental stress</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bruschi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Eder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_NrBrMfe">Multidisciplinary Research and Practice for Information Systems; Alfredo Cuzzocrea</title>
		<title level="s" xml:id="_fuqVkGV">Springer Lecture Notes in Computer Science LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Dimitris</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Weippl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg/Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8127</biblScope>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,84.01,182.13,434.74,8.63;22,98.05,194.93,420.69,8.74;22,97.76,207.84,293.46,8.74" xml:id="b74">
	<analytic>
		<title level="a" type="main" xml:id="_4Qe23f2">Current advances, trends and challenges of machine learning and knowledge extraction: From machine learning to explainable AI</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kieseberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Weippl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Tjoa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_m8dXUdY">Springer Lecture Notes in Computer Science LNCS 11015</title>
				<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
