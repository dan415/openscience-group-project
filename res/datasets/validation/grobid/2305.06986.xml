<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_255BkGP">Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-11">11 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,165.87,180.29,87.90,17.29"><forename type="first">Eshaan</forename><surname>Nichani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,263.48,180.29,73.27,17.29"><forename type="first">Alex</forename><surname>Damian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,371.64,180.29,74.50,17.29"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_wzyDVZE">Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-11">11 May 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">E7DA7AEFAF6F86582F0060A86E2A4E71</idno>
					<idno type="arXiv">arXiv:2305.06986v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-05-12T20:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_ZBD6Naa"><p xml:id="_eNyp56K">One of the central questions in the theory of deep learning is to understand how neural networks learn hierarchical features. The ability of deep networks to extract salient features is crucial to both their outstanding generalization ability and the modern deep learning paradigm of pretraining and finetuneing. However, this feature learning process remains poorly understood from a theoretical perspective, with existing analyses largely restricted to two-layer networks. In this work we show that three-layer neural networks have provably richer feature learning capabilities than two-layer networks. We analyze the features learned by a three-layer network trained with layer-wise gradient descent, and present a general purpose theorem which upper bounds the sample complexity and width needed to achieve low test error when the target has specific hierarchical structure. We instantiate our framework in specific statistical learning settings -single-index models and functions of quadratic features -and show that in the latter setting three-layer networks obtain a sample complexity improvement over all existing guarantees for two-layer networks. Crucially, this sample complexity improvement relies on the ability of three-layer networks to efficiently learn nonlinear features. We then establish a concrete optimization-based depth separation by constructing a function which is efficiently learnable via gradient descent on a three-layer network, yet cannot be learned efficiently by a two-layer network. Our work makes progress towards understanding the provable benefit of three-layer neural networks over two-layer networks in the feature learning regime.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="37" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="38" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="39" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="40" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="41" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="42" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="43" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="44" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="45" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="46" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="47" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="48" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="49" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="50" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="51" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="52" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="53" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="54" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="55" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="56" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="57" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="58" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="59" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="60" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="61" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="62" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_SgpJVBz">Introduction</head><p xml:id="_qhyQ2Ap">The success of modern deep learning can largely be attributed to the ability of deep neural networks to decompose the target function into a hierarchy of learned features. This feature learning process enables both improved accuracy <ref type="bibr" coords="1,271.35,628.32,19.92,14.41" target="#b27">[28]</ref> and transfer learning <ref type="bibr" coords="1,399.13,628.32,18.32,14.41" target="#b19">[20]</ref>. Despite its importance, we still have a rudimentary theoretical understanding of the feature learning process. Fundamental questions include understanding what features are learned, how they are learned, and how they affect generalization.</p><p xml:id="_sgfNnS7">From a theoretical viewpoint, a fascinating question is to understand how depth can be leveraged to learn more salient features and as a consequence a richer class of hierarchical functions. The base case for this question is to understand which features (and function classes) can be learned efficiently by three-layer neural networks, but not two-layer networks. Recent work on feature learning has shown that two-layer neural networks learn features which are linear functions of the input (see Section 1.2 for further discussion). It is thus a natural question to understand if threelayer networks can learn nonlinear features, and how this can be leveraged to obtain a sample complexity improvement. Initial learning guarantees for three-layer networks <ref type="bibr" coords="2,446.71,145.14,18.93,14.41" target="#b14">[15,</ref><ref type="bibr" coords="2,468.81,145.14,8.97,14.41" target="#b4">5,</ref><ref type="bibr" coords="2,480.94,145.14,8.63,14.41" target="#b3">4]</ref>, however, consider simplified model and function classes and do not discern specifically what the learned features are or whether a sample complexity improvement can be obtained over shallower networks or kernel methods.</p><p xml:id="_3Uz6GC7">On the other hand, the standard approach in deep learning theory to understand the benefit of depth has been to establish "depth separations" <ref type="bibr" coords="2,276.85,224.59,18.32,14.41" target="#b49">[49]</ref>, i.e. functions that cannot be efficiently approximated by shallow networks, but can be via deeper networks. However, depth separations are solely concerned with the representational capability of neural networks, and ignore the optimization and generalization aspects. In fact, depth separation functions such as <ref type="bibr" coords="2,390.86,267.93,19.92,14.41" target="#b49">[49]</ref> are often not learnable via gradient descent <ref type="bibr" coords="2,152.09,282.37,18.32,14.41" target="#b34">[35]</ref>. To reconcile this, recent papers <ref type="bibr" coords="2,328.09,282.37,18.93,14.41" target="#b43">[43,</ref><ref type="bibr" coords="2,349.54,282.37,15.94,14.41" target="#b42">42]</ref> have established optimization-based depth separation results, which are functions which cannot be efficiently learned using gradient descent on a shallow network but can be learned with a deeper network. We thus aim to answer the following question:</p><p xml:id="_PPxy2Fb">What features are learned by gradient descent on a three-layer neural network, and can these features be leveraged to obtain a provable sample complexity guarantee?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1" xml:id="_px5HX6W">Our contributions</head><p xml:id="_Svz5AFA">We provide theoretical evidence that three-layer neural networks have provably richer feature learning capabilities than their two-layer counterparts. We specifically study the features learned by a three-layer network trained with a layer-wise variant of gradient descent (Algorithm 1). Our main contributions are as follows.</p><p xml:id="_NFjHmNP">• Theorem 1 is a general purpose sample complexity guarantee for Algorithm 1 to learn an arbitrary target function f * . We first show that Algorithm 1 learns a feature roughly corresponding to a low-frequency component of the target function f * with respect to the random feature kernel K induced by the first layer. We then derive an upper bound on the population loss in terms of the learned feature. As a consequence, we show that if f * can be written as a 1D function of the learned feature, then the sample complexity for learning f * is equal to the sample complexity of learning the feature. This demonstrates that three-layer networks can perform hierarchical learning.</p><p xml:id="_k7Rgha6">• We next instantiate Theorem 1 in two statistical learning settings which satisfy such hierarchical structure. As a warmup, we show that Algorithm 1 learns single-index models (i.e f * (x) = g * (w • x)) in d 2 samples, which is comparable to existing guarantees for two-layer networks and crucially has d-dependence not scaling with the degree of the link function g * . We next show that Algorithm 1 learns the target f * (x) = g * (x T Ax), where g * is either Lipschitz or a degree p = O(1) polynomial, up to o d (1) error with d 4 samples. This improves on all existing guarantees for learning with two-layer networks, which all require sample complexity d Ω(p) . A key technical step is to show that for the target f * (x) = g * (x T Ax), the learned feature is approximately x T Ax. This argument relies on the universality principle in high-dimensional probability, and may be of independent interest.</p><p xml:id="_JjQvXFT">• We conclude by establishing an explicit optimization-based depth separation: We show that the target function f * (x) = ReLU(x T Ax) for appropriately chosen A can be learned by Algorithm 1 up to o d <ref type="bibr" coords="3,200.12,140.80,14.96,10.48" target="#b0">(1)</ref> error in d 4 samples, whereas any two layer network needs either superpolynomial width or weight norm in order to approximate f * up to comparable accuracy. This implies that such an f * is not efficiently learnable via two-layer networks.</p><p xml:id="_KXWgXKD">The above separation hinges on the ability of three-layer networks to learn the nonlinear feature x T Ax and leverage this feature learning to obtain an improved sample complexity. Altogether, our work presents a general framework demonstrating the capability of three-layer networks to learn nonlinear features, and makes progress towards a rigorous understanding of feature learning, optimization-based depth separations, and the role of depth in deep learning more generally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2" xml:id="_SCFpjZC">Related Work</head><p xml:id="_VTZgB2z">Neural Networks and Kernel Methods. Early guarantees for neural networks relied on the Neural Tangent Kernel (NTK) theory <ref type="bibr" coords="3,234.23,318.64,18.93,14.41" target="#b29">[30,</ref><ref type="bibr" coords="3,256.37,318.64,14.94,14.41" target="#b48">48,</ref><ref type="bibr" coords="3,274.53,318.64,14.94,14.41" target="#b21">22,</ref><ref type="bibr" coords="3,292.69,318.64,14.19,14.41" target="#b15">16]</ref>. The NTK theory shows global convergence by coupling to a kernel regression problem and generalization via the application of kernel generalization bounds <ref type="bibr" coords="3,146.54,347.53,12.95,14.41" target="#b6">[7,</ref><ref type="bibr" coords="3,163.22,347.53,14.94,14.41" target="#b13">14,</ref><ref type="bibr" coords="3,181.89,347.53,8.63,14.41" target="#b4">5]</ref>. The NTK can be characterized explicitly for certain data distributions <ref type="bibr" coords="3,72.00,361.97,18.93,14.41" target="#b25">[26,</ref><ref type="bibr" coords="3,95.03,361.97,14.94,14.41" target="#b37">38,</ref><ref type="bibr" coords="3,114.08,361.97,14.19,14.41" target="#b36">37]</ref>, which allows for tight sample complexity and width analyses. This connection to kernel methods has also been used to study the role of depth, by analyzing the signal propagation and evolution of the NTK in MLPs <ref type="bibr" coords="3,242.42,390.87,18.93,14.41" target="#b40">[41,</ref><ref type="bibr" coords="3,264.32,390.87,14.94,14.41" target="#b46">46,</ref><ref type="bibr" coords="3,282.24,390.87,14.19,14.41" target="#b26">27]</ref>, convolutional networks <ref type="bibr" coords="3,418.86,390.87,12.95,14.41" target="#b5">[6,</ref><ref type="bibr" coords="3,434.78,390.87,14.94,14.41" target="#b53">53,</ref><ref type="bibr" coords="3,452.70,390.87,14.94,14.41" target="#b54">54,</ref><ref type="bibr" coords="3,470.63,390.87,14.19,14.41" target="#b38">39]</ref>, and residual networks <ref type="bibr" coords="3,137.77,405.31,18.32,14.41" target="#b28">[29]</ref>. However, the NTK theory is insufficient as neural networks outperform their NTK in practice <ref type="bibr" coords="3,152.10,419.76,12.95,14.41" target="#b5">[6,</ref><ref type="bibr" coords="3,167.84,419.76,14.19,14.41" target="#b30">31]</ref>. In fact, <ref type="bibr" coords="3,226.80,419.76,19.92,14.41" target="#b25">[26]</ref> shows that kernels cannot adapt to low-dimensional structure and require d k samples to learn any degree k polynomials in d dimensions. Ultimately, the NTK theory fails to explain generalization or the role of depth in practical networks not in the kernel regime. A key distinction is that networks in the kernel regime cannot learn features <ref type="bibr" coords="3,472.93,463.10,18.32,14.41">[55]</ref>. A recent goal has thus been to understand the feature learning mechanism and how this leads to sample complexity improvements <ref type="bibr" coords="3,200.63,491.99,18.93,14.41" target="#b51">[51,</ref><ref type="bibr" coords="3,223.03,491.99,14.94,14.41" target="#b20">21,</ref><ref type="bibr" coords="3,241.45,491.99,14.94,14.41">56,</ref><ref type="bibr" coords="3,259.88,491.99,8.97,14.41" target="#b2">3,</ref><ref type="bibr" coords="3,272.32,491.99,14.94,14.41" target="#b23">24,</ref><ref type="bibr" coords="3,290.74,491.99,14.94,14.41" target="#b24">25,</ref><ref type="bibr" coords="3,309.17,491.99,14.94,14.41" target="#b18">19,</ref><ref type="bibr" coords="3,327.59,491.99,14.94,14.41" target="#b52">52,</ref><ref type="bibr" coords="3,346.03,491.99,14.94,14.41" target="#b31">32,</ref><ref type="bibr" coords="3,364.45,491.99,14.19,14.41" target="#b33">34]</ref>. Crucially, our analysis is not in the kernel regime, and shows an improvement of three-layer networks over two-layer networks in the feature-learning regime.</p><p xml:id="_h9hqgAd">Feature Learning. Recent work has studied the provable feature learning capabilities of twolayer neural networks. <ref type="bibr" coords="3,189.24,567.25,12.95,14.41" target="#b8">[9,</ref><ref type="bibr" coords="3,206.59,567.25,8.97,14.41" target="#b0">1,</ref><ref type="bibr" coords="3,219.97,567.25,8.97,14.41" target="#b1">2,</ref><ref type="bibr" coords="3,233.35,567.25,8.97,14.41" target="#b7">8,</ref><ref type="bibr" coords="3,246.71,567.25,14.94,14.41" target="#b12">13,</ref><ref type="bibr" coords="3,266.07,567.25,15.94,14.41" target="#b9">10]</ref> show that for isotropic data distributions, two-layer networks learn linear features of the data, and thus efficiently learn functions of low-dimensional projections of the input (i.e of the form f * (x) = g(U x) for U ∈ R r×d ). These include low-rank polynomials <ref type="bibr" coords="3,135.47,610.59,18.93,14.41" target="#b16">[17,</ref><ref type="bibr" coords="3,158.75,610.59,9.96,14.41" target="#b1">2]</ref> and single-index models <ref type="bibr" coords="3,296.84,610.59,12.95,14.41" target="#b7">[8,</ref><ref type="bibr" coords="3,314.14,610.59,15.94,14.41" target="#b12">13]</ref> for Gaussian covariates, as well as sparse boolean functions <ref type="bibr" coords="3,161.59,625.04,13.94,14.41" target="#b0">[1]</ref> such as the k-sparse parity problem <ref type="bibr" coords="3,355.46,625.04,19.92,14.41" target="#b9">[10]</ref> for covariates uniform on the hypercube. The above approaches rely on layerwise training procedures, and our Algorithm 1 is an adaptation of the algorithm in <ref type="bibr" coords="3,217.11,653.93,18.32,14.41" target="#b16">[17]</ref>.</p><p xml:id="_P5MwAt2">Another approach uses the quadratic Taylor expansion of the network to learn classes of polynomials <ref type="bibr" coords="3,101.53,690.04,12.95,14.41" target="#b8">[9,</ref><ref type="bibr" coords="3,118.09,690.04,15.94,14.41" target="#b39">40]</ref> This approach can be extended to three-layer networks. <ref type="bibr" coords="3,412.28,690.04,19.92,14.41" target="#b14">[15]</ref> replace the outermost layer with its quadratic approximation, and by viewing z p as the hierarchical function (z p/2 ) 2</p><p xml:id="_DTtHxN3">show that their three-layer network can learn low rank, degree p polynomials in d p/2 samples. <ref type="bibr" coords="4,72.00,87.35,13.94,14.41" target="#b4">[5]</ref> similarly uses a quadratic approximation to improperly learn a class of three-layer networks via sign-randomized GD. An instantiation of their upper bound to the target g * (x T Ax) for degree p polynomial g * yields a sample complexity of d p+1 . However, <ref type="bibr" coords="4,392.53,116.25,18.93,14.41" target="#b14">[15,</ref><ref type="bibr" coords="4,415.86,116.25,9.96,14.41" target="#b4">5]</ref> are proved via opaque landscape analyses, do not concretely identify the learned features, and rely on nonstandard algorithmic modifications. Our Theorem 1 directly identifies the learned features, and when applied to the quadratic feature setting in Section 4.2 obtains an improved sample complexity guarantee independent of the degree of g * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_jtEEcYE">Depth</head><p xml:id="_PVTjD7n">Separations. <ref type="bibr" coords="4,181.66,205.96,19.92,14.41" target="#b49">[49]</ref> constructs a function which can be approximated by a poly-width network with large depth, but not with smaller depth. <ref type="bibr" coords="4,315.49,220.41,19.92,14.41" target="#b22">[23]</ref> is the first depth separation between depth 2 and 3 networks, with later works <ref type="bibr" coords="4,243.01,234.85,18.93,14.41" target="#b44">[44,</ref><ref type="bibr" coords="4,265.28,234.85,14.94,14.41" target="#b17">18,</ref><ref type="bibr" coords="4,283.57,234.85,15.94,14.41" target="#b45">45]</ref> constructing additional such examples. However, such functions are often not learnable via three-layer networks <ref type="bibr" coords="4,387.35,249.30,18.32,14.41" target="#b32">[33]</ref>. <ref type="bibr" coords="4,418.53,249.30,19.92,14.41" target="#b34">[35]</ref> shows that approximatability by a shallow (depth 3 network) is a necessary condition for learnability via a deeper network.</p><p xml:id="_4uQpsyb">These issues have motivated the development of optimization-based, or algorithmic, depth separations, which construct functions which are learnable by a three-layer network but not by two-layer networks. <ref type="bibr" coords="4,123.49,328.75,19.92,14.41" target="#b43">[43]</ref> shows that certain ball indicator functions 1( x ≥ λ) are not approximatable by two-layer networks, yet are learnable via GD on a special variant of a three-layer network with second layer width equal to 1. However, their choice of the second layer activation is tailored for learning the ball indicator, and the explicit polynomial sample complexity is weak (n d 36 samples are needed). <ref type="bibr" coords="4,182.42,386.53,19.92,14.41" target="#b42">[42]</ref> shows that a multi-layer variant of a mean-field network with a 1D bottleneck layer can learn the target ReLU(1 − x ), which <ref type="bibr" coords="4,362.54,400.98,19.92,14.41" target="#b45">[45]</ref> previously showed was inaproximatable via two-layer networks. However, their analysis relies on the rotational invariance of the target function, and it is difficult to read off explicit sample complexity and width guarantees beyond just being poly(d). Our Section 4.2 shows that three-layer networks can learn a larger class of features (x T Ax versus x ) and functions on top of these features (any Lipschitz q versus ReLU), with explicit dependence on the width and sample complexity needed (n, m 1 , m 2 = Õ(d 4 )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" xml:id="_evJMn23">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1" xml:id="_WvQGSUH">Problem Setup</head><p xml:id="_ApBRqzb">Data distribution. Our aim is to learn the target function f * : X d → R,with X d ⊂ R d the space of covariates. We let ν be some distribution on X d , and draw two independent datasets D 1 , D 2 , each with n samples, so that each x ∈ D 1 or x ∈ D 2 is sampled i.i.d as x ∼ ν. Without loss of generality, we normalize so E x∼ν [f * (x) 2 ] ≤ 1. We make the following assumptions on ν:</p><formula xml:id="formula_0" coords="4,72.00,631.78,468.00,50.50">Definition 1 (Sub-Gaussian Vector). A mean-zero random vector X ∈ R d is γ-subGaussian if, for all unit vectors v ∈ R d , E[exp(λX • v)] ≤ exp(γ 2 λ 2 ) for all λ ∈ R. Assumption 1. E x∼ν [x] = 0 and ν is C γ -subGaussian for some constant C γ .</formula><p xml:id="_BkCYEwf">Assumption 2. f * has polynomially growing moments, i.e there exist constants (C f , ) such that</p><formula xml:id="formula_1" coords="4,72.00,705.49,183.41,22.73">E x∼ν [f * (x) q ] 1/q ≤ C f q for all q ≥ 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_gEB7h5b">Algorithm 1 Layer-wise training algorithm</head><p xml:id="_bRn8P8t">Input: Initialization θ (0) ; learning rates η 1 , η 2 ; weight decay λ; time T {Stage 1:</p><formula xml:id="formula_2" coords="5,83.96,106.10,150.24,51.42">Train W } W (1) ← W (0) − η 1 ∇ W L 1 (θ (0) ) a (1) ← a (0)</formula><p xml:id="_9KnC73A">θ (1) ← (a (1) , W (1) , b (0) , V (0) ) {Stage 2: Train a}</p><formula xml:id="formula_3" coords="5,72.00,180.11,227.12,85.04">for t = 2, • • • , T do a (t) ← a (t−1) − η 2 ∇ a L 2 (θ (t−1) ) + λa (t−1) θ (t) ← (a (t) , W (1) , b (0) , V (0) ) end for θ ← θ (T ) Output: θ</formula><p xml:id="_Mj5DPMc">We note that Assumption 2 is satisfied by a number of common distributions and functions, and we will verify that Assumption 2 holds for each example in Section 4.</p><p xml:id="_sttf2Xt">Three-layer neural network. Let m 1 , m 2 be the two hidden layer widths, and σ 1 , σ 2 be two activation functions. Our learner is a three-layer neural network parameterized by θ = (a, W, b, V ),</p><formula xml:id="formula_4" coords="5,72.00,365.98,468.00,61.99">where a ∈ R m 1 , W ∈ R m 1 ×m 2 , b ∈ R m 1 , and V ∈ R m 2 ×d . The network f (x; θ) is defined as: f (x; θ) := 1 m 1 a T σ 1 (W σ 2 (V x) + b) = 1 m 1 m 1 i=1 a i σ 1 w i , h (0) (x) + b i .<label>(1)</label></formula><p xml:id="_32hT6A6">Here, w i ∈ R m 2 is the ith row of W , and h (0) (x) := σ 2 (V x) ∈ R m 2 is the random feature embedding arising from the innermost feature layer. The parameter vector θ (0) := (a (0) , W</p><formula xml:id="formula_5" coords="5,72.00,455.54,468.00,37.33">(0) , b (0) , V (0) ) is initialized with a (0) i ∼ iid Unif({±1}), W (0) = 0, the biases b (0) i ∼ iid N (0, 1)</formula><p xml:id="_twbXtnB">, and the rows v (0) i of V (0) drawn v i ∼ iid τ , where τ is the uniform measure on S d−1 (1), the d-dimensional unit sphere. We make the following assumption on the activations, and note that the polynomial growth assumption on σ 2 is sastisfied by all activations used in practice. Assumption 3. σ 1 is the ReLU activation, i.e σ 1 (z) = max(z, 0), and σ 2 has polynomial growth, i.e</p><formula xml:id="formula_6" coords="5,72.00,551.51,468.00,68.90">|σ 2 (x)| ≤ C σ (1 + |x|) ασ for some constants C σ , α σ &gt; 0. Training Algorithm. Let L i (θ) denote the empirical loss on dataset D i ; that is for i = 1, 2: L i (θ) := 1 n x∈D i (f (x; θ) − f * (x)) 2 .</formula><p xml:id="_MZ8zXyF">Our network is trained via layer-wise gradient descent with sample splitting. Throughout training, the first layer weights V and second layer bias b are held constant. First, the second layer weights W are trained for t = 1 timesteps. Next, the outer layer weights a are trained for t = T timesteps. This two stage training process is common in prior works analyzing gradient descent on two-layer networks <ref type="bibr" coords="5,344.42,655.66,18.93,14.41" target="#b16">[17,</ref><ref type="bibr" coords="5,366.27,655.66,8.97,14.41" target="#b7">8,</ref><ref type="bibr" coords="5,378.16,655.66,8.97,14.41" target="#b0">1,</ref><ref type="bibr" coords="5,390.04,655.66,14.19,14.41" target="#b9">10]</ref>, and as we see in Section 5, is already sufficient to establish a separation between two and three-layer networks. Pseudocode for the training procedure is presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2" xml:id="_NPwsA99">Technical definitions</head><p xml:id="_UZWGznj">The activation σ 2 admits a random feature kernel K :</p><formula xml:id="formula_7" coords="6,72.00,95.42,468.00,36.97">X d × X d → R and corresponding integral operator K : L 2 (X d , ν) → L 2 (X d , ν):</formula><p xml:id="_sBMwGXr">Definition 2 (Kernel objects). σ 2 admits the random feature kernel K(x, x</p><formula xml:id="formula_8" coords="6,72.00,133.32,493.45,27.05">) := E v∼τ [σ 2 (x • v)σ 2 (x • v)] and corresponding integral operator (Kf )(x) := E x ∼ν [K(x, x )f (x )].</formula><p xml:id="_BdcWU5s">We make the following assumption on K, which we verify for the examples in Section 4: Assumption 4. Kf * has polynomially bounded moments, i.e there exist constants C K , χ such that, for all</p><formula xml:id="formula_9" coords="6,104.55,205.03,208.20,21.26">1 ≤ q ≤ d, Kf * L q (ν) ≤ C K q χ Kf * L 2 (ν)</formula><p xml:id="_ndjy5Ba">. We also require the definition of the Sobolev space:</p><formula xml:id="formula_10" coords="6,72.00,247.11,468.00,36.97">Definition 3. Let W 2,∞ ([−1, 1]) be the Sobolev space of twice continuously differentiable functions q : [−1, 1] → R equipped with the norm q k,∞ := max s≤k max x∈[−1,1] q (s) (x) for k = 1, 2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3" xml:id="_vpw2cf5">Notation</head><p xml:id="_DWrgnXE">We use big O notation (i.e O, Θ, Ω) to ignore absolute constants (C σ , C f , etc.) that do not depend on d, n, m 1 , m 2 . We further write</p><formula xml:id="formula_11" coords="6,238.77,333.93,301.23,14.41">a d b d if a d = O(b d ), and a d = o(b d ) if lim d→∞ a d /b d = 0.</formula><p xml:id="_tpWuUMd">Additionally, we use Õ notation to ignore terms that depend logarithmically on dnm 1 m 2 . For f :</p><formula xml:id="formula_12" coords="6,88.94,362.61,231.26,22.73">X d → R, define f L p (X d ,ν) = (E x∼ν [f (x) p ]) 1/p .</formula><p xml:id="_GanNtJY">To simplify notation we also call this quantity f L p (ν) , and g L p (X d ,ν) , g L p (τ ) are defined analogously for functions g : S d−1 (1) → R. When the domain is clear from context, we write f L p , g L p . We let L p (X d , ν) be the space of f with finite f L p (X d ,ν) . Finally, we write E x and E v as shorthand for E x∼ν and E v∼τ respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" xml:id="_86b6nQH">Main Result</head><p xml:id="_B6eXGCf">The following is our main theorem which upper bounds the population loss of Algorithm 1:</p><formula xml:id="formula_13" coords="6,72.00,497.09,468.00,22.73">Theorem 1. Select q ∈ W 2,∞ ([−1, 1]). Let η 1 = m 1 m 2 η, and assume n, m 1 , m 2 = Ω( Kf * −2 L 2 )</formula><p xml:id="_NZXQXaK">There exist η, λ, η 2 such that after T = poly(n, m 1 , m 2 , d, q 2,∞ ) timesteps, with high probability over the initialization and datasets the output θ of Algorithm 1 satisfies the population L 2 loss bound</p><formula xml:id="formula_14" coords="6,114.73,569.15,250.57,80.98">E x f (x; θ) − f * (x) 2 ≤ Õ q • (η • Kf * ) − f * 2 L 2 accuracy of feature learning + q 2 1,∞ Kf * −2 L 2 min(n, m 1 , m 2 )</formula><p xml:id="_tzxksZT">sample complexity of feature learning</p><formula xml:id="formula_15" coords="6,368.99,600.50,171.01,49.08">+ q 2 2,∞ m 1 + q 2 2,∞ + 1 √ n complexity of q (2)</formula><p xml:id="_xQK4hJy">The full proof of this theorem is in Appendix D. The population risk upper bound (2) has three terms:</p><p xml:id="_TR4GVFq">1. The first term quantifies the extent to which feature learning is useful for learning the target f * , and depends on how close f * is to having hierarchical structure. Concretely, if there exists q : R → R such that the compositional function q • η • Kf * is close to the target f * , then this first term is small. In Section 4, we show that this is true for certain hierarchical functions. In particular, say that f * satisfies the hierarchical structure f * = g * • h * . If the quantity Kf * is nearly proportional to the true feature h * , then this first term is negligible.</p><p xml:id="_fSwjGth">As such, we refer to the quantity Kf * as the learned feature.</p><p xml:id="_rNXGj2t">2. The second term is the sample (and width) complexity of learning the feature Kf * . It is useful to compare the Kf * −2 L 2 term to the standard kernel generalization bound, which requires n f , K −1 f L 2 . Unlike in the kernel bound, the feature learning term in (2) does not require inverting the kernel K as it only requires a lower bound on Kf L 2 . This difference can be best understood by considering the alignment of f with the eigenfunctions of K. Say that f has nontrivial alignment with eigenfunctions of K with both small (λ min ) and large (λ max ) eigenvalues. Kernel methods require Ω(λ −1 min ) samples, which blows up when λ min is small; the sample complexity of kernel methods depends on the high frequency components of f * . On the other hand, the guarantee in Theorem 1 scales with Kf −2 L 2 = O(λ −2 max ), which can be much smaller. In other words, the sample complexity of feature learning scales with the low-frequency components of f * . The feature learning process can thus be viewed as extracting the low-frequency components of the target.</p><p xml:id="_UK2H5KW">3. The last two terms measure the complexity of learning the univariate function q. In the examples in Section 4, the effect of these terms is benign.</p><p xml:id="_xnRjgde">Altogether, if f * satisfies the hierarchical structure that its high-frequency components can be inferred from the low-frequency ones, then a good q for Theorem 1 exists and the dominant term in (2) is the sample complexity of feature learning term, which only depends on the low-frequency components. This is not the case for kernel methods, as small eigenvalues blow up the sample complexity. As we show in Section 4, this ability to ignore the small eigenvalue components of f during the feature learning process is critical for achieving good sample complexity in many problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" xml:id="_nDtFErM">Proof Sketch</head><p xml:id="_UuGpRjw">At initialization, f (x; θ (0) ) ≈ 0. The first step of GD on the population loss for a neuron w j is thus</p><formula xml:id="formula_16" coords="7,204.24,581.24,335.76,71.52">w (1) = −η 1 ∇ w j E x f (x; θ (0) ) − f * (x) 2 (3) = η 1 E x f * (x)∇ w j f (x; θ (0) ) (4) = 1 m 2 η1 b (0) j ≥0 a (0) j E x f * (x)h (0) (x) .<label>(5)</label></formula><p xml:id="_FWuhggg">Therefore the network f (x ; θ (1) ) after the first step of GD is given by</p><formula xml:id="formula_17" coords="8,117.08,99.08,422.92,76.00">f (x ; θ (1) ) = 1 m 1 m 1 j=1 a j σ 1 w (1) j , h (0) (x) + b j (6) = 1 m 1 m 1 j=1 a j σ 1 a (0) j • η 1 m 2 E x f * (x)h (0) (x) T h (0) (x ) + b j 1 b (0) j ≥0 .<label>(7)</label></formula><p xml:id="_eVwHy3V">We first notice that this network now implements a 1D function of the quantity</p><formula xml:id="formula_18" coords="8,206.47,212.46,333.53,27.79">φ(x ) := η 1 m 2 E x f * (x)h (0) (x) T h (0) (x ) .<label>(8)</label></formula><p xml:id="_Qcsm9R2">Specifically, the network can be rewritten as</p><formula xml:id="formula_19" coords="8,177.34,274.53,362.66,35.82">f (x ; θ (1) ) = 1 m 1 m 1 j=1 a j σ 1 a (0) j • φ(x ) + b j • 1 b (0) j ≥0 .<label>(9)</label></formula><p xml:id="_vXsvnAc">Since f implements a hierarchical function of the quantity φ(x), we term φ the learned feature.</p><p xml:id="_KbqYRfD">The second stage of Algorithm 1 is equivalent to random feature regression. We first use results on ReLU random features to show that any q ∈ W 2,∞ ([−1, 1]) can be approximated on [−1, 1] as</p><formula xml:id="formula_20" coords="8,72.00,388.55,126.39,23.03">q(z) ≈ 1 m 1 m 1 j=1 a * j σ 1 (a<label>(0)</label></formula><p xml:id="_7ugJhBK">j z + b j ) for some a * q 2,∞ (Lemma 3). Next, we use the standard kernel Rademacher bound to show that the excess risk scales with the smoothness of q (Lemma 5). Hence we can efficiently learn functions of the form q • φ.</p><p xml:id="_bFrH2kX">It suffices to compute this learned feature φ. For m 2 large, we observe that</p><formula xml:id="formula_21" coords="8,98.94,465.38,441.06,35.81">φ(x ) = η m 2 m 2 j=1 E x [f * (x)σ(x • v)σ(x • v)]) ≈ ηE x [f * (x)K(x, x )] = η(Kf * )(x ).<label>(10)</label></formula><p xml:id="_hEFSDQD">The learned feature is thus approximately η </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" xml:id="_NMxp9ha">Examples</head><p xml:id="_tWY3PUd">We next instantiate Theorem 1 in two specific statistical learning settings which satisfy the hierarchical prescription detailed in Section 3. As a warmup, we show that three-layer neural networks can efficiently learn single index models. Our second example shows how three-layer networks can obtain sample complexity improving on existing guarantees for two-layer networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" xml:id="_d5YEzpy">Warmup: single index models</head><p xml:id="_W2GgNzQ">Let f * = g * (w * • x), for unknown direction w * ∈ R d and unknown link function g * : R → R, and take X d = R d with ν = N (0, I). Prior work <ref type="bibr" coords="9,291.12,109.87,18.93,14.41" target="#b16">[17,</ref><ref type="bibr" coords="9,313.37,109.87,15.94,14.41" target="#b12">13]</ref> shows that two-layer neural networks learn such functions with an improved sample complexity over kernel methods. Let σ 2 (z) = z, so that the network is of the form f (x; θ) = 1 m 1 a T σ 1 (W V x). We can verify that Assumptions 1 to 4 are satisfied, and thus applying Theorem 1 in this setting yields the following:</p><formula xml:id="formula_22" coords="9,72.00,174.88,182.79,22.52">Theorem 2. Let f * (x) = g * (w * • x)</formula><p xml:id="_zKYedbq">, where w * 2 = 1. Assume that g * , (g * ) and (g * ) are polynomially bounded and that E z∼N (0,1) [g (z)] = 0. Then with high probability Algorithm 1 satisfies the population loss bound</p><formula xml:id="formula_23" coords="9,166.55,229.76,373.45,29.41">E x f (x; θ) − f * (x) 2 = Õ d 2 min(n, m 1 , m 2 ) + 1 √ n .<label>(11)</label></formula><p xml:id="_mWvGWsA">Given widths m 1 , m 2 = Θ(d 2 ), n = Θ(d 2 ) samples suffice to learn f * , which matches existing guarantees for two-layer neural networks <ref type="bibr" coords="9,297.69,293.34,18.93,14.41" target="#b12">[13,</ref><ref type="bibr" coords="9,320.97,293.34,14.19,14.41" target="#b16">17]</ref>. We remark that prior work on learning single-index models with specific link functions (such as ReLU) require d samples <ref type="bibr" coords="9,479.59,307.79,18.93,14.41" target="#b47">[47,</ref><ref type="bibr" coords="9,502.31,307.79,14.94,14.41" target="#b35">36,</ref><ref type="bibr" coords="9,521.07,307.79,14.19,14.41" target="#b11">12]</ref>. However, our sample complexity improves on that of kernel methods, which require d p samples when g * is a degree p polynomial, and applies more generally to any link function with nonzero first derivative.</p><p xml:id="_hMcKBp7">Theorem 2 is proved in Appendix E.1; a brief sketch is as follows. Since</p><formula xml:id="formula_24" coords="9,72.00,372.79,468.00,36.97">σ 2 (z) = z, the kernel is K(x, x ) = E v [(x • v)(x • v)] = x•x d</formula><p xml:id="_JxTb92g">. By an application of Stein's Lemma, the learned feature is</p><formula xml:id="formula_25" coords="9,183.48,425.19,356.52,27.73">(Kf * )(x) = 1 d E x [x • x f * (x)] = 1 d x T E x [∇f * (x )]<label>(12)</label></formula><p xml:id="_srnb9E5">Since f * (x) = g * (w * • x), ∇f * (x) = w * (g * ) (w * • x), and thus</p><formula xml:id="formula_26" coords="9,183.83,486.69,356.17,27.73">(Kf * )(x) = 1 d E z∼N (0,1) [(g * ) (z)]w * • x ∝ 1 d w * • x.<label>(13)</label></formula><p xml:id="_kJD4dz8">The learned feature is proportional to the learned feature, so an appropriate choice of η and choosing q = g * in Theorem 1 implies that Kf * −2 L 2 = d 2 samples are needed to learn f * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" xml:id="_UAT5sRX">Functions of quadratic features</head><p xml:id="_8nNSkK4">The next example shows how three-layer networks can learn nonlinear features, and thus obtain a sample complexity improvement over two-layer networks.</p><p xml:id="_rhvBJ8z">Let X d = S d−1 ( √ d), the sphere of radius √ d, and ν the uniform measure on X d . The integral operator K has been well studied <ref type="bibr" coords="9,239.95,643.95,18.93,14.41" target="#b37">[38,</ref><ref type="bibr" coords="9,263.07,643.95,14.94,14.41" target="#b25">26,</ref><ref type="bibr" coords="9,282.21,643.95,14.19,14.41" target="#b36">37]</ref>, and its eigenfunctions correspond to the spherical harmonics. Preliminaries on spherical harmonics and this eigendecomposition are given in Appendix F.</p><p xml:id="_mqyrSKF">Consider the target f * (x) = g * (x T Ax), where A ∈ R d×d is a symmetric matrix and g * : R → R is an unknown link function. In contrast to a single-index model, the feature x T Ax we aim to learn is a quadratic function of x. Since one can write x T Ax = x T A − Tr(A) • I d x + Tr(A), we without loss of generality assume Tr(A) = 0. We also select the normalization</p><formula xml:id="formula_27" coords="10,72.00,88.83,468.00,29.06">A 2 F = d+2 2d = Θ(1); this ensures that E x∼ν [(x T Ax) 2 ] = 1.</formula><p xml:id="_XW6NCA2">We first make the following assumptions on the target function.: <ref type="formula" coords="10,342.67,142.48,5.41,10.48" target="#formula_4">1</ref>), g * is 1-Lipschitz, and (g * ) has polynomial growth.</p><formula xml:id="formula_28" coords="10,72.00,140.86,270.67,13.19">Assumption 5. E x [f * (x)] = 0, E z∼N (0,1) [(g * ) (z)] = Θ(</formula><p xml:id="_2Bxdnsx">The first assumption can be achieved via a preprocessing step which subtracts the mean of f * , the second is a nondegeneracy condition, and the last two assume the target is sufficiently smooth.</p><p xml:id="_HavW2yS">We next require the eigenvalues of A to satisfy an incoherence condition:</p><formula xml:id="formula_29" coords="10,72.00,225.09,273.45,24.49">Assumption 6. Define κ := A op √ d. Then κ = o( √ d).</formula><p xml:id="_pTuBxPv">Note that κ ≤ √ d. If A has rank Θ(d) and condition number Θ(1), then κ = Θ(1). Furthermore, when the entries of A are sampled i.i.d, κ = Θ(1) with high probability by Wigner's semicircle law.</p><p xml:id="_CC4ABfm">Finally, we make the following nondegeneracy assumption on the Gegenbauer decomposition of σ 2 (defined in Appendix F). We show that λ 2 2 (σ 2 ) = O(d −2 ), and later argue that the following assumption is mild and indeed satisfied by standard activations such as σ 2 = ReLU.</p><formula xml:id="formula_30" coords="10,72.00,359.02,433.17,15.55">Assumption 7. Let λ 2 (σ 2 ) be the 2nd Gegenbauer coefficient of σ 2 . Then λ 2 2 (σ 2 ) = Θ(d −2</formula><p xml:id="_4vf6uDg">). We can verify Assumptions 1 to 4 hold for this setting, and thus applying Theorem 1 the following: Theorem 3. Under Assumptions 5 to 7, with high probability Algorithm 1 satisfies the population loss bound</p><formula xml:id="formula_31" coords="10,135.59,454.94,404.41,31.77">E x f (x; θ) − f * (x) 2 Õ d 4 min(n, m 1 , m 2 ) + 1 √ n + κ √ d 1/3<label>(14)</label></formula><p xml:id="_sGQHwUC">We thus require sample size n = Ω(d 4 ) and widths m 1 , m 2 = Ω(d 4 ) to obtain o d (1) test loss.</p><p xml:id="_wp4a3G5">Proof Sketch. The integral operator K has eigenspaces correspond to spherical harmonics of degree k. In particular, <ref type="bibr" coords="10,184.11,553.92,19.92,14.41" target="#b25">[26]</ref> shows that, in L 2 ,</p><formula xml:id="formula_32" coords="10,259.03,580.24,280.97,26.70">Kf * = k≥0 c k P k f * ,<label>(15)</label></formula><p xml:id="_hdCu4A9">where P k is the orthogonal projection onto the subspace of degree k spherical harmonics, and the c k are constants satisfying</p><formula xml:id="formula_33" coords="10,72.00,630.81,468.00,61.60">c k = O(d −k ). Since f * is an even function and E x [f * (x)] = 0, truncating this expansion at k = 2 yields Kf * = Θ(d −2 ) • P 2 f * + O(d −4 ),<label>(16)</label></formula><p xml:id="_5RpCSfT">It thus suffices to compute P 2 f * . To do so, we draw a connection to the universality phenomenon in high dimensions. Consider two features x T Ax and x T Bx with A, B = 0. We show that, when d is large, the distribution of x T Ax approaches that of the standard Gaussian; furthermore x T Bx approaches a mixture of χ 2 and Gaussian random variables independent of x T Ax. As such, we can show</p><formula xml:id="formula_34" coords="11,148.83,128.20,391.17,41.22">E x g * (x T Ax)x T Bx ≈ E x g * (x T Ax) • E x x T Bx = 0 (17) E x g * (x T Ax)x T Ax ≈ E z∼N (0,1) [g * (z)z] = E z∼N (0,1) [(g * ) (z)].<label>(18)</label></formula><p xml:id="_BEVdCUb">The second equality can be viewed as an approximate version of Stein's lemma, which was applied in Section 4.1 to compute the learned feature. Altogether, our key technical result (stated formally in Lemma 20) is that for f * = g * (x T Ax), the projection P 2 f * satisfies</p><formula xml:id="formula_35" coords="11,190.53,228.60,349.47,22.52">(P 2 f * )(x) = E z∼N (0,1) [(g * ) (z)] • x T Ax + o d (1)<label>(19)</label></formula><p xml:id="_XuPpEFq">The learned feature is thus</p><formula xml:id="formula_36" coords="11,201.48,256.26,142.77,21.26">Kf * = Θ(d −2 ) • P 2 f * + o d (1)</formula><p xml:id="_67jmNZg">; plugging this into Theorem 1 yields the d 4 sample complexity.</p><p xml:id="_xMdrXcm">The full proof of Theorem 3 is deferred to Appendix E.2. In Appendix E.3 we show that when g * is a degree p = O(1) polynomial, Algorithm 1 learns f * in Õ(d 4 ) samples with an improved error floor.</p><p xml:id="_ExubdZV">Comparison to two-layer networks. Existing guarantees for two-layer networks cannot efficiently learn functions of the form f * (x) = g * (x T Ax) for arbitrary Lipschitz g * . In fact, in Section 5 we provide an explicit lower bound against two-layer networks efficiently learning a subclass of these functions. When g * is a degree p polynomial, networks in the kernel regime require d 2p d 4 samples to learn f * <ref type="bibr" coords="11,240.93,409.72,18.32,14.41" target="#b25">[26]</ref>. Improved guarantees for two-layer networks learn degree p polynomials in r p samples when the target only depends on a rank r d projection of the input <ref type="bibr" coords="11,99.88,438.61,18.32,14.41" target="#b16">[17]</ref>. However, g * (x T Ax) cannot be written in this form for some r d, and thus existing guarantees do not apply. We conjecture that two-layer networks require d Ω(p) samples to learn such an f * .</p><p xml:id="_b2fDgTS">Altogether, the ability of three-layer networks to efficiently learn the class of functions f * (x) = g * (x T Ax) hinges on their ability to extract the correct nonlinear feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" xml:id="_YrSdKwC">An Optimization-Based Depth Separation</head><p xml:id="_z9Q5GMq">We complement the learning guarantee in Section 4.2 with a lower bound showing that there exist functions in this class that cannot be approximated by a polynomial size two-layer network.</p><p xml:id="_JhnuQdd">The class of candidate two-layer networks is as follows. For a parameter vector θ</p><formula xml:id="formula_37" coords="11,72.00,610.18,468.00,73.10">= (a, W, b 1 , b 2 ), where a ∈ R m , W ∈ R m×d , b 1 ∈ R m , b 2 ∈ R, define the associated two-layer network as N θ (x) := a T σ(W x + b 1 ) + b 2 = m i=1 a i σ(w T i x + b 1,i ) + b 2 . (<label>20</label></formula><formula xml:id="formula_38" coords="11,535.02,656.85,4.98,14.41">)</formula><p xml:id="_PsdkeTu">Let</p><formula xml:id="formula_39" coords="11,96.93,696.68,188.39,13.20">θ ∞ := max( a ∞ , W ∞ , b 1 ∞ , b<label>2</label></formula><p xml:id="_pnArDmr">∞ ) denote the maximum parameter value. We make the following assumption on σ, which holds for all commonly-used activations.</p><formula xml:id="formula_40" coords="12,72.00,72.70,377.57,22.73">Assumption 8. There exist constants C σ , α σ such that |σ(z)| ≤ C σ (1 + |z|) ασ .</formula><p xml:id="_maHbWRa">Our main theorem establishing the separation is the following. </p><formula xml:id="formula_41" coords="12,138.66,187.77,344.35,22.73">N θ − f * 2 L 2 ≤ must satisfy max(m, θ ∞ ) ≥ C 1 exp C 2 −1/2 log(d )</formula><p xml:id="_VGdbkp9">. However, Algorithm 1 outputs a predictor satisfying the population L 2 loss bound</p><formula xml:id="formula_42" coords="12,176.26,229.78,358.76,29.35">E x f (x; θ) − f * (x) 2 O d 4 n + d n + d −1/6 . (<label>21</label></formula><formula xml:id="formula_43" coords="12,535.02,236.60,4.98,14.41">)</formula><formula xml:id="formula_44" coords="12,72.00,271.30,190.40,14.39">after T = poly(d, m 1 , m 2 , n) timesteps.</formula><p xml:id="_FRXchud">The lower bound follows from a modification of the argument in <ref type="bibr" coords="12,391.50,292.96,19.92,14.41" target="#b17">[18]</ref> along with an explicit decomposition of the ReLU function into spherical harmonics. We remark that the separation applies for any link function g * whose Gegenbauer coefficients decay sufficiently slow. The upper bound follows from an application of Theorem 1 to a smoothed version of ReLU, since ReLU isn't in</p><formula xml:id="formula_45" coords="12,72.00,352.01,64.87,21.26">W 2,∞ ([−1, 1]</formula><p xml:id="_7auqx3h">). The full proof of the theorem is given in Appendix G.</p><p xml:id="_HJr4zR4">Remarks. In order for a two-layer network to achieve test loss matching the d −1/6 error floor, either the width or maximum weight norm of the network must be exp Ω(d δ ) for some constant δ; which is superpolynomial in d. As a consequence, gradient descent on a poly-width two-layer neural network with stable step size must run for superpolynomially many iterations in order for some weight to grow this large and thus converge to a solution with low test error. Therefore f * is not learnable via gradient descent in polynomial time. This reduction from a weight norm lower bound to a runtime lower bound is made precise in <ref type="bibr" coords="12,317.42,469.02,18.32,14.41" target="#b43">[43]</ref>.</p><p xml:id="_3rmy3K8">We next describe a specific example of such an f * . Let S be a d/2-dimensional subspace of R d , and let</p><formula xml:id="formula_46" coords="12,110.19,507.71,344.09,23.23">A = d − 1 2 P S − d − 1 2 P ⊥ S . Then, f * (x) = ReLU 2d − 1 2 P S x 2 − d 1 2 .</formula><p xml:id="_mmYntbR">[42] established an optimization-based separation for the target ReLU(1 − x ), under a different distribution ν. However, their analysis relies heavily on the rotational symmetry of the target, and they posed the question of learning ReLU(1− P S x ), where P S is the projection onto a subspace. Our separation applies to a similar target function, and crucially does not rely on this rotational invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" xml:id="_7Wxk5Jv">Discussion</head><p xml:id="_cPuTnhd">In this work we showed that three-layer networks can both learn nonlinear features and leverage these features to obtain a provable sample complexity improvement over two-layer networks. There are a number of interesting directions for future work. First, can our framework be used to learn hierarchical functions of a larger class of features beyond quadratic functions? Next, since Theorem 1 is general purpose and makes minimal distributional assumptions, it would be interesting to understand if it can be applied to standard empirical datasets such as CIFAR-10, and what the learned feature Kf * and hierarchical learning correspond to in this setting. Finally, our analysis studies the nonlinear feature learning that arises from a neural representation σ 2 (V x) where V is fixed at initialization. This alone was enough to establish a separation between two and threelayer networks. A fascinating question is to understand the additional features that can be learned when both V and W are jointly trained. Such an analysis, however, is incredibly challenging in feature-learning regimes. ceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 10462-10472. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/xiao20b.html.</p><p xml:id="_cResjxR">[55] Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In Proceedings of the 38th International Conference on Machine Learning, 2021.</p><p xml:id="_Fs6s5nA">[56] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding neural networks. In Advances in Neural Information Processing Systems, 2019.</p><p xml:id="_XtFA6kq">A Empirical Validation Quadratic Feature Setting (Random Projection)</p><formula xml:id="formula_47" coords="19,310.21,237.37,222.22,99.02">−2 0 2 4 h (x) = x • β −3 −2 −1 0 1 2 3 4 φ (x) (Learned Feature) Single Index, d = 64 n = 2 4 n = 2 8 n = 2 16 −2 0 2 4 h (x) = x T Ax −3 −2 −1 0 1 2 3 φ (x) (Learned Feature)</formula><p xml:id="_nHPrRYf">Random Projection, d = 16</p><formula xml:id="formula_48" coords="19,516.81,298.23,5.95,16.01">n = n = n =</formula><p xml:id="_nr9Hkph">Correlation Between Learned and Predicted Features We empirically verify our conclusions in the single index setting of Section 4.1 and the quadratic feature setting of Section 4.2:</p><p xml:id="_sRptxJY">Single Index Setting We learn the target function g (w •x) using Algorithm 1 where w ∈ S d−1 is drawn randomly and g (x) = sigmoid(x) = 1 1+e −x , which satisfies the condition E z∼N (0,1) [g (z)] = 0. As in Theorem 2, we choose the initial activation σ 2 (z) = z. We optimize the hyperparameters η 1 , λ using grid search over a holdout validation set of size 2 15 and report the final error over a test set of size 2 15 .</p><p xml:id="_ecbtGSq">Quadratic Feature Setting We learn the target function g (x T Ax) using Algorithm 1 where g (x) = x 3 . We ran our experiments with two different choices of A:</p><p xml:id="_JJKxhqg">• A is symmetric with random entries, i.e. A ij ∼ N (0, 1) and A ji = A ij for i ≤ j.</p><p xml:id="_Xuem6Kn">• A is a random projection, i.e. A = Π S where S is a random d/2 dimensional subspace.</p><p xml:id="_2HfjD7b">Both choices of A were then normalized so that tr A = 0 and A F = 1 by subtracting the trace and dividing by the Frobenius norm. We chose initial activation σ 2 (z) = ReLU(z). We note that in both examples, κ = Θ(1). As above, we optimize the hyperparameters η 1 , λ using grid search over a holdout validation set of size 2 15 and report the final error over a test set of size 2 15 .</p><p xml:id="_q4KEGgw">To focus on the sample complexity and avoid width-related bottlenecks, we directly simulate the infinite width limit (m 2 → ∞) of Algorithm 1 by computing the kernel K in closed form. Finally, we run each trial with 5 random seeds and report the min, median, and max values in Figure <ref type="figure" coords="20,497.46,137.91,4.48,14.41" target="#fig_2">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_mzgtEMq">B Notation B.1 Asymptotic Notation</head><p xml:id="_k9s4P3B">Throughout the proof we will let C be a fixed but sufficiently large constant.</p><p xml:id="_a3p9hK8">Definition 4 (high probability events). Let ι = C log(dnm 1 m 2 ). We say that an event happens with high probability if it happens with probability at least</p><formula xml:id="formula_49" coords="20,72.00,267.60,412.18,44.19">1 − poly(d, n, m 1 , m 2 )e −ι . Example 5. If z ∼ N (0, 1) then |z| ≤ √ 2ι with high probability.</formula><p xml:id="_uMVkweD">Note that high probability events are closed under union bounds over sets of size poly(d, n, m 1 , m 2 ).</p><p xml:id="_tW6Gae8">We will also assume throughout that ι ≤ C −1 d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_UAYsHGQ">B.2 Tensor Notation</head><p xml:id="_nsBF5FD">For a k-tensor T ∈ (R d ) ⊗k , let Sym be the symmetrization of T across all k axes, i.e</p><formula xml:id="formula_50" coords="20,72.00,409.84,336.94,67.17">Sym(T ) i 1 ,••• ,i k = 1 k! σ∈S k T i σ(1) ,••• ,i σ(k) Next, given tensors A ∈ (R d ) ⊗a , B ∈ (R d ) ⊗b , we let A ⊗ B ∈ (R d</formula><p xml:id="_9RgGvdb">) ⊗a+b be their tensor product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_W5TCGAX">Definition 5 (Symmetric tensor product). Given tensors</head><formula xml:id="formula_51" coords="20,72.00,490.60,468.00,63.37">A ∈ (R d ) ⊗a , B ∈ (R d ) ⊗b , we define their symmetric tensor product A ⊗B ∈ (R d ) ⊗a+b as A ⊗B := Sym (A ⊗ B).</formula><p xml:id="_WjntDsG">We note that ⊗ satisfies associativity.</p><p xml:id="_HjT3nqM">Definition 6 (Tensor contraction). Given a symmetric k-tensor T ∈ (R d ) ⊗k and an a-tensor A ∈ (R d ) ⊗a , where k ≥ a, we define the tensor contraction T (A) to be the k − a tensor given by</p><formula xml:id="formula_52" coords="20,182.68,625.34,246.63,24.98">T (A) i 1 ,...,i k−a = (j 1 ,...,ja)∈[d] a T j 1 ,...,ja,i 1 ,...,i k−a A j 1 ,...,ja .</formula><p xml:id="_rXzcAbx">When A is also a k-tensor, then T, A := T (A) ∈ R denotes their Euclidean inner product. We further define T 2 F := T, T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_S43cweQ">C Univariate Approximation</head><p xml:id="_m4pgp8J">Throughout this section, let µ(b) :=</p><formula xml:id="formula_53" coords="21,247.81,101.90,44.87,21.18">exp(−b 2 /2) √ 2π</formula><p xml:id="_TRtpDuK">denote the PDF of a standard Gaussian.  Proof. Let v(a, b) = ca1 b∈ <ref type="bibr" coords="21,198.76,440.41,10.63,6.99" target="#b0">[1,</ref><ref type="bibr" coords="21,209.38,440.41,7.08,6.99" target="#b1">2]</ref> where c =</p><formula xml:id="formula_54" coords="21,72.00,206.17,384.98,108.61">Proof. Let v(a, b) = c1 b∈[1,2] where c = 1 µ(1)−µ(2) . Then for |x| ≤ 1, E a,b [v(a, b)σ(ax + b)] = c 2 1 1 2 [σ(x + b) + σ(−x + b)]µ(b)db = c 2 1 bµ(b) = 1.</formula><formula xml:id="formula_55" coords="21,72.00,432.94,468.00,224.43">1 1 0 µ(b)db . Then for |x| ≤ 1, E a,b [v(a, b)σ(ax + b)] = c 2 1 1 2 [σ(x + b) − σ(−x + b)]µ(b)db = cx 2 1 µ(b)db = x. Lemma 3. Let f : [−1, 1] → R be any twice differentiable function. Then there exists v(a, b) supported on {−1, 1} × [0, 2] such that for any |x| ≤ 1, E a,b [v(a, b)σ(ax + b)] = f (x) and sup a,b |v(a, b)| sup x∈[−1,1] k∈{0,1,2} |f (k) (x)|. Proof. First consider v(a, b) = 1 b∈[0,1]</formula><p xml:id="_TQgnxby">µ(b) 2f (−ab). Then when x ≥ 0 we have by integration by parts:</p><formula xml:id="formula_56" coords="22,72.00,112.30,468.00,239.66">E ab [v(a, b)σ(ax + b)] = 1 0 [f (−b)σ(x + b) + f (b)σ(−x + b)]db = f (0)x − f (−1)(x + 1) + f (1)(−x + 1) + 1 0 f (−b)db − 1 x f (b)db = f (x) + c 1 + c 2 x where c 1 = f (0) − f (1) − f (−1) + f (1) − f (−1) and c 2 = f (0) − f (1) − f (−1). In addition when x &lt; 0, E ab [v(a, b)σ(ax + b)] = 1 0 [f (−b)σ(x + b) + f (b)σ(−x + b)]db = f (0)x − f (−1)(x + 1) + f (1)(−x + 1) + 1 −x f (−b)db − 1 0 f (b)db = f (x) + c 1 + c 2 x</formula><p xml:id="_x76B5cv">so this equality is true for all x. By Lemmas 1 and 2 we can subtract out the constant and linear terms so there exists v(a, b) such that</p><formula xml:id="formula_57" coords="22,232.41,398.79,147.18,11.50">E a,b [v(a, b)σ(ax + b)] = f (x).</formula><p xml:id="_kp4ajgU">In addition, using that µ(b)</p><formula xml:id="formula_58" coords="22,154.53,417.85,302.95,53.88">1 for b ∈ [0, 1] gives that this v(a, b) satisfies sup a,b |v(a, b)| |c 1 | + |c 2 | + max x∈[−1,1] |f (x)| sup x∈[−1,1] k∈{0,1,2} |f (k) (x)|.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_qHbrAh7">D Proofs for Theorem 3</head><p xml:id="_6b96kzZ">The following is a formal restatement of Theorem 1. <ref type="figure" coords="22,72.00,602.10,26.79,14.27">and λ</ref>, η 2 such that with high probability the output θ of Algorithm 1 satisfies the population L 2 loss bound</p><formula xml:id="formula_59" coords="22,72.00,567.43,539.11,33.68">Theorem 6. Select q ∈ W 2,∞ ([−1, 1]). Let η 1 = m 1 m 2 η, and n Kf * −2 L 2 ι 2 +2ασ+1 , m 1 Kf * −2 L 2 ι, m 2 Kf * −2 L 2 ι 2ασ+1 . There exists a choice of η = Θ(ι −χ Kf * −1 L 2 ), T = poly(d, n, m 1 , m 2 , q 2,∞ ),</formula><formula xml:id="formula_60" coords="22,98.22,636.35,413.88,70.51">E x f (x; θ) − f * (x) 2 q 2 1,∞ Kf * −2 L 2 • ι 2 +2ασ+2χ+1 n + ι 2ασ+2χ+1 m 2 + ι m 1 + q • (η • Kf * ) − f * 2 L 2 + q 2 2,∞ ι m 1 + q 4 2,∞ ι + ι 4 +1 n</formula><p xml:id="_DnmDHHW">Proof of Theorem 6. The gradient with respect to w j is</p><formula xml:id="formula_61" coords="23,72.00,95.73,390.60,158.60">∇ w j L 1 (θ (0) ) = 1 n n i f (x i ; θ (0) ) − f * (x i ) ∇ w j f (x i ; θ (0) ) = σ 1 (b (0) j ) • 1 n n i=1 f (x i ; θ (0) ) − f * (x i ) a (0) j m 1 h (0) (x i ) = 1 b (0) j &gt;0 • 1 n n i=1 f (x i ; θ (0) ) − f * (x i ) a (0) j m 1 h (0) (x i ) Therefore w<label>(1)</label></formula><formula xml:id="formula_62" coords="23,170.93,241.63,278.52,53.93">j = −η 1 ∇ w j L 1 (θ (0) ) = 1 b (0) j &gt;0 • η m 2 a (0) j 1 n n i=1 f * (x i ) − f (x i ; θ (0) ) h (0) (x i ).</formula><p xml:id="_wc2kZAf">One then has f (x; (a, W (1) , b (1) ,</p><formula xml:id="formula_63" coords="23,109.63,324.93,397.59,95.19">V (0) )) = m 1 j=1 a j m 1 σ 1 1 b (0) j &gt;0 • a (0) j • η m 2 1 n n i=1 f * (x i ) − f (x i ; θ (0) ) h (0) (x i ), h (0) (x) + b j = m 1 j=1 a j m 1 σ 1 (a (0) j • ηφ(x) + b j ) • 1 b (0) j &gt;0 ,</formula><p xml:id="_FAw6xYx">where φ(x) := 1</p><formula xml:id="formula_64" coords="23,147.07,429.63,227.65,22.73">m 2 n n i=1 f * (x i ) − (x i ; θ (0) ) h (0) (x i ), h (0) (x)</formula><p xml:id="_hb7TsjG">. The second stage of Algorithm 1 is equivalent to random feature regression. The next lemma shows that there exists a * with small norm that acheives low empirical loss on the dataset D 2 . Lemma 4. There exists a * with a * 2 q 2,∞ √ m 1 such that θ * = a * , W (1) , b (0) , V (0) satisfies</p><formula xml:id="formula_65" coords="23,151.18,519.02,299.14,65.40">L 2 (θ * ) q 2 1,∞ Kf * −2 L 2 • ι 2 +2ασ+2χ+1 n + ι 2ασ+2χ+1 m 2 + ι m 1 + ι 2 +1 n + q 2 2,∞ ι m 1 + q • (η • Kf * ) − f * 2 L 2</formula><p xml:id="_syUN84g">The proof of Lemma 4 is deferred to Appendix D.1. We first show that φ is approximately proportional to Kf * , and then invoke the ReLU random feature expressivity results from Appendix C.</p><p xml:id="_5WaW3mb">Next, set</p><formula xml:id="formula_66" coords="23,144.21,654.78,313.09,65.40">λ = a * −2 2 q 2 1,∞ Kf * −2 L 2 • ι 2 +2ασ+2χ+1 n + ι 2ασ+2χ+1 m 2 + ι m 1 + ι 2 +1 n + q 2 2,∞ ι m 1 + q • (η(Kf * )) − f * 2 L 2 , so that L 2 (θ * )</formula><p xml:id="_SmU6PxM">a * 2 2 λ. Define the regularized loss to be L(a) := L 2 (a, W (1) , b (0) , V (0) ) + λ 2 a 2 2 . Let a (∞) = arg min a L(a), and a (t) be the predictor from running gradient descent for t steps initialized at a (0) . We first note that</p><formula xml:id="formula_67" coords="24,239.26,152.07,134.86,22.73">L(a (∞) ) ≤ L(a * ) a * 2 2 λ.</formula><p xml:id="_7qf9fZy">Next, we remark that L(a) is λ-strongly convex. Additionally, we can write f (x; (a, W (1) , b (0) , V (0) )) =</p><formula xml:id="formula_68" coords="24,72.00,205.34,498.51,133.06">a T ψ(x) where ψ(x) = Vec 1 m 1 σ 1 (a (0) j ηφ(x) + b (0) j ) • 1 b (0) j &gt;0 . In Appendix D.2 we show sup x∈D 2 ψ(x) 1 m 1 . Therefore λ max ∇ 2 a L ≤ 2 n x∈D 2 ψ(x) 2 1 m 1 , so L is O( 1 m 1 ) + λ smooth. Choosing a learning rate η 2 = Θ(m 1 ), after T = Õ 1 λm 1 = poly(d, n, m 1 , m 2 , q 2,</formula><p xml:id="_ukKzZ7H">∞ ) steps we reach an iterate â = a (T ) , θ = (â, W (1) , b (0) , V (0) )) satisfying</p><formula xml:id="formula_69" coords="24,212.70,362.89,186.60,16.35">L 2 ( θ) L 2 (θ * ) and â 2 a * 2 .</formula><p xml:id="_JFNdspp">For τ &gt; 0, define the truncated loss τ by τ (z) = min(z 2 , τ 2 ). We have that τ (z) ≤ z 2 , and thus</p><formula xml:id="formula_70" coords="24,216.40,416.09,180.40,32.53">1 n x∈D 2 τ f (x; θ) − f * (x) ≤ L 2 ( θ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_S3UDWRg">Consider the function class</head><formula xml:id="formula_71" coords="24,184.49,493.01,243.02,21.86">F(B a ) := {f (•; (a, W (1) , b (0) , V (0) ) : a 2 ≤ B a )}</formula><p xml:id="_spAcVGW">The following lemma bounds the empirical Rademacher complexity of this function class Lemma 5. Given a dataset D, recall that the empirical Rademacher complexity of F is defined as</p><formula xml:id="formula_72" coords="24,204.70,573.39,202.59,35.77">R D (F) := E σ∈{±1} n sup f ∈F 1 n n i=1 σ i f (x i ) .</formula><p xml:id="_DDVwAVh">Then with high probability</p><formula xml:id="formula_73" coords="24,246.65,651.26,118.71,28.53">R D 2 (F(B a )) B 2 a nm 1 .</formula><p xml:id="_hKAyCsg">Since τ loss is 2τ -Lipschitz, the above lemma with B a = O( a *</p><p xml:id="_sbFGKEC">2 ) = O q 2,∞ √ m 1 along with the standard empirical Rademacher complexity bound yields</p><formula xml:id="formula_74" coords="25,110.55,120.42,380.41,140.76">E x τ f (x; θ) − f * (x) 1 n x∈D 2 τ f (x; θ) − f * (x) + τ • R D 2 (F) + τ 2 ι n q 2 1,∞ Kf * −2 L 2 • ι 2 +2ασ+2χ+1 n + ι 2ασ+2χ+1 m 2 + ι m 1 + q • (η • Kf * ) − f * 2 L 2 + ι 2 +1 n + q 2 2,∞ ι m 1 + τ q 2 2,∞ n + τ 2 ι n .</formula><p xml:id="_e6Xfarq">Finally, we relate the τ population loss to the 2 population loss.</p><p xml:id="_nDFmc3v">Lemma 6. Let τ = Θ(max(ι , q 2,∞ )). Then with high probability over θ,</p><formula xml:id="formula_75" coords="25,154.23,329.12,303.55,32.49">E x f (x; θ) − f * (x) 2 ≤ E x τ f (x; θ) − f * (x) + q 2 2,∞ m 1 .</formula><p xml:id="_rNGpCqY">Plugging in τ and applying Lemma 6 yields</p><formula xml:id="formula_76" coords="25,98.22,405.39,387.67,70.51">E x f (x; θ) − f * (x) 2 q 2 1,∞ Kf * −2 L 2 • ι 2 +2ασ+2χ+1 n + ι 2ασ+2χ+1 m 2 + ι m 1 + q • (η • Kf * ) − f * 2 L 2 + q 2 2,∞ ι m 1 + q<label>4</label></formula><p xml:id="_ZG5SRZ2">2,∞ ι + ι 4 +1 n as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_4rhmmhD">D.1 Proof of Lemma 4</head><p xml:id="_vWp7xWe">We require three auxiliary lemmas, all of whose proofs are deferred to Appendix D.4. The first lemma bounds the error between the population learned feature Kf * and the finite sample learned feature φ over the dataset D 2 .</p><p xml:id="_vb39B5y">Lemma 7. With high probability</p><formula xml:id="formula_77" coords="25,159.23,645.25,293.55,29.02">sup x∈D 2 |φ(x) − (Kf * )(x)| ι 2 +2ασ+1 n + ι 2ασ+1 m 2 + ι m 1 .</formula><p xml:id="_NFAx8tU">The second lemma shows that with appropriate choice of η, the quantity ηφ(x) is small. The third lemma expresses the compositional function q • ηφ via an infinite width network.</p><formula xml:id="formula_78" coords="26,72.00,72.70,274.47,16.72">Lemma 8. Let n Kf * −2 L 2 ι 2 +2ασ+1 , m 1 Kf * −2 L 2 ι,</formula><formula xml:id="formula_79" coords="26,72.00,146.42,468.00,37.11">Lemma 9. Assume that n, m 1 , m 2 = Ω( Kf * −2 L 2 ). There exists v : {±1} × R → R such that sup a,b |v(a, b)| ≤ q 2,</formula><p xml:id="_jjU7GZj">∞ and, with high probability over D 2 , the infinite width network</p><formula xml:id="formula_80" coords="26,72.00,186.97,336.18,92.66">f ∞ v (x) := E a,b [v(a, b)σ 1 (aηφ(x) + b)1 b&gt;0 ] satisfies f ∞ v (x) = q(ηφ(x)) for all x ∈ D 2 .</formula><p xml:id="_SYaQ7nh">Proof of Lemma 4. Let v be the infinite width construction defined in Lemma 9. Define a * ∈ R m 1 to be the vector with a * j = v(a</p><formula xml:id="formula_81" coords="26,216.50,299.80,32.36,16.16">(0) j , b<label>(1)</label></formula><p xml:id="_92SGEwp">j ). We can decompose</p><formula xml:id="formula_82" coords="26,117.94,325.65,375.63,104.72">L 2 (θ * ) = 1 n x∈D 2 (f (x; θ * ) − f * (x)) 2 1 n x∈D 2 (f (x; θ * ) − f ∞ v (x)) 2 + 1 n x∈D 2 (f ∞ v (x) − q(ηφ(x))) 2 + 1 n x∈D 2 (q(ηφ(x)) − q(η(Kf * )(x))) 2 + 1 n x∈D 2 (q(η(Kf * )(x)) − f * ) 2</formula><p xml:id="_ahBy7Mc">Take m 1 ι. The first term is the error between the infinite width network f ∞ v (x) and the finite width network f (x; θ * ). This error can be controlled via standard concentration arguments: by Corollary 2 we have that with high probability a * q 2,∞ √ m 1 , and by Lemma 17 we have</p><formula xml:id="formula_83" coords="26,211.14,516.26,190.91,37.24">1 n x∈D 2 (f (x; θ * ) − f ∞ v (x)) 2 q 2 2,∞ ι m 1 .</formula><p xml:id="_z2WeCHf">Next, by Lemma 9 we get that the second term is zero with high probability. We next turn to the third term. Since q is q 1,∞ -Lipschitz on [−1, 1], and</p><formula xml:id="formula_84" coords="26,327.99,576.50,225.06,21.26">sup x∈D 2 |ηφ(x)| ≤ 1, sup x∈D 2 |(η • Kf * )(x)| ≤</formula><p xml:id="_xZ3w6hX">1 by Lemma 8, we can apply Lemma 7 to get</p><formula xml:id="formula_85" coords="26,91.72,613.95,417.41,102.65">sup x∈D 2 |q(ηφ(x)) − q(η(Kf * )(x))| ≤ q 1,∞ sup x∈D 2 |ηφ(x) − η(Kf * )(x)| q 1,∞ η ι 2 +2ασ+1 n + ι 2ασ+1 m 2 + ι m 1 q 1,∞ Kf * −1 L 2 ι 2 +2ασ+1 n + ι 2ασ+1 m 2 + ι m 1</formula><p xml:id="_eDDmStx">and thus</p><formula xml:id="formula_86" coords="27,80.21,95.02,452.78,34.15">1 n x∈D 2 (q(ηφ(x)) − q(η(Kf * )(x))) 2 q 2 1,∞ Kf * −2 L 2 • ι 2 +2ασ+2χ+1 n + ι 2ασ+2χ+1 m 2 + ι m 1 .</formula><p xml:id="_aZK8Rg5">Finally, we must relate the empirical error between q • η • Kf * and f * to the population error. This can be done via standard concentration arguments: in Lemma 19, we show</p><formula xml:id="formula_87" coords="27,120.83,183.13,371.53,35.69">1 n x∈D 2 (q(η(Kf * )(x)) − f * (x)) 2 q • (η(Kf * )) − f * 2 L 2 + q 2 ∞ ι + ι 2 +1 n .</formula><p xml:id="_Xz8phuD">Altogether,</p><formula xml:id="formula_88" coords="27,72.00,258.19,457.19,131.82">L 2 (θ * ) q 2 1,∞ Kf * −2 L 2 • ι 2 +2ασ+2χ+1 n + ι 2ασ+2χ+1 m 2 + ι m 1 + q 2 ∞ ι + ι 2 +1 n + q 2 2,∞ ι m 1 + q • (η(Kf * )) − f * 2 L 2 q 2 1,∞ Kf * −2 L 2 • ι 2 +2ασ+2χ+1 n + ι 2ασ+2χ+1 m 2 + ι m 1 + ι 2 +1 n + q 2 2,∞ ι m 1 + q • (η(Kf * )) − f * 2 L 2 since K op 1 and thus Kf * L 2 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_4fxMTB2">D.2 Proof of Lemma 5</head><p xml:id="_rKmFeXf">Proof. The standard linear Rademacher bound states that for functions of the form x → w T ψ(x) with w 2 B w , the empirical Rademacher complexity is upper bounded by</p><formula xml:id="formula_89" coords="27,260.68,473.05,91.83,31.81">B w n x∈D ψ(x) 2 .</formula><p xml:id="_Umyn5yF">We have that f (x; θ) = a T ψ(x), where</p><formula xml:id="formula_90" coords="27,72.00,543.23,358.36,90.65">ψ(x) = Vec 1 m 1 σ 1 (a (0) j ηφ(x) + b (0) j ) • 1 b (0) j &gt;0 . By Lemma 8, with high probability, sup x∈D 2 |ηφ(x)| ≤ 1. Thus for x ∈ D 2 ψ(x) 2 ≤ 1 m 2 1 (1 + |b j |) 2 1 m 1</formula><p xml:id="_ah6U8Wn">with high probability by Lemma 10. Altogether, we have</p><formula xml:id="formula_91" coords="27,199.19,670.27,213.62,28.89">R D 2 (F(B a )) B a • n • 1/m 1 n = B 2 a nm 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_qXHK8uj">D.3 Proof of Lemma 6</head><p xml:id="_B5mDPVP">Proof. We can bound</p><formula xml:id="formula_92" coords="28,109.54,123.19,389.27,161.05">E x f (x; θ) − f * (x) 2 − E x τ f (x; θ) − f * (x) = E x f (x; θ) − f * (x) 2 − τ 2 • 1 |f(x; θ)−f * (x)|≥τ ≤ E x f (x; θ) − f * (x) 2 • 1 |f(x; θ)−f * (x)|≥τ ≤ E x f (x; θ) − f * (x) 4 1/2 • P f (x; θ) − f * (x) ≥ τ 1/2 E x f (x; θ) 4 1/2 + E x f * (x) 4 1/2 • P f (x; θ) ≥ τ + P(|f * (x)| ≥ τ )</formula><p xml:id="_vQBtHfA">Next, we bound f (x; θ):</p><formula xml:id="formula_93" coords="28,184.66,324.96,242.19,110.26">f (x; θ) = 1 m 1 m 1 j=1 âj σ 1 a (0) j • ηφ(x) + b j 1 b (0) j &gt;0 ≤ 1 m 1 m 1 j=1 |â j |(|ηφ(x)| + |b j |) ≤ â 2 √ m 1 |ηφ(x)| + 1 m 1 â 2 b 2</formula><p xml:id="_ps3nsmS">By Lemma 10, with high probability over the initialization we have b 2 √ m 1 . Next, by Lemma 4, with high probability over the initialization and dataset we have â 2 a * 2 q 2,∞ √ m 1 . Thus with high probability we have</p><formula xml:id="formula_94" coords="28,232.10,513.67,151.78,16.35">f (x; θ) q 2,∞ (|ηφ(x)| + 1)</formula><p xml:id="_9hwgMAz">uniformly over x.</p><p xml:id="_8dSnEqk">We naively bound</p><formula xml:id="formula_95" coords="28,153.63,590.25,304.75,35.77">|φ(x)| ≤ 1 m 2 h (0) (x) • 1 n n i=1 h (0) (x i ) f (x i ; θ (0) ) − f * (x i ) .</formula><p xml:id="_vYrpUGR">By Lemma 11 and Lemma 12, with high probability we have f (x i ; θ (0) ) − f * (x i ) ι for all i.</p><p xml:id="_pkWyb8D">With high probability, we also have h (0) (x i ) √ m 2 . Additionally, we have</p><formula xml:id="formula_96" coords="29,223.38,100.78,171.87,118.96">h (0) (x) 4 2 = m 2 j=1 σ 2 (x • v j ) 2 2 ≤ m 2 m 2 j=1 σ 2 (x • v j ) 4 m 2 m 2 j=1 (1 + |x • v j | 4ασ )</formula><p xml:id="_uqq4kb2">Since x is C γ subGaussian and v = 1, we have</p><formula xml:id="formula_97" coords="29,72.00,258.85,311.81,96.26">E x h (0) (x) 4 2 m 2 2 E |x • v j | 4ασ m 2 2 . Altogether, E x |φ(x)| 4 ι 4 ,</formula><p xml:id="_grhAfaR">and thus</p><formula xml:id="formula_98" coords="29,72.00,383.65,369.36,56.77">E x f (x; θ) 4 1/2 q 2 2,∞ η 2 ι 2 ≤ q 2 2,∞ m 2 1 Kf * −2 L 2 ι 2 By Assumption 2, E x [(f * (x)) 4 ] 1.</formula><p xml:id="_rb6Ndtp">We choose τ = Θ(max(ι , q 2,∞ ). By Lemma 7 and Lemma 8 { f (x, θ) &gt; τ } and {|f * (x)| &gt; τ } are both high probability events. Therefore by choosing C sufficiently large, we have the bound</p><formula xml:id="formula_99" coords="29,191.24,498.12,231.21,22.80">P f (x; θ) ≥ τ + P(|f * (x)| ≥ τ ) ≤ m −4 1 ι −2</formula><p xml:id="_XjzMZuY">Altogether, this gives</p><formula xml:id="formula_100" coords="29,82.70,553.55,446.61,33.63">E x f (x; θ) − f * (x) 2 − E x τ f (x; θ) − f * (x) q 2 2,∞ Kf * −2 L 2 m 2 1 + 1 m 4 1 ≤ q 2 2,∞<label>m 1 .</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_SNvPZXJ">D.4 Auxiliary Lemmas</head><p xml:id="_rdCvP3D">Proof of Lemma 7. We can decompose</p><formula xml:id="formula_101" coords="30,86.90,120.91,428.56,146.81">|φ(x) − (Kf * )(x)| = 1 m 2 • 1 n n i=1 f (x i ; θ (0) ) − f * (x i ) h (0) (x i ), h (0) (x) − (Kf * )(x) ≤ 1 m 2 n n i=1 f (x i ; θ (0) ) h (0) (x i ), h (0) (x) + 1 m 2 1 n n i=1 f * (x i ) h (0) (x i ), h (0) (x) − E x f * (x)h (0) (x) , h (0) (x) + 1 m 2 E x f * (x)h (0) (x) , h (0) (x) − (Kf * )(x) .</formula><p xml:id="_JzdaS3A">By Lemma 11 and Lemma 13 we can upper bound the first term by ι m 1 . For the second term, by Lemma 13 and Lemma 15 we have for all</p><formula xml:id="formula_102" coords="30,73.20,307.98,497.51,61.17">x ∈ D 2 1 m 2 1 n n i=1 f * (x i )h (0) (x i ) − E x f * (x)h (0) (x) 2 h (0) (x) 2 1 m 2 √ m 2 • m 2 ι 2 +2ασ+1 n ι 2 +2ασ+1</formula><p xml:id="_6t8XSrG">n .</p><p xml:id="_KhYu322">The third term can be bounded as</p><formula xml:id="formula_103" coords="30,105.16,400.97,405.17,35.81">1 m 2 m 2 i=1 E x [f * (x )σ 2 (x • v j )σ 2 (x • v j )] − E x ,v [f * (x)σ 2 (x • v)σ 2 (x • v)] ι 2ασ+1 m 2</formula><p xml:id="_zN4ZK6J">Proof of Lemma 8. Conditioning on the event where Corollary 1 holds, and with the choice η =</p><formula xml:id="formula_104" coords="30,72.00,486.66,447.29,220.04">1 2 C −1 K e −1 Kf * −1 L 2 ι −χ , we get that with high probability sup x∈D 2 |(η • Kf * )(x)| ≤ 1 2 . Therefore sup x∈D 2 |ηφ(x)| = C −1 K e −1 Kf * −1 L 2 ι −χ • sup x |φ(x)| ≤ C −1 K e −1 Kf * −1 L 2 ι −χ sup x |(Kf * )(x)| + O ι 2 +2ασ+1 n + ι 2ασ+1 m 2 + ι m 1 ≤ 1 2 + O Kf * −1 L 2 ι −χ • ι 2 +2ασ+1 n + ι 2ασ+1 m 2 + ι m 1 ≤ 1.</formula><p xml:id="_eT7SXcr">Proof of Lemma 9. Conditioning on the event where Lemma 8 holds and applying Lemma 3, we get that there exists v such that</p><formula xml:id="formula_105" coords="31,210.36,116.64,191.28,11.50">E a,b [v(a, b)σ 1 (aηφ(x) + b)] = q(ηφ(x))</formula><p xml:id="_bWVAz8U">for all x ∈ D 2 . Since v(a, b) = 0 for b ≤ 0, we have that v(a, b) = v(a, b)1 b&gt;0 . Thus the desired claim is true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_e2xF98V">D.5 Concentration</head><p xml:id="_yGPbbrN">Lemma 10. Let m 1 ι. With high probability,</p><formula xml:id="formula_106" coords="31,257.71,238.46,97.77,35.81">1 m 1 m 1 i=1 b (0) i 2 1.</formula><p xml:id="_wweGu2T">Proof. By Bernstein, we have</p><formula xml:id="formula_107" coords="31,224.85,312.33,167.48,35.82">1 m 1 m 1 i=1 b (0) i 2 − 1 ι m 1 ≤ 1.</formula><p xml:id="_6gdAr4B">Lemma 11. With high probability, f (x; θ (0) ) ι m 1 . for all x.</p><p xml:id="_Cam9xQQ">Proof. We have</p><formula xml:id="formula_108" coords="31,235.43,442.22,141.14,35.81">f (x; θ (0) ) = 1 m 1 m 1 i=1 a i σ 1 (b i ).</formula><p xml:id="_KyqFAaJ">Since a i ∼ Unif({±1}) and b i ∼ N (0, 1), the quantities a i σ 1 (b i ) are 1-subGaussian, and thus by Hoeffding with high probability we have</p><formula xml:id="formula_109" coords="31,259.17,530.90,97.64,27.79">f (x; θ (0) ) ι m 1 .</formula><p xml:id="_24CEQ6j">Lemma 12. With high probability</p><formula xml:id="formula_110" coords="31,163.58,622.78,284.83,23.61">sup x∈D 2 E v σ 2 (x • v) 4 1 and sup j∈[m 2 ] E x σ 2 (x • v j ) 4 1.</formula><p xml:id="_hRZd8Qx">Proof. First, we have that σ 2 (x • v) 4 1 + (x • v) 4ασ . Therefore</p><formula xml:id="formula_111" coords="31,168.40,686.12,275.21,22.73">E v σ 2 (x • v) 4 1 + E v (x • v) 4ασ 1 + x 4ασ d −2ασ .</formula><p xml:id="_wVWgwKn">Next, with high probability we have sup</p><formula xml:id="formula_112" coords="32,72.00,72.70,454.35,92.87">x∈D 2 x 2 − E x 2 ≤ ιC 2 γ ι. Since E x 2 dγ 2 x d, we can thus bound sup x∈D 2 E v σ 2 (x • v) 4 1 + γ 4ασ v sup x∈D 2 x 2 1 + d −2ασ (d + ι) 2ασ 1.</formula><p xml:id="_VbrPrxa">The proof for the other inequality is identical.</p><formula xml:id="formula_113" coords="32,72.00,195.17,400.22,22.07">Lemma 13. Let m 2 ι 2ασ+1 . With high probability, sup x∈D 1 ∪D 2 h (0) (x) √ m 2</formula><p xml:id="_QWfwv4v">Proof. Since x is C γ subGaussian and v is 1/ √ d subGaussian, with high probability we have</p><formula xml:id="formula_114" coords="32,248.48,253.66,115.04,20.74">x • v ≤ O(C γ ι) = O(ι).</formula><p xml:id="_2nnXDav">Union bounding over x ∈ D 2 , j ∈ [m 2 ], with high probability we have</p><formula xml:id="formula_115" coords="32,72.00,274.92,468.00,56.84">sup x∈D 1 ∪D 2 ,j∈[m 2 ] |x • v j | ≤ O(ι). Therefore sup x,v j |σ 2 (x • v j )| ≤ O(ι ασ ).</formula><p xml:id="_rWEmGnH">Next, see that</p><formula xml:id="formula_116" coords="32,218.52,364.80,176.16,35.81">1 m 2 h (0) (x) 2 = 1 m 2 m 2 j=1 σ 2 (x • v j ) 2 .</formula><p xml:id="_SdS68h8">Pick truncation radius R = O(ι ασ ). By Hoeffding's inequality and a union bound over x ∈ D 1 ∪D 2 , we have that with high probability</p><formula xml:id="formula_117" coords="32,108.74,445.87,394.52,35.81">sup x∈D 1 ∪D 2 1 m 2 m 2 j=1 1 σ 2 (x•v j ) 2 ≤R σ 2 (x • v j ) 2 − E v 1 σ 2 (x•v j ) 2 ≤R σ 2 (x • v) 2 ι 2ασ+1 m 2 .</formula><p xml:id="_7JYUHTJ">Observe that</p><formula xml:id="formula_118" coords="32,114.95,509.71,385.60,67.27">E v 1 σ 2 (x•v) 2 ≤R σ 2 (x • v) 2 − E v σ(x • v) 2 = E v 1 σ 2 (x•v) 2 &gt;R σ 2 (x • v) 2 ≤ P(σ 2 (x • v) 2 &gt; R)E σ 2 (x • v) 4 1/2 ≤ 1 d .</formula><p xml:id="_NnUxfQ9">Therefore with high probability</p><formula xml:id="formula_119" coords="32,189.45,605.95,238.28,28.52">1 m 2 h (0) (x) 2 − E v σ(x • v) 2 ι 2ασ+1 m 2 + 1 d .</formula><p xml:id="_Q9rskWk">by Lemma 12. Lemma 12 also tells us that</p><formula xml:id="formula_120" coords="32,72.00,642.63,467.50,60.80">E v [σ 2 (x • v) 2 ] = O(1). Altogether, for m 2 ι 2ασ+1 we have 1 m 2 h (0) (x) 2 ≤ 2</formula><p xml:id="_nz2qaEh">and hence h (0) (x) √ m 2 , as desired.</p><formula xml:id="formula_121" coords="33,72.00,72.91,446.10,22.52">Lemma 14. P x∼ν |f * (x)| ≥ C f eι ≤ e −ι and P x∼ν (|(Kf * )(x)| ≥ C K eι χ • Kf * L 2 ) ≤ e −ι</formula><p xml:id="_kX8atSE">Proof. By Markov's inequality, we have</p><formula xml:id="formula_122" coords="33,175.16,125.60,261.67,32.43">P(|f * (x)| &gt; t) = P(|f * (x)| q &gt; t q ) ≤ f * q q t q ≤ C q f q q t q .</formula><p xml:id="_wPrBRf5">Choose t = C f eι . We select q = ι e 1−1/ , which is at least 1 for C in the definition of ι sufficiently large. Plugging in, we get</p><formula xml:id="formula_123" coords="33,164.55,206.33,282.89,33.73">P |f * (x)| &gt; C f eι ≤ C q f ι q C q f e q ι q = e −q = e −ι e 1/ −1 ≤ e −ι ,</formula><p xml:id="_S5tktjW">since e 1/ −1 ≥ 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_H8d8848">An analogous derivation for the function</head><formula xml:id="formula_124" coords="33,72.00,306.23,468.00,27.78">|(Kf * )(x)| C K e • ι χ Kf * L 2</formula><p xml:id="_bX75UKW">Proof. Union bounding the previous lemma over x ∈ D 2 yields the desired result.</p><p xml:id="_fz9FJya">Lemma 15. With high probability,</p><formula xml:id="formula_125" coords="33,161.05,401.71,296.05,37.46">1 n n i=1 f * (x i )h (0) (x i ) − E x f * (x)h (0) (x) 2 m 2 ι 2 +2ασ+1 n</formula><p xml:id="_mWsNFe8">Proof. Consider the quantity</p><formula xml:id="formula_126" coords="33,72.00,476.08,468.00,76.25">1 n n i=1 f * (x i )σ 2 (x i • v j ) − E x f * (x)h (0) (x i ) . With high probability, sup x∈D 2 ,j∈[m 2 ] |f * (x i )σ 2 (x i • v j )| ι +ασ . Pick truncation radius R = O(ι +ασ ).</formula><p xml:id="_CuxjV27">By Hoeffding, we have with high probability.</p><formula xml:id="formula_127" coords="33,77.68,566.48,461.82,35.77">1 n n i=1 f * (x i )σ 2 (x i • v j )1 f * (x i )σ 2 (x i •v j )≤R − E x f * (x)σ 2 (x i • v j )1 f * (x i )σ 2 (x i •v j )≤R ι 2 +2ασ+1 n .</formula><p xml:id="_9jDNw4y">Furthermore, note that</p><formula xml:id="formula_128" coords="33,164.15,639.03,283.04,81.97">E x f * (x)σ 2 (x • v j )1 f * (x i )σ 2 (x•v j )≤R − E x [f * (x)σ 2 (x i • v j )] = E 1 f * (x)σ 2 (x•v j )&gt;R f * (x)σ 2 (x i • v j ) ≤ P(f * (x)σ 2 (x • v j ) &gt; R)E x σ 2 (x • v j ) 4 1 n .</formula><p xml:id="_pEua3BT">Thus</p><formula xml:id="formula_129" coords="34,163.21,97.58,289.08,35.77">1 n n i=1 f * (x i )σ 2 (x i • v j ) − E x f * (x)h (0) (x i ) ι 2 +2ασ+1 n</formula><p xml:id="_6rKzHuP">By a union bound, the above holds with high probability for all j ∈ [m 2 ], and thus</p><formula xml:id="formula_130" coords="34,72.00,171.39,468.00,216.28">1 n n i=1 f * (x i )h (0) (x i ) − E x f * (x)h (0) (x) 2 m 2 ι 2 +2ασ+1 n Lemma 16. With high probability, sup x∈X d 1 m 2 m 2 i=1 E x [f * (x )σ 2 (x • v j )σ 2 (x • v j )] − E x ,v [f * (x)σ 2 (x • v)σ 2 (x • v)] ι 2ασ+1 m 2 Proof. Fix x ∈ D 2 . Consider the random variables Z(v) = E x [f * (x )σ 2 (x • v)σ 2 (x • v)]. With high probability we have |σ 2 (x • v j )| ι ασ and thus |Z(v j )| = |E x [f * (x )σ 2 (x • v j )σ 2 (x • v j )]| ι ασ • E x f * (x ) 2 1/2 E σ 2 (x • v j ) 2 2 ι ασ .</formula><p xml:id="_Edm8MvJ">For all j ∈ [m 2 ]. Choosing truncation radius R = ι ασ , with Hoeffding we have with high probability that</p><formula xml:id="formula_131" coords="34,77.18,435.02,499.51,35.81">1 m 2 m 2 i=1 E x [f * (x )σ 2 (x • v j )]σ 2 (x • v j )1 |Z(v j )|≤R − E x ,v f * (x )σ 2 (x • v)σ 2 (x • v)1 |Z(v)|≤R ι 2ασ+1 m 2</formula><p xml:id="_eMVBCyq">Next, we have that</p><formula xml:id="formula_132" coords="34,129.14,507.61,353.05,85.82">E x ,v f * (x )σ 2 (x • v)σ 2 (x • v)1 |Z(v)|≤R − E x ,v [f * (x )σ 2 (x • v)σ 2 (x • v)] = E x ,v f * (x )σ 2 (x • v)σ 2 (x • v)1 |Z(v)|&gt;R ≤ P(Z(v) &gt; R) • E x ,v f * (x ) 2 σ 2 (x • v) 2 σ 2 (x • v) 2 1/2 1 m 2 .</formula><p xml:id="_9XuFr9G">Conditioning on the high probability event that sup j∈[m 2 ] |Z(v j )| ≤ R, we have with high probability that</p><formula xml:id="formula_133" coords="34,102.14,646.23,412.90,35.81">1 m 2 m 2 i=1 E x [f * (x )σ 2 (x • v j )]σ 2 (x • v j ) − E x ,v [f * (x )σ 2 (x • v)σ 2 (x • v)] ι 2ασ+1 m 2 .</formula><p xml:id="_bQ4zvnd">A union bound over x ∈ D 2 yields the desired result.</p><p xml:id="_9dagv7g">Lemma 17. With high probability,</p><formula xml:id="formula_134" coords="35,144.83,98.84,321.14,35.81">sup x∈D 2 1 m 1 m 1 i=1 v(a i , b i )σ 1 (a i ηφ(x) + b i )1 b i &gt;0 − f ∞ v (x) v 2 ∞ ι m 1</formula><p xml:id="_7EKW5Ne">Proof. Condition on the high probability event sup x∈D 2 |ηφ(x)| ≤ 1. Next, note that whenever b &gt; 2 that v(a, b) = 0. Therefore we can bound</p><formula xml:id="formula_135" coords="35,210.93,184.86,189.63,20.74">|v(a, b)σ 1 (aηφ(x) + b i )1 b i &gt;0 | ≤ 2 v ∞</formula><p xml:id="_rjcXFNH">Therefore by Hoeffding's inequality we have that</p><formula xml:id="formula_136" coords="35,157.01,231.34,303.16,35.82">1 m 1 m 1 i=1 v(a i , b i )σ 1 (a i ηφ(x) + b i )1 b i &gt;0 − f ∞ v (x) v 2 ∞ ι m 1 +</formula><p xml:id="_jB6AUdV">The desired result follows via a Union bound over x ∈ D 2 .</p><p xml:id="_2kmj6BX">Lemma 18. With high probability,</p><formula xml:id="formula_137" coords="35,72.00,326.27,332.69,91.53">1 m 1 m 1 i=1 v(a i , b i ) 2 − v 2 L 2 v 4 ∞ ι m 1 . Proof. Note that v(a i , b i ) 2 ≤ v 2 ∞</formula><p xml:id="_V8whyQq">Thus by Hoeffding's inequality we have that</p><formula xml:id="formula_138" coords="35,212.49,443.54,192.21,35.81">1 m 1 m 1 i=1 v(a i , b i ) 2 − v 2 L 2 v 4 ∞ ι m 1 .</formula><p xml:id="_fFxh9Xa">Corollary 2. Let m 1 ι. Then with high probability</p><formula xml:id="formula_139" coords="35,246.04,534.17,121.71,35.92">m i i=1 v(a i , b i ) 2 v 2 ∞ m 1 .</formula><p xml:id="_eVfhA2r">Proof. By the previous lemma, we have that</p><formula xml:id="formula_140" coords="35,191.01,607.13,235.16,35.81">1 m 1 m 1 i=1 v(a i , b i ) 2 − v 2 L 2 v 4 ∞ ι m 1 v 2 ∞ .</formula><p xml:id="_nCVgYEZ">Thus</p><formula xml:id="formula_141" coords="35,188.16,667.60,237.47,35.92">m i i=1 v(a i , b i ) 2 v 2 L 2 m 1 + v 2 ∞ m 1 v 2 ∞ m 1 . 1 n x∈D 2 (q(η(Kf * )(x)) − f * (x)) 2 q • (η(Kf * )) − f * 2 L 2 + q 2 ∞ ι + ι 2 +1 n .</formula><p xml:id="_Y5qCbmr">Proof. Let S be the set of x so that |η(Kf * )(x)| ≤ 1 and |f * (x)| ι . Consider the random variables (q(η(Kf * )(x)) − f * (x)) 2 • 1 x∈S . We have that</p><formula xml:id="formula_142" coords="36,172.63,183.55,270.73,23.10">(q(η(Kf * )(x)) − f * (x)) 2 • 1 x∈S sup z∈[−1,1] |q(z)| 2 + ι 2 .</formula><p xml:id="_RkU4Bgh">and</p><formula xml:id="formula_143" coords="36,91.93,246.04,428.13,24.29">E (q(η(Kf * )(x)) − f * (x)) 2 • 1 x∈S 2 sup z∈[−1,1] |q(z)| 2 + ι 2 • q • (η(Kf * )) − f * 2 L 2 .</formula><p xml:id="_H3pFUeh">Therefore by Berstein's inequality we have that</p><formula xml:id="formula_144" coords="36,85.01,309.85,447.15,108.79">1 n x∈D 2 (q(η(Kf * )(x)) − f * (x)) 2 1 x∈S − (q • (η(Kf * )) − f * )1 x∈S 2 L 2 sup z∈[−1,1] |q(z)| 2 + ι 2 • (q • (η(Kf * )) − f * )1 x∈S 2 L 2 ι n + sup z∈[−1,1] |q(z)| 2 + ι 2 n ι (q • (η(Kf * )) − f * )1 x∈S 2 L 2 + sup z∈[−1,1] |q(z)| 2 + ι 2 n ι.</formula><p xml:id="_pJ3fwGp">Conditioning on the high probability event that x ∈ S for all x ∈ D 2 , we get that</p><formula xml:id="formula_145" coords="36,82.86,446.81,447.47,72.94">1 n x∈D 2 (q(η(Kf * )(x)) − f * (x)) 2 (q • (η(Kf * )) − f * )1 x∈S 2 L 2 + sup z∈[−1,1] |q(z)| 2 + ι 2 n ι ≤ q • (η(Kf * )) − f * 2 L 2 + sup z∈[−1,1] |q(z)| 2 ι + ι 2 +1</formula><p xml:id="_nUw5S76">n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Nb2gT5Y">E Proofs for Section 4 E.1 Single Index Model</head><p xml:id="_bhRm3vf">Proof of Theorem 2. It is easy to see that Assumptions 1 and 3 are satisfied. By assumption g * is polynomially bounded, i.e there exist constants C g , α g such that</p><formula xml:id="formula_146" coords="36,196.58,655.37,218.85,22.73">g * (z) ≤ C g (1 + |z|) αg ≤ C g 2 αg−1 (1 + |z| αg ).</formula><p xml:id="_j5rjZBg">Therefore</p><formula xml:id="formula_147" coords="36,115.96,702.99,386.06,23.01">f * q = g * L q (R,N (0,1) ≤ C g 2 αg−1 (1 + E z∼N (0,1) [|z| αgq ] 1/q ) ≤ C g 2 αg α αg/2 g q αg/2 .</formula><p xml:id="_hbE55K3">Thus Assumption 2 is satisfied with C f = C g 2 αg α αg/2 g and = α g /2.</p><p xml:id="_gr8XFR6">Next, we see that</p><formula xml:id="formula_148" coords="37,72.00,118.29,359.92,83.48">K(x, x ) = E v [(x • v)(x • v)] = x • x d Therefore (Kf )(x) = E x [f * (x )x • x/d] = 1 d E x [∇f * (x )] • x.</formula><p xml:id="_23cnW8k">Furthermore, we have</p><formula xml:id="formula_149" coords="37,170.29,232.09,271.42,21.86">E x [∇f * (x )] = E x [w * g (w * • x )] = w * E z∼N (0,1) [g (z)].</formula><p xml:id="_nSxXkRg">Altogether, letting c 1 = E z∼N (0,1) [g (z)] = Ω(1), we have</p><formula xml:id="formula_150" coords="37,72.00,256.06,468.00,28.85">(Kf * )(x) = 1 d c 1 x T w * . Assumption 4 is thus satisfied with χ = 1/2.</formula><p xml:id="_7sJ2nNY">Next, see that Kf * L 2 = c 1 /d. We select the test function q to be q(z) = g(η</p><formula xml:id="formula_151" coords="37,220.52,292.17,314.26,47.16">−1 d/c 1 • z), so that q(η(Kf * )(x)) = g(x * • x) = f * (x).</formula><p xml:id="_jtJhdjT">Since η = Θ(dι −χ ), we see that</p><formula xml:id="formula_152" coords="37,165.84,367.86,280.33,76.27">sup z∈[−1,1] |q(z)| = sup z∈[−Θ(ι χ ),Θ(ι χ )] |g(z)| = poly(ι) sup z∈[−1,1] |q (z)| = η −1 d/c 1 sup z∈[−Θ(ι χ ),Θ(ι χ )] |g (z)| = poly(ι) sup z∈[−1,1] |q (z)| = η −2 d 2 /c 2 1 sup z∈[−Θ(ι χ ),Θ(ι χ )] |g (z)| = poly(ι)</formula><p xml:id="_9rTtJs6">Therefore we can bound the population loss as</p><formula xml:id="formula_153" coords="37,179.89,497.96,252.22,29.41">E x f (x; θ) − f * (x) 2 d 2 poly(ι) min(n, m 1 , m 2 ) + 1 √ n .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_mMRHqSV">E.2 Quadratic Feature</head><p xml:id="_CCQcZHm">Throughout this section, we call x T Ax a degree 2 spherical harmonic if A is symmetric, E x T Ax = 0, and E (x T Ax) 2 = 1. Then, we have that Tr(A) = 0, and also</p><formula xml:id="formula_154" coords="37,93.03,637.70,425.95,27.79">1 = E[x ⊗4 ](A ⊗2 ) = 3χ 2 I ⊗2 (A ⊗2 ) = 2χ 2 A 2 F =⇒ A F = 1 2χ 2 = d + 2 2d = Θ(1).</formula><p xml:id="_ACbTDhp">See Appendix F for technical background on spherical harmonics.</p><p xml:id="_rnAEfAq">Our goal is to prove the following key lemma, which states that the projection of f * onto degree 2 spherical harmonics is approximately x T Ax.</p><p xml:id="_W9UYKV9">Lemma 20. Let q be a L-Lipschitz function with |q(0)| ≤ L, and let the target f * be of the form f * (x) = q(x T Ax), where x T Ax is a spherical harmonic. Let c 1 = E z∼N (0,1) [q (z)]. Then</p><formula xml:id="formula_155" coords="38,211.04,113.08,196.56,21.86">P 2 f * − c 1 x T Ax L 2 Lκ 1/6 d −1/12 log d.</formula><p xml:id="_dheBHJf">We defer the proof of this Lemma to Appendix E.2.1.</p><p xml:id="_Fk9m4FD">As a consequence, the learned feature Kf * is approximately proportional to x T Ax.</p><formula xml:id="formula_156" coords="38,72.00,186.71,468.00,113.41">Lemma 21. Recall c 1 = E z∼N (0,1) [q (z)]. Then Kf * − λ 2 2 (σ)c 1 x T Ax L 2 Lκ 1/6 d −2−1/12 log d Proof. Since E[f * (x)] = 0, P 0 f * = 0. Next, since f * is an even function, P k f * = 0 for k odd. Thus Kf * − λ 2 2 (σ)P 2 f * L 2 d −4 .</formula><p xml:id="_hxecA4p">Additionally, by Lemma 20 we have that</p><formula xml:id="formula_157" coords="38,72.00,325.75,402.22,264.77">P 2 f * − c 1 x T Ax L 2 T 2 − c 1 • A F Lκ 1/6 d −1/12 log d. Since λ 2 2 (σ) = Θ(d −2 ), we have Kf * − λ 2 2 (σ)c 1 x T Ax L 2 κLκ 1/6 d −2−1/12 log d. Corollary 3. Assume κ = o( √ d). Then x T Ax − Kf * −1 L 2 Kf * L 2 Lκ 1/6 d −1/12 log d Proof. x T Ax − Kf * −1 L 2 Kf * L 2 = Kf * −1 L 2 x T Ax Kf * L 2 − Kf * L 2 ≤ Kf * −1 L 2 Kf * − λ 2 2 (σ)c 1 x T Ax L 2 + Kf * −1 L 2 Kf * − λ 2 2 (σ)|c 1 | Lκ 1/6 d −2−1/12 log d Kf * −1 L 2 Lκ 1/6 d −1/12 log d.</formula><p xml:id="_zF2W2YA">Proof of Theorem 3. By our choice of ν, we see that Assumption 1 is satisfied. We next verify Assumption 2. Since f * is 1-Lipschitz, we can bound |g * (z)| ≤ |g * (0)| + |z|, and thus</p><formula xml:id="formula_158" coords="38,189.25,667.85,232.73,59.29">E x g * (x T Ax) q 1/q ≤ |g * (0)| + E x x T Ax q 1/q ≤ |g * (0)| + q ≤ (1 + g * (0))q,</formula><p xml:id="_62ERMAM">where we used Lemma 35. Thus Assumption 2 holds with = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_kANATeE">Finally, we have</head><formula xml:id="formula_159" coords="39,247.89,120.82,116.22,26.03">Kf * = k≥2 λ 2 k (σ 2 )P k f * .</formula><p xml:id="_6QUkqPs">By Lemma 21 we have Kf * L 2 ≥ 1 2 λ 2 2 (σ)c 1 for d larger than some absolute constant. Next, by Lemma 35 we have for any q</p><formula xml:id="formula_160" coords="39,72.00,172.41,346.94,252.20">≤ 1 4 d 2 Kf * q ≤ k≥2 λ 2 k (σ 2 ) P k f * q k≥2 d −k q k/2 P k f * L 2 k≥2 ( √ q/d) k = q d 2 • 1 1 − √ qd ≤ 2qd −2 Therefore Kf * q 4 d 2 λ 2 2 (σ 2 )c 1 q Kf * L 2 q Kf * L 2 , since λ 2 2 (σ 2 ) = Ω(d −2</formula><p xml:id="_GjDYXTu">). Thus Assumption 4 holds with = 1. Next, observe that Kf * L 2 d −2 . We select the test function q to be q(z) = g * (η</p><formula xml:id="formula_161" coords="39,72.00,430.52,468.00,54.61">−1 Kf * −1 L 2 • z). We see that q(η(Kf * )(x)) = g * Kf * −1 L 2 (Kf * )(x)</formula><p xml:id="_bJZN3wj">, and thus</p><formula xml:id="formula_162" coords="39,146.86,516.61,323.27,52.77">f * − q(η(Kf * )(x)) L 2 = g * (x T Ax) − g * Kf * −1 L 2 (Kf * )(x) L 2 x T Ax − Kf * −1 L 2 Kf * L 2 κ 1/6 d −1/12 log d,</formula><p xml:id="_TrF8eh8">where the first inequality follows from Lipschitzness of g * , and the second inequality is Corollary 3.</p><formula xml:id="formula_163" coords="39,72.00,594.14,404.61,105.79">Furthermore since η = Θ( Kf * −1 L 2 ι −χ ), we get that η −1 Kf * −1 L 2 = Θ(ι χ ), and thus sup z∈[−1,1] |q(z)| = sup z∈[−Θ(ι χ ),Θ(ι χ )] |g * (z)| = poly(ι) sup z∈[−1,1] |q (z)| = η −1 Kf * −1 L 2 sup z∈[−Θ(ι χ ),Θ(ι χ )] |(g * ) (z)| = poly(ι) sup z∈[−1,1] |q (z)| = η −1 Kf * −1 L 2 2 sup z∈[−Θ(ι χ ),Θ(ι χ )] |(g * ) (z)| = poly(ι)</formula><p xml:id="_4kSwj7t">Therefore by Theorem 6 we can bound the population loss as</p><formula xml:id="formula_164" coords="40,150.74,99.19,310.52,29.41">E x f (x; θ) − f * (x) d 4 poly(ι) min(n, m 1 , m 2 ) + 1 √ n + κ 1/3 d −1/6 ι .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Mbry9as">E.2.1 Proof of Lemma 20</head><p xml:id="_jF6wYSX">The high level sketch of the proof of Lemma 20 is as follows. Consider a second spherical harmonic x T Bx satisfying E[(x T Ax)(x T Bx)] = 0 (a simple computation shows that this is equivalent to Tr(AB) = 0). We appeal to a key result in universality to show that in the large d limit, the distribution of x T Ax converges to a standard Gaussian; additionally, x T Bx converges to an independent mean-zero random variable. As a consequence, we show that</p><formula xml:id="formula_165" coords="40,156.06,277.69,299.89,21.86">E[q(x T Ax)x T Ax] ≈ E z∼N (0,1) [q(z)z] = E z∼N (0,1) [q (z)] = c 1 .</formula><p xml:id="_MEbMxKU">and</p><formula xml:id="formula_166" coords="40,186.77,330.49,238.47,21.86">E[q(x T Ax)x T Bx] ≈ E[q(x T Ax)] • E[x T Bx] = 0.</formula><p xml:id="_Ba65qVg">From this, it immediately follows that</p><formula xml:id="formula_167" coords="40,256.02,357.49,81.20,21.26">P 2 f * ≈ c 1 x T Ax.</formula><p xml:id="_tyTtzuY">The key universality theorem is the following.</p><p xml:id="_dN4XKPU">Definition 7. For two probability measures µ, ν, the Wasserstein 1-distance between µ and ν is defined as</p><formula xml:id="formula_168" coords="40,190.40,442.19,231.20,22.06">W 1 (µ, ν) := sup f Lip ≤1 |E z∼µ [f (z)] − E z∼ν [f (z)]|,</formula><p xml:id="_efzQan2">where</p><formula xml:id="formula_169" coords="40,109.74,478.93,126.16,18.92">f Lip := sup x =y |f (x)−f (y)| x−y 2</formula><p xml:id="_krA2z4y">is the Lipschitz norm of f . Lemma 22 ([50][Theorem 9.20). ] Let z ∼ N (0, I d ) be a standard Gaussian vector, and let f :</p><formula xml:id="formula_170" coords="40,88.94,517.27,371.35,55.03">R d → R satisfy E[f (z)] = 0, E[(f (z)) 2 ] = 1. Then W 1 (Law(f (z)), N (0, 1)) E ∇f (z) 4 1/4 E ∇ 2 f (z) 4 op 1/4</formula><p xml:id="_2WGAbzb">, where W 1 is the Wasserstein 1-distance.</p><p xml:id="_bxxCjwY">We next apply this lemma to show that the quantities x T Ax + x T Bx and x T Ax + x • u are approximately Gaussian, given appropriate operator norm bounds on A, B.</p><p xml:id="_jfQ2Mcj">Lemma 23. Let x T Ax and x T Bx be orthogonal spherical harmonics. Then, for constants</p><formula xml:id="formula_171" coords="40,72.00,639.35,467.50,60.49">c 1 , c 2 with c 2 1 + c 2 2 = 1, we have that the random variable Y = c 1 x T Ax + c 2 x T Bx satisfies W 1 (Y, N (0, 1)) A op + B op . Proof. Define the function f (z) = c 1 d z T Az z 2 + c 2 d z T Bz z 2 , and let x = z √ d z . Observe that when z ∼ N (0, I), we have x ∼ Unif(S d−1 ( √ d)). Therefore f (z) is equal in distribution to Y . Define f 1 (z) = d z T Az z 2 . We compute ∇f 1 (z) = 2d Az z 2 − z T Az • z z 4</formula><p xml:id="_zMtFKHh">and</p><formula xml:id="formula_172" coords="41,136.49,203.77,339.02,29.82">∇ 2 f 1 (z) = 2d A z 2 − 2Azz T z 4 − 2zz T A z 4 − 2 z T Az z 4 I + 4 z T Azzz T z 6 .</formula><p xml:id="_TrE5K7V">Thus</p><formula xml:id="formula_173" coords="41,159.74,261.83,298.50,39.49">∇f 1 (z) ≤ 2d Az z 2 + z T Az z 3 ≤ √ d z • Ax + x T Ax z .</formula><p xml:id="_HTkuR2H">and</p><formula xml:id="formula_174" coords="41,244.52,338.65,129.59,28.21">∇ 2 f 1 (z) op d z 2 A op .</formula><p xml:id="_UTVperv">z 2 is distributed as a chi-squared random variable with d degrees of freedom, and thus</p><formula xml:id="formula_175" coords="41,72.00,406.90,361.82,112.96">E z −2k = 1 k j=1 (d − 2j) Therefore E ∇ 2 f 1 (z) 4 op 1/4 d A op E z −8 1/4 A op .</formula><p xml:id="_UR7yHgy">and, using the fact that x and z are independent,</p><formula xml:id="formula_176" coords="41,99.77,547.75,412.47,31.51">E ∇f 1 (z) 4 1/4 √ dE z −4 1/4 E Ax 4 1/4 + E z −4 1/4 E (x T Ax) 4 1/4 1.</formula><p xml:id="_w5VdDZV">As a consequence, we have</p><formula xml:id="formula_177" coords="41,142.11,610.26,327.79,27.51">E ∇f (z) 4 1/4 1 and E ∇ 2 f (z) 4 op 1/4 A op + B op .</formula><p xml:id="_Z33cWfr">Thus by Lemma 22 we have</p><formula xml:id="formula_178" coords="41,168.84,672.31,274.33,20.74">W 1 (Y, N (0, 1)) = W 1 (f (z), N (0, 1)) A op + B op .</formula><p xml:id="_zD3rz3U">Lemma 24. Let x T Ax be a spherical harmonic, and u = 1. Then, for constants c 1 , c 2 with</p><formula xml:id="formula_179" coords="42,72.00,87.35,373.94,46.47">c 2 1 + c 2 2 = 1, we have that the random variable Y = c 1 x T Ax + c 2 x T u satisfies W 1 (Y, N (0, 1)) A op .</formula><p xml:id="_rew859J">where W 1 is the 1-Wasserstein distance.</p><formula xml:id="formula_180" coords="42,72.00,156.99,139.35,22.62">Proof. Define f 2 (z) = √ dz T u z</formula><p xml:id="_EhXd7XG">. We have</p><formula xml:id="formula_181" coords="42,230.64,188.39,150.71,31.99">∇f 2 (z) = √ d u z − zz T u z 3 .</formula><p xml:id="_BTNX3Xj">and</p><formula xml:id="formula_182" coords="42,174.15,248.16,263.71,31.99">∇ 2 f 2 (z) = √ d − uz T + zu T z 3 − z T u z 3 I + 3 z T uzz T z 5 .</formula><p xml:id="_jpsjKGx">Thus</p><formula xml:id="formula_183" coords="42,72.00,301.80,367.66,94.67">∇f 2 (z) √ d z and ∇ 2 f 2 (z) op √ d z 2 , so E ∇f 1 (z) 4 1/4 1 and E ∇f 1 (z) 4 1/4 1 √ d .</formula><p xml:id="_xHZe7mz">We finish using the same argument as above.</p><p xml:id="_wWj42j8">Lemma 23 implies that, when A op , B op are small, (x T Ax, x T Bx) is close in distribution to the standard Gaussian in 2-dimensions. As a consequence, E[q(x T Ax)x T Bx] ≈ E[q(z 1 )] E[z 2 ] = 0, where z 1 , z 2 are i.i.d Gaussians. This intuition is made formal in the following lemma.</p><p xml:id="_UDmeKuV">Lemma 25. Let x T Ax, x T Bx be two orthogonal spherical harmonics. Then</p><formula xml:id="formula_184" coords="42,173.22,510.58,262.41,21.86">E q(x T Ax)x T Bx ≤ L A op log d + B op log d</formula><p xml:id="_VfDNHjS">Proof. Define the function F (t) = t 0 q(s)ds. Then F (t) = q(t), so by a Taylor expansion we get that</p><formula xml:id="formula_185" coords="42,72.00,582.32,445.61,69.75">|F (x + y) − F (x) − yq(x)| ≤ 2 y 2 L. Therefore E q(x T Ax)x T Bx ≤ LE (x T Bx) 2 + −1 E F (x T Ax + x T Bx) − E F (x T Ax) .</formula><p xml:id="_E2xAQgC">Pick truncation radius R, and define the function</p><formula xml:id="formula_186" coords="42,72.00,654.93,468.00,36.97">F (z) = F (max(−R, min(R, z)). F has Lipschitz constant sup z∈[−R,R] |q(z)|, and thus since W 1 x T Ax, N (0, 1)</formula><p xml:id="_bS68bPT">A op , we have</p><formula xml:id="formula_187" coords="42,175.07,696.14,265.85,22.22">EF (x T Ax) − E z∼N (0,1) F (z) sup z∈[−R,R] |q(z)| • A op .</formula><p xml:id="_kcEWWR4">Next, we have</p><formula xml:id="formula_188" coords="43,80.74,97.03,454.51,24.15">E F (x T Ax) − F (x T Ax) ≤ E 1 |x T Ax|&gt;R F (x T Ax) ≤ P( x T Ax &gt; R) • E F (x T Ax) 2 1/2 .</formula><p xml:id="_jVw5xFz">Likewise,</p><formula xml:id="formula_189" coords="43,192.19,147.07,231.61,23.93">E z F (z) − F (z) ≤ P(|z| &gt; R) • E F (z) 2 1/2 .</formula><p xml:id="_2wvPKx4">Since q is L-Lipschitz, we can bound |F (z)| ≤ |z||q(0)| + 1 2 L|z| 2 , and thus</p><formula xml:id="formula_190" coords="43,196.85,199.96,218.30,14.41">E F (z) 2 L 2 and E F (x T Ax) 2 L 2 .</formula><p xml:id="_6AM6Mks">The standard Gaussian tail bound yields</p><formula xml:id="formula_191" coords="43,268.46,224.26,271.54,14.41">P(|z| &gt; R) exp(−C 1 R 2 ) for appropriate constant C 1 ,</formula><p xml:id="_VCExVJr">and polynomial concentration yields</p><formula xml:id="formula_192" coords="43,72.00,238.71,468.00,66.67">P x T Ax &gt; R exp(−C 2 R) for appropriate constant C 2 . Thus choosing R = C 3 log d for appropriate constant C 3 , we get that E F (x T Ax) − F (x T Ax) + E z F (z) − F (z) L d .</formula><p xml:id="_aJ2STTK">Altogether, since |q(z)| ≤ |q(0)| + L|z|, we get that</p><formula xml:id="formula_193" coords="43,195.68,335.84,224.62,21.86">EF (x T Ax) − E z∼N (0,1) F (z) A op L log d.</formula><p xml:id="_wdjTCSR">By an identical calculation, we have that for &lt; 1,</p><formula xml:id="formula_194" coords="43,119.37,377.98,377.25,31.20">EF (x T Ax + x T Bx) − E z∼N (0,1) F (z √ 1 + 2 ) A op + B op • L log d</formula><p xml:id="_FQXHHsC">Altogether, we get that</p><formula xml:id="formula_195" coords="43,119.38,438.40,377.22,43.92">E F (x T Ax + x T Bx) − E F (x T Ax) ≤ A op + B op • L log d + E z∼N (0,1) F (z) − E z∼N (0,1) F (z √ 1 + 2 ) .</formula><p xml:id="_2sY8yXF">Via a simple calculation, one sees that</p><formula xml:id="formula_196" coords="43,72.00,507.60,426.40,215.76">F (z √ 1 + 2 ) − F (z) ≤ |q(0)||z| √ 1 + 2 − 1 + L 2 z 2 2 L|z| 2 + Lz 2 2 . Therefore E F (x T Ax + x T Bx) − E F (x T Ax) L A op + B op log d + 2 , so E q(x T Ax)x T Bx ≤ L −1 A op log d + + B op log d . Setting = A op log d yields E q(x T Ax)x T Bx L A op log d + B op log d , as desired.</formula><p xml:id="_sUdFNF5">Similarly, we use the consequence of Lemma 24 that (x T Ax, u T x) is close in distribution to a 2d standard Gaussian, and show that E[q(x T Ax)</p><formula xml:id="formula_197" coords="44,72.00,88.62,405.63,66.35">(u T x) 2 − 1 ] ≈ E[q(z 1 )(z 2 2 − 1)] = 0. Lemma 26. E q(x T Ax)(u T x) 2 − E z [q(z)] L A 1/3 op log 2/3 d.</formula><p xml:id="_NzujEk5">Proof. Let G(t) = t 0 F (s)ds. Then G (t) = F (t), G (t) = q(t), so a Taylor expansion yields</p><formula xml:id="formula_198" coords="44,171.63,187.93,268.74,42.00">G(x + y) = G(x) + yF (x) + 2 y 2 q(x) + O( 3 |y| 3 L) G(x − y) = G(x) − yF (x) + 2 y 2 q(x) + O( 3 |y| 3 L).</formula><p xml:id="_2JXZJAD">Thus</p><formula xml:id="formula_199" coords="44,72.00,251.53,456.79,77.49">2 y 2 q(x) = 1 2 (G(x + y) + G(x − y) − 2G(x)) + O( 3 |y| 3 L). Therefore E q(x T Ax)(u T x) 2 L + −2 E G(x T Ax + u T x) + G(x T Ax − u T x) − 2G(x T Ax) .</formula><p xml:id="_4cnS9pa">For truncation radius R, define</p><formula xml:id="formula_200" coords="44,72.00,331.37,468.00,62.32">G(z) = G(max(−R, min(R, z))). We get that G has Lipschitz constant sup |z|≤R |F (z)| LR 2 . Therefore EG(x T Ax) − E z∼N (0,1) G(z) LR 2 |A| op ,</formula><p xml:id="_24ns8jr">and by a similar argument in the previous lemma, setting R = C 3 log d yields</p><formula xml:id="formula_201" coords="44,178.75,418.21,258.48,27.73">E G(x T Ax) − G(x T Ax) + E z G(z) − G(z) L d .</formula><p xml:id="_bqMjfGU">Altogether,</p><formula xml:id="formula_202" coords="44,209.34,473.63,197.30,22.07">EG(x T Ax) − E z G(z) A op L log 2 d.</formula><p xml:id="_nGwDHkc">By an identical calculation,</p><formula xml:id="formula_203" coords="44,165.55,514.24,284.89,31.20">EG(x T Ax ± u T x) − E z G(z √ 1 + 2 ) A op • L log 2 d.</formula><p xml:id="_dvxuQjV">Additionally, letting z, w be independent standard Gaussians,</p><formula xml:id="formula_204" coords="44,123.97,566.45,368.78,62.42">−2 E z 2G(z √ 1 + 2 − 2G(z) = −2 E z,w [G(z + w) + G(z − w) − 2G(z)] = E z,w q(z)w 2 + O( L) = E z [q(z)] + O( L).</formula><p xml:id="_E8SVn2p">Altogether,</p><formula xml:id="formula_205" coords="44,163.58,661.80,288.83,22.07">E q(x T Ax)(u T x) 2 − E z [q(z)] L + −2 A op • L log 2 d</formula><p xml:id="_6yBuffY">L A 1/3 op log 2/3 d, where we set = A 1/3 op log 2/3 d.</p><p xml:id="_CwXfyf4">Lemma 25 shows that when B op 1, E[q(x T Ax)x T Bx] ≈ 0. However, we need to show this is true for all spherical harmonics, even those with B op = Θ(1). To accomplish this, we decompose B into the sum of a low rank component and small operator norm component. We use Lemma 25 to bound the small operator norm component, and Lemma 26 to bound the low rank component. Optimizing over the rank threshold yields the following desired result: Lemma 27. Let A, B be orthogonal spherical harmonics. Then E q(x T Ax)x T Bx ≤ L A 1/6 op log d.</p><p xml:id="_sC73V9M">Proof. Let τ &gt; A op be a threshold to be determined later. Decompose B as follows:</p><formula xml:id="formula_206" coords="45,131.19,228.08,349.62,37.31">B = d i=1 λ i u i u T i = |λ i |&gt;τ λ i u i u T i − 1 d I − 1 A 2 F |λ i |&gt;τ u T i Au i • A + B,</formula><p xml:id="_hzwAHwt">where</p><formula xml:id="formula_207" coords="45,160.64,291.32,293.51,33.17">B = |λ i |≤τ λ i u i u T i + I • 1 d |λ i |&gt;τ λ i + 1 A 2 F |λ i |&gt;τ λ i u T i Au i • A.</formula><p xml:id="_KEamEsY">By construction, we have,</p><formula xml:id="formula_208" coords="45,72.00,355.86,370.55,84.70">Tr B = |λ i |≤τ λ i + |λ i |&gt;τ λ i = i∈[d] λ i = 0 and B, A = |λ i |≤τ λ i u T i Au i + |λ i |&gt;τ λ i u T i Au i = A, B = 0.</formula><p xml:id="_bvsXkrM">Therefore by Lemma 25,</p><formula xml:id="formula_209" coords="45,158.51,472.51,298.96,21.00">E q(x T Ax)x T Bx L A op log d B F + L B op log d.</formula><p xml:id="_5se6Cqz">There are at most O(τ −2 ) indices i satisfying |λ i | &gt; τ , and thus</p><formula xml:id="formula_210" coords="45,215.12,528.92,181.76,28.18">|λ i |&gt;τ |λ i | τ −2 • |λ i |&gt;τ |λ i | 2 ≤ τ −1 .</formula><p xml:id="_PJZgadh">We thus compute that</p><formula xml:id="formula_211" coords="45,186.95,587.27,247.04,130.72">B 2 F |λ i |≤τ λ 2 i + 1 d λ i &gt;τ λ i 2 + |λ i |&gt;τ λ i u T i Au i 2 |λ i |≤τ λ 2 i + A 2 op |λ i |&gt;τ λ i 2 1 + τ −2 A 2 op 1.</formula><p xml:id="_PhHPKDQ">and</p><formula xml:id="formula_212" coords="46,208.03,103.44,200.89,54.75">B op ≤ τ + 1 d + A op |λ i |&gt;τ λ i u T i Au i τ + A 2 op τ −1 .</formula><p xml:id="_pFNPzwR">Next, since E q(x T Ax) − E z [q(x)] L A op , Lemma 26 yields</p><formula xml:id="formula_213" coords="46,95.90,202.81,415.22,98.12">E   q(x T Ax) • |λ i |&gt;τ λ i u i u T i − 1 d I   ≤ |λ i |&gt;τ |λ i | E q(x T Ax)(u T i x) 2 − E q(x T Ax) |λ i |&gt;τ |λ i | • L A 1/3 op log 2/3 d Lτ −1 A 1/3 op log 2/3 d.</formula><p xml:id="_BbphG4y">Finally,</p><formula xml:id="formula_214" coords="46,111.76,341.35,392.48,41.43">E   q(x T Ax) • 1 A 2 F |λ i |&gt;τ λ i u T i Au i • x T Ax   L |λ i |&gt;τ λ i u T i Au i L A op τ −1 .</formula><p xml:id="_YHcEzxd">Altogether,</p><formula xml:id="formula_215" coords="46,75.99,427.04,488.71,16.29">E q(x T Ax)x T Bx L log d A 1/2 op + τ + A 2 op τ −1 + τ −1 A 1/3 op + A op τ −1 L A 1/6 op log d.</formula><p xml:id="_JRkTnxc">where we set τ = A 1/6 op .</p><p xml:id="_NPNZqcn">Finally, we use the fact that x T Ax is approximately Gaussian to show that E[q(x T Ax)x T Ax] ≈ c 1 .</p><p xml:id="_NcYrudm">Lemma 28. Let x T Ax be a spherical harmonic. Then</p><formula xml:id="formula_216" coords="46,72.00,548.25,468.00,125.51">E[q(x T Ax)x T Ax] − c 1 L A op log d Proof. Define H(z) = q(z)z. For truncation radius R, define H(z) = H(max(−R, min(R, z))). For x, y ∈ [−R, R] we can bound |H(x) − H(y)| = |q(x)x − q(y)y| ≤ |x||q(x) − q(y)| + |q(y)| x − y RL x − y .</formula><p xml:id="_badUNFt">Thus H has Lipschitz constant O(RL). Since W 1 (x T Ax, N (0, 1))</p><p xml:id="_SN3sHFU">A op , we have</p><formula xml:id="formula_217" coords="46,200.36,704.58,215.26,21.86">E H(x T Ax) − E z∼N (0,1) H(z) RL A op .</formula><p xml:id="_uyPWMYJ">Furthermore, choosing R = C log d for appropriate constant C, we have that</p><formula xml:id="formula_218" coords="47,135.66,99.50,343.47,56.17">E x [H(x T Ax) − H(x T Ax)] ≤ P( x T Ax &gt; R) • E[H(x T Ax) 2 ] 1/2 L d E z [H(z) − H(z)] ≤ P(|z| &gt; R) • E[H(z) 2 ] 1/2 L d .</formula><p xml:id="_GpZdm3z">Altogether,</p><formula xml:id="formula_219" coords="47,72.00,189.52,467.50,55.96">E x [H(x T Ax)] − E z∼N (0,1) [H(z)] L A op log d. Substituting H(x T Ax) = q(x T Ax)x T Ax and E z∼N (0,1) [H(z)] = E z [q(z)z] = E z [q (z)] = c 1 yields the desired bound.</formula><p xml:id="_ShCwCRz">We are now set to prove Lemma 20.</p><p xml:id="_dR4tDTw">Proof of Lemma 20.</p><formula xml:id="formula_220" coords="47,172.96,286.37,307.96,22.52">Let P 2 f * = x T T 2 x. Then P 2 f * − c 1 x T Ax L 2 T 2 − c 1 A F .</formula><p xml:id="_dfr9WHZ">Write T 2 = αA + A ⊥ , where A ⊥ , A = 0. We first have that <ref type="bibr" coords="47,115.51,363.93,54.46,14.41">Lemma 27,</ref><ref type="bibr" coords=""></ref> we have</p><formula xml:id="formula_221" coords="47,72.00,335.82,403.79,42.52">E f * (x)x T A ⊥ x = E x T T 2 x • x T A ⊥ x = 2χ 2 T 2 , A ⊥ = 2χ 2 A ⊥ 2 F . Also, by</formula><formula xml:id="formula_222" coords="47,72.00,390.13,396.59,137.62">E f * (x)x T A ⊥ x = E q(x T Ax)x T A ⊥ x A ⊥ F • L A 1/6 op log d. Therefore A ⊥ F L A 1/6 op log d. Next, see that E f * (x)x T Ax = 2αχ 2 A 2 F = α, so by Lemma 28 we have |c 1 − α| L A op log d. Altogether, T 2 − c 1 A F ≤ |α − c 1 | A F + A ⊥ F L A 1/6</formula><p xml:id="_GT6NHXd">op log d = Lκ 1/6 d −1/12 log d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_9pJ64hn">E.3 Improved Error Floor for Polynomials</head><p xml:id="_5GqpGU5">When q is a polynomial of degree p = O(1), we can improve the exponent of d in the error floor.</p><p xml:id="_rRKWMt5">Theorem 7. Assume that q is a degree p polynomial, where p = O(1). Under Assumption 5, Assumption 6, and Assumption 7, with high probability Algorithm 1 satisfies the population loss bound</p><formula xml:id="formula_223" coords="47,153.94,678.43,293.62,29.41">E x f (x; θ) − f * (x) 2 Õ d 4 min(n, m 1 , m 2 ) + 1 √ n + κ 2 d</formula><p xml:id="_RdCCSJx">The high level strategy to prove Theorem 7 is similar to that for Theorem 3, as we aim to show Kf * is approximately proportional to x T Ax. Rather to passing to universality as in Lemma 20, however, we use an algebraic argument to estimate P 2 f * .</p><p xml:id="_sENHaER">The key algebraic lemma is the following:</p><formula xml:id="formula_224" coords="48,72.00,145.14,339.38,63.96">Lemma 29. Let Tr(A) = 0. Then A ⊗k (I ⊗k−1 ) = k s=1 d k,s A s • A ⊗k−s (I ⊗(k−s) ),</formula><p xml:id="_J6A4YDx">where the constants d k,s are defined by</p><formula xml:id="formula_225" coords="48,219.17,245.99,172.46,37.03">d k,s := 2 s−1 (2k − 2s − 1)!!(k − 1)! (2k − 1)!!(k − s)!</formula><p xml:id="_HbwAKRX">and we denote (−1)!! = 1.</p><p xml:id="_kfCeSpH">Proof. The proof proceeds via a counting argument. We first have that</p><formula xml:id="formula_226" coords="48,167.76,341.62,272.27,29.32">A ⊗k (I ⊗k−1 ) = (α 1 ,...,α k−1 )∈[d] k−1 A ⊗k α 1 ,α 1 ,...,α k−1 ,α k−1 ,i,j</formula><p xml:id="_Ezz6pf5">.</p><p xml:id="_9xP6Dmg">Consider any permutation σ ∈ S 2k . We can map this permutation to the graph G(σ) on k vertices and k − 1 edges as follows: for m ∈ [k − 1], if σ −1 (2m − 1) ∈ {2a − 1, 2a} and σ −1 (2m) ∈ {2b − 1, 2b}, then we draw an edge e(m) between a and b. In the resulting graph G(σ) each node has degree at most 2, and hence there are either two vertices with degree 1 or one vertex with degree 0. For a vertex v, let e 1 (v), e 2 (v) ∈ [k − 1] be the two edges v is incident to if v has degree 2, and otherwise e 1 (v) be the only edge v is incident to. For shorthand, let (i 1 , . . . , i 2k ) = (α 1 , α 1 , . . . , α k−1 , α k−1 , i, j).</p><p xml:id="_VwDCcNp">If there are two vertices (u 1 , u 2 ) with degree 1, we have that</p><formula xml:id="formula_227" coords="48,164.24,520.18,288.00,26.75">A ⊗k i σ(1) ,i σ(2) ,•••i σ(2k) = A i,α e 1 (u 1 ) A j,α e 2 (u 2 ) v =u 1 ,u 2 A α e 1 (v) ,α e 2 (v)</formula><p xml:id="_ceCA79t">Let u 1 , u 2 be connected to eachother via a path of s total vertices, and let P be the ordered set of vertices in this path. Via the matrix multiplication formula, one sees that</p><formula xml:id="formula_228" coords="48,142.63,600.69,326.73,27.93">(α 1 ,...,α k−1 )∈[d] k−1 A ⊗k i σ(1) ,i σ(2) ,•••i σ(2k) = (A s ) i,j v / ∈P A α e 1 (v) ,α e 2 (v) ,</formula><p xml:id="_yruxyue">where the sum is over the k − s α's that are still remaining in {e i (v) : v / ∈ P} Likewise, if there is one vertex u 1 with degree 0, we have</p><formula xml:id="formula_229" coords="48,201.82,690.79,149.84,26.75">A ⊗k i σ(1) ,i σ(2) ,•••i σ(2k) = A i,j v =u 1</formula><p xml:id="_5trFhjB">A α e 1 (v) ,α e 2 (v) .</p><p xml:id="_k9E9ffN">and thus, since</p><formula xml:id="formula_230" coords="49,134.35,74.69,279.14,55.25">P = {u 1 } (α 1 ,...,α k−1 )∈[d] k−1 A ⊗k i σ(1) ,i σ(2) ,•••i σ(2k) = A i,j (α 1 ,...,α k−1 ) v / ∈P</formula><p xml:id="_bXeQYBt">A α e 1 (v) ,α e 2 (v) .</p><p xml:id="_3S6gsza">Altogether, we have that</p><formula xml:id="formula_231" coords="49,185.74,166.86,239.52,32.83">A ⊗k (I ⊗k−1 ) = 1 (2k)! σ∈S 2k A s v / ∈P A α e 1 (v) ,α e 2 (v)</formula><p xml:id="_vqHH9Ph">where s, P are defined based on the graph G(σ). Consider a graph with fixed path P, and let S P be the set of permutations which give rise to the path P. We have that</p><formula xml:id="formula_232" coords="49,175.48,269.62,261.04,32.77">A ⊗k (I ⊗k−1 ) = 1 (2k)! P A s σ∈S P v / ∈P A α e 1 (v) ,α e 2 (v) .</formula><p xml:id="_z8hdxP3">There are (k − 1) • • • (k − s + 1) choices for the k edges to use in the path, and at each vertex v there are two choices for which edge should correspond to 2v or 2v + 1. Additionally, there are 2 ways to orient each edge. Furthermore, there are k! (k−s)! ways to choose the ordering of the path. Altogether, there are</p><formula xml:id="formula_233" coords="49,133.36,360.81,383.42,66.88">2 2s−1 (k−1)! (k−s)! k! (k−s)! ways to construct a path of length s. We can thus write A ⊗k (I ⊗k−1 ) = 1 (2k)! s A s 2 2s−1 (k − 1)! (k − s)! k! (k − s)! v / ∈P A α e 1 (v) ,α e 2 (v) ,</formula><p xml:id="_93kwAT7">where this latter sum is over all permutations where the mapping corresponding to vertices not on the path have not been decided, along with the sum over the unused α's. Reindexing, this latter sum is (letting</p><formula xml:id="formula_234" coords="49,72.00,466.02,410.77,231.95">(i 1 , . . . , i 2k−2s ) = (α 1 , α 1 , . . . , α k−s , α k−s )) σ∈S 2k−2s (α 1 ,...,α k−s )∈[d] k−s A i σ(2j−1) ,i σ(2j) = (2k − 2s)!A ⊗k−s (I ⊗k−s ) Altogether, we obtain A ⊗k (I ⊗k−1 ) = s≥1 (2k − 2s)! (2k)! (k − 1)! (k − s)! k! (k − s)! 2 2s−1 • A s • A ⊗k−s (I ⊗k−s ) = s≥1 k! (2k)! (2k − 2s)! (k − s)! (k − 1)! (k − s)! • A s • A ⊗k−s (I ⊗k−s ) = s≥1 (2k − 2s − 1)!! (2k − 1)!!2 s (k − 1)! (k − s)! 2 2s−1 • A s • A ⊗k−s (I ⊗k−s ) = s≥1 d k,s • A s • A ⊗k−s (I ⊗k−s ),</formula><p xml:id="_xKhNdHj">as desired.</p><p xml:id="_wwjT3j5">Definition 8. Define the operator T : R d×d → R d×d by T</p><formula xml:id="formula_235" coords="50,72.00,72.91,394.27,44.19">(M ) = M − Tr(M ) • I d . Lemma 30. Let P 2 f * (x) = x T T 2 x. Then T 2 − E x (g * ) (x T Ax) • A F κ √ d .</formula><p xml:id="_mUVkR6A">Proof. Throughout, we treat p = O(1) and thus functions of p independent of d as O(1) quantities. Let q be of the form g * (z) = p k=0 α k z k . We then have</p><formula xml:id="formula_236" coords="50,72.00,164.91,297.64,62.27">f * (x) = p k=0 α k A ⊗k (x ⊗2k ) Therefore P 2 f * (x) = x T T 2 x,<label>where</label></formula><formula xml:id="formula_237" coords="50,191.86,238.94,228.29,36.52">T 2 := p k=0 α k (2k − 1)!!k χ k+1 χ 2 T A ⊗k (I ⊗k−1 ) .</formula><p xml:id="_AxqFT86">Applying Lemma 29, one has</p><formula xml:id="formula_238" coords="50,158.41,310.95,295.18,36.52">T 2 = p s=0 T (A s ) • p k=s α k (2k − 1)!!k χ k+1 χ 2 d k,s A ⊗k−s (I ⊗(k−s) ).</formula><p xml:id="_bAVhV97">Define</p><formula xml:id="formula_239" coords="50,190.19,368.54,231.62,36.52">β s = p k=s α k (2k − 1)!!k χ k+1 χ 2 d k,s A ⊗k−s (I ⊗(k−s) )</formula><p xml:id="_PYks3cT">We first see that</p><formula xml:id="formula_240" coords="50,195.13,432.61,221.75,36.52">β 1 = p k=1 (2k − 3)!! • kα k χ k+1 χ 2 A ⊗k−1 (I ⊗(k−1) )</formula><p xml:id="_gePZKyf">Next, see that</p><formula xml:id="formula_241" coords="50,175.05,505.33,263.10,35.93">χ k+1 χ 2 = d(d + 2) (d + 2k)(d + 2k − 2) χ k−1 = χ k−1 + O(1/d).</formula><p xml:id="_2VAa6wm">Thus</p><formula xml:id="formula_242" coords="50,72.00,566.15,461.11,146.02">β 1 = p k=1 (2k − 3)!! • kα k χ k−1 A ⊗k−1 (I ⊗(k−1) ) + O(1/d) • p k=1 (2k − 3)!! • k|α k |A ⊗k−1 (I ⊗(k−1) ) = p k=1 kα k A ⊗k−1 E[x ⊗2k−2 ] + O(1/d) = E x (g * ) (x T Ax) + O(1/d). since A ⊗k (I ⊗k ) E x (x T Ax) k E x (x T Ax) 2 k/2 = O(1)</formula><p xml:id="_FQgfQjN">where Tr(A) = 0 implies E x (x T Ax) 2 = O(1) and we invoke spherical hypercontractivity (Lemma 35). Similarly, |β s | = O(1), and thus</p><formula xml:id="formula_243" coords="51,165.57,108.90,287.50,36.17">T 2 − E x (g * ) (x T Ax) • A F 1 d + p s=2 T (A s ) F κ √ d ,</formula><p xml:id="_XaCUNs3">where we use the inequality</p><formula xml:id="formula_244" coords="51,72.00,171.64,396.76,134.71">T (X) F ≤ X F + |Tr(X)| • 1 √ d ≤ 2 X F , along with A s F ≤ A 2 F ≤ κ √ d for s ≥ 2. Lemma 31. Let c 1 = E x (g * ) (x T Ax) . Then Kf * − λ 2 2 (σ)c 1 x T Ax L 2 κd −5/2</formula><p xml:id="_q7CvDmG">Proof. Since E[f * (x)] = 0, P 0 f * = 0. Next, since f * is an even function, P k f * = 0 for k odd. Thus</p><formula xml:id="formula_245" coords="51,238.75,348.37,141.14,21.86">Kf * − λ 2 2 (σ)P 2 f * L 2 d −4 .</formula><p xml:id="_72F7sZS">Additionally, by Lemma 30 we have that</p><formula xml:id="formula_246" coords="51,72.00,388.42,402.22,270.04">P 2 f * − c 1 x T Ax L 2 T 2 − c 1 • A F κ √ d . Since λ 2 2 (σ) = Θ(d −2 ), we have Kf * − λ 2 2 (σ)c 1 x T Ax L 2 κd −5/2 . Corollary 4. Assume κ = o( √ d). Then x T Ax − Kf * −1 L 2 Kf * L 2 κ/ √ d Proof. x T Ax − Kf * −1 L 2 Kf * L 2 = Kf * −1 L 2 x T Ax Kf * L 2 − Kf * L 2 ≤ Kf * −1 L 2 Kf * − λ 2 2 (σ)c 1 x T Ax L 2 + Kf * −1 L 2 Kf * − λ 2 2 (σ)|c 1 | κd −5/2 Kf * −1 L 2 κ/ √ d.</formula><p xml:id="_gXCrj44">The proof of Theorem 7 follows directly from Corollary 4 in an identical manner to the proof of Theorem 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_V38JJ36">F Preliminaries on spherical harmonics</head><p xml:id="_ZsXVv4z">In this section we restrict to X d = S d−1 ( √ d), the sphere of radius √ d, and ν the uniform distribution on X d .</p><p xml:id="_ybX2XNP">The moments of ν are given by the following <ref type="bibr" coords="52,291.27,137.04,18.59,14.41" target="#b16">[17]</ref>:</p><formula xml:id="formula_247" coords="52,72.00,158.71,307.71,103.43">Lemma 32. Let x ∼ ν. Then E x x ⊗2k = χ k • (2k − 1)!!I ⊗k where χ k := k−1 j=0 d d + 2j = Θ(1).</formula><p xml:id="_UwDavCR">For integer ≥ 0, let V d, be the space of homogeneous harmonic polynomials on R d of degree restricted to X d . One has that V d, form an orthogonal decomposition of L 2 (ν) <ref type="bibr" coords="52,448.04,293.23,18.32,14.41" target="#b25">[26]</ref>, i.e</p><formula xml:id="formula_248" coords="52,265.18,316.21,77.68,36.04">L 2 (ν) = ∞ =0 V d,</formula><p xml:id="_MFJatnY">Homogeneous polynomials of degree can be written as T (x ⊗ ) for an -tensor T ∈ (R d ) ⊗ . The following lemma characterizes V d, :</p><p xml:id="_zDN9kXp">Lemma 33. T (x ⊗ ) ∈ V d, if and only if T (I) = 0.</p><p xml:id="_ahKajaN">Proof. By definition, a degree l homogeneous polynomial p(x) ∈ V d,l if and only if ∆p(x) = 0 for all x ∈ S d−1 . Note that ∇ 2 p(x) = l(l − 1)T (x ⊗(l−2) ) so this is satisfied if and only if</p><formula xml:id="formula_249" coords="52,174.56,469.07,262.87,21.86">0 = tr T (x ⊗(l−2) ) = T (x ⊗(l−2) ⊗ I) = T (I), x ⊗(l−2) .</formula><p xml:id="_vEr7tvj">As this must hold for all x, this holds if and only if T (I) = 0.</p><p xml:id="_B66svkC">From the above characterization, we see that dim(V d,k ) = B(d, k), where</p><formula xml:id="formula_250" coords="52,105.66,542.79,400.69,37.55">B(d, k) = 2k + d − 2 k k + d − 3 k − 1 = (k + d − 3)!(2k + d − 2) k!(d − 2)! = (1 + o d (1)) d k k! .</formula><p xml:id="_ycSacpH">Define P : L 2 (ν) → L 2 (ν) to be the orthogonal projection onto V d, . The action of P 0 , P 1 , P 2 on a homogeneous polynomial is given by the following lemma:</p><p xml:id="_Bgq6fZZ">Lemma 34. Let T ∈ (R d ) ⊗2k be a symmetric 2k tensor, and let p(x) = T (x ⊗2k ) be a polynomial. Then:</p><formula xml:id="formula_251" coords="52,161.74,661.15,277.88,65.01">P 0 p = χ k (2k − 1)!!T (I ⊗k ) P 1 p = 0 P 2 p = k(2k − 1)!!χ k+1 χ 2 T (I ⊗k−1 ) − T (I ⊗k ) • I d , xx T</formula><p xml:id="_VUrmGPV">Proof. First, we see</p><formula xml:id="formula_252" coords="53,202.36,99.98,207.28,21.86">P 0 p = E T (x ⊗2k ) = χ k (2k − 1)!!T (I ⊗k ).</formula><p xml:id="_rmHZg6z">Next, since p is even, P 1 p = 0. Next, let P 2 p = x T T 2 x. For symmetric B so that Tr(B) = 0, we have that</p><formula xml:id="formula_253" coords="53,213.32,167.22,185.36,13.72">E T (x ⊗2k )x T Bx = E x T T 2 xx T Bx .</formula><p xml:id="_Bqc9rY5">The LHS is</p><formula xml:id="formula_254" coords="53,139.33,220.03,333.33,75.92">E T (x ⊗2k )x T Bx = (2k + 1)!!χ k+1 (T ⊗B)I ⊗k+1 = (2k + 1)!!χ k+1 2k 2k + 1 T (I ⊗k−1 ), B = 2k • (2k − 1)!χ k+1 T (I ⊗k−1 ) − T (I ⊗k ) • I d , B ,</formula><p xml:id="_dM2uRma">where the last step is true since Tr(B) = 0. The RHS is</p><formula xml:id="formula_255" coords="53,213.49,330.61,185.03,31.15">E x T T 2 xx T Bx = 3!!χ 2 (T 2 ⊗B)(I ⊗2 ) = 2χ 2 T 2 , B .</formula><p xml:id="_MPRQbNm">Since these two quantities must be equal for all B with Tr(B) = 0, and Tr(T 2 ) = 0, we see that</p><formula xml:id="formula_256" coords="53,184.94,400.75,242.12,28.89">T 2 = k(2k − 1)!!χ k+1 χ 2 T (I ⊗k−1 ) − T (I ⊗k ) • I d ,</formula><p xml:id="_hBHZ7PK">as desired.</p><p xml:id="_pv2S34Z">Polynomials over the sphere verify hypercontractivity:</p><p xml:id="_amYtUeQ">Lemma 35 (Spherical hypercontractivity <ref type="bibr" coords="53,269.32,510.50,18.93,14.41" target="#b10">[11,</ref><ref type="bibr" coords="53,290.69,510.50,13.75,14.41" target="#b36">37]</ref>). Let f be a degree p polynomial. Then for q ≥ 2 f L q (ν) ≤ (q − 1) p/<ref type="foot" coords="53,331.80,537.57,4.24,6.99" target="#foot_0">2</ref> f L 2 (ν) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_BsgawCN">F.1 Gegenbauer Polynomials</head><p xml:id="_XuMYXta">For an integer d &gt; 1, let µ d be the density of x • e 1 , where x ∼ Unif(S d−1 (1)) and e 1 is a fixed unit vector. One can verify that µ d is supported on [−1, 1] and given by</p><formula xml:id="formula_257" coords="53,219.64,638.76,172.72,30.85">dµ d (x) = Γ(d/2) √ πΓ( d−1 2 ) (1 − x 2 ) d−3 2 dx</formula><p xml:id="_w4DjUnQ">where Γ(n) is the Gamma function. For convenience, we let</p><formula xml:id="formula_258" coords="53,371.40,681.55,127.36,20.67">Z d := Γ(d/2) √ πΓ( d−1 2 ) = 1 β( 1 2 , d−1</formula><p xml:id="_JDwUP4b">The Gegenbauer polynomials G</p><formula xml:id="formula_259" coords="54,232.40,74.19,43.57,21.37">(d) k k∈Z ≥0</formula><p xml:id="_P5RGgTU">are a sequence of orthogonal polynomials with respect to the density µ d , defined as</p><formula xml:id="formula_260" coords="54,208.42,96.05,145.35,16.03">G (d) 0 (x) = 1, G (d) 1 (x) = x,<label>and</label></formula><formula xml:id="formula_261" coords="54,173.95,119.33,366.05,37.03">G (d) k (x) = d + 2k − 4 d + k − 3 xG (d) k−1 (x) − k − 1 d + k − 3 G (d) k−2 (x).<label>(22)</label></formula><p xml:id="_XdUpCQB">By construction, G</p><p xml:id="_kvmTN9w">k is a polynomial of degree k. The G</p><p xml:id="_Mmds7V6">k are orthogonal in that</p><formula xml:id="formula_264" coords="54,207.17,182.89,197.17,16.43">E x∼µ d G (d) k (x)G (d) j (x) = δ j=k B(d, k) −1</formula><p xml:id="_NS7KU8g">For a function f ∈ L 2 (µ d ), we can write its Gegenbauer decomposition as</p><formula xml:id="formula_265" coords="54,205.02,240.48,201.95,36.04">f (x) = ∞ k=0 B(d, k) f, G (d) k L 2 (µ d ) G (d) k (x),</formula><p xml:id="_xvQ52Hq">where convergence is in L 2 (µ d ). For an integer k, we define the operator P</p><formula xml:id="formula_266" coords="54,430.58,286.40,104.37,23.03">(d) k : L 2 (µ d ) → L 2 (µ d</formula><p xml:id="_8HGSGEg">) to be the projection onto the degree k Gegenbauer polynomial, i.e</p><formula xml:id="formula_267" coords="54,220.10,325.22,171.79,16.43">P (d) k f = B(d, k) f, G (d) k L 2 (µ d ) G (d) k .</formula><p xml:id="_jwrm2vM">We also define the operators P A key property of the Gegebauer coefficients is that they allow us to express the kernel operator K in closed form <ref type="bibr" coords="54,144.05,475.57,18.93,14.41" target="#b25">[26,</ref><ref type="bibr" coords="54,165.97,475.57,15.94,14.41" target="#b37">38]</ref> Lemma 36. For a function g ∈ L 2 (ν), the operator K acts as</p><formula xml:id="formula_268" coords="54,255.91,523.14,100.19,26.03">Kg = k≥0 λ 2 k (σ)P k g,</formula><p xml:id="_jJWyjAA">One key fact about Gegenbauer polynomials is the following derivative formula:</p><p xml:id="_4pRmbxb">Lemma 37 (Derivative Formula).</p><formula xml:id="formula_269" coords="54,223.45,608.43,166.30,37.03">d dx G (d) k = k(k + d − 2) d − 1 G (d+2) k−1 (x).</formula><p xml:id="_psapMW5">Furthermore, the following is a corollary of eq. ( <ref type="formula" coords="54,304.79,649.01,9.30,14.41" target="#formula_261">22</ref>): .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_v3bnr33">G Proofs for Section 5</head><p xml:id="_MKTR7Vu">The proof of Theorem 4 relies on the following lemma, which gives the Gegenbauer decomposition of the ReLU function:</p><p xml:id="_UZjgkx3">Lemma 38 (ReLU Gegenbauer). Let ReLU(x) = max(x, 0). Then The proof of this lemma is deferred to Appendix G.1.</p><p xml:id="_rcFGFSV">We also require a key result from <ref type="bibr" coords="55,239.09,334.54,18.32,14.41" target="#b17">[18]</ref>, which lower bounds the approximation error of an inner product function. .</p><p xml:id="_kpeJs3v">Lemma 39. [18, Theorem 3] Let f be an inner product function and g 1 , . . . , g r be separable functions. Then, for any k ≥ 1,</p><formula xml:id="formula_270" coords="55,136.19,543.85,346.26,41.31">f − r i=1 g i 2 L 2 (ν d ) ≥ P k f L 2 (µ d ) • P k f L 2 (µ d ) − 2 r i=1 g r L 2 (ν d ) B(d, k) 1/2 .</formula><p xml:id="_rhbvxGz">We now can prove Theorem 4</p><p xml:id="_ZkWpwbv">Proof of Theorem 4. We begin with the lower bound. Let x = U x 1 x 2 , where x 1 , x 2 ∈ R d/<ref type="foot" coords="55,532.24,636.27,4.28,6.99" target="#foot_1">2</ref> .</p><p xml:id="_QAjXBWa">Assume that there exists some θ such that f * − N θ L 2 (X ) ≤ . Then where r is the random variable defined as r = x 1 2 and µ is the associated measure. The equality comes from the fact that conditioned on r, x 1 and x 2 are independent and distributed uniformly on the spheres of radii Next, see that when x 1 2 = r, x 2 2 = d − r, we have that</p><formula xml:id="formula_271" coords="56,197.16,292.63,217.69,29.50">x T Ax = 2 √ d x 1 , x 2 = 2 r(d − r) d x 1 , x 2 ,</formula><p xml:id="_pSwRe5B">where now x 1 , x 2 ∼ S d/2−1 (1) i.i.d. Defining q(z) = ReLU 2 r(d−r) d z − c 0 , we thus have q( x 1 , x 2 ) = ReLU(x T Ax) − c 0 = f * (x). We aim to invoke Lemma 39. We note that (x 1 , x 2 ) ∼ νd/2 , and that q is an inner product function. Define g i (x) = a i σ(w T i x + b 1,i ). We see that g i is a separable function, and also that</p><formula xml:id="formula_272" coords="56,247.53,574.42,116.94,35.77">N θ (x) = m i=1 g i (x) + b 2 .</formula><p xml:id="_PSYjcpJ">Hence N θ ) is the sum of m + 1 separable functions. We can bound the a single function as  </p><formula xml:id="formula_273" coords="56,213.96,644.48,182.87,63.43">|g i (x)| ≤ |a i |C σ 1 + w T i x + b 1,i ασ ≤ C σ B(1 + √ d w i ∞ + B) ασ ≤ C σ B(1 + Bd</formula><formula xml:id="formula_274" coords="57,267.75,662.56,94.92,60.72">≥ √ 3d • 1 512m 2 d ≥ 1 16m</formula><p xml:id="_V7p3kkC">.</p><p xml:id="_JHuhZR8">We thus have, for any integer k &lt; d/8, 2 /2 ≥ E x 1 ,x 2 (q( x 1 , x 2 ) − N θ (x)) 2</p><p xml:id="_cpZkGFR">≥ P ≥2k q L 2 • P ≥2k q L 2 − 2(m + 1)(Bd 3/2 ) ασ+1 B d/2,2k</p><p xml:id="_AyWXUnS">Choose ≤ 1 512k 2 ; we then must have 2(m + 1)(Bd for less than a universal constant c 3 .</p><p xml:id="_zVQSnk6">We next show the upper bound, It is easy to see that Assumptions 1 and 3 are satisfied. Next, since the verification of Assumptions 2 and 4 only required Lipschitzness, those assumptions are satisfied as well with , χ = 1. Finally, we have</p><formula xml:id="formula_275" coords="58,138.97,558.91,334.05,27.73">E x f * (x) 2 ≤ E x ReLU 2 (x T Ax) = 1 2 E x (x T Ax) 2 = d d + 2 &lt; 1.</formula><p xml:id="_UZyHJv7">Next, observe that Kf * Next, eq. ( <ref type="formula" coords="61,123.28,139.00,9.96,14.41">23</ref>) gives</p><formula xml:id="formula_276" coords="61,208.59,164.62,193.62,68.46">A (d) 2 = Z d • −1 (d − 1) 2 + 2d (d − 1) 2 • Z d d + 1 = Z d (d − 1)(d + 1)</formula><p xml:id="_aA8KGMs">.</p><p xml:id="_ExhpBk2">Finally, eq. ( <ref type="formula" coords="61,133.32,234.66,9.96,14.41">24</ref>) gives</p><formula xml:id="formula_277" coords="61,233.37,261.38,137.75,97.05">B (d) 3 = d + 2 d A (d) 2 − 2 d B (d) 1 = Z d d − 1 d + 2 d(d + 1) − 2 d = − Z d (d − 1)(d + 1)</formula><p xml:id="_axRHd6v">.</p><p xml:id="_5aTnnF3">Therefore the base case is proven for k = 0, 1.</p><p xml:id="_4BPRzZt">Now, assume that the claim is true for some k ≥ 1 for all d. We first have .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="12,72.00,116.25,384.26,14.27;12,457.53,117.51,4.23,6.99;12,462.27,119.13,72.61,10.48;12,534.88,117.51,4.93,6.99;12,541.49,119.13,29.28,10.48;12,72.00,140.55,5.04,10.48;12,77.04,145.06,4.23,6.99;12,81.77,137.67,57.65,14.27;12,147.53,138.56,4.23,6.99;12,143.94,141.71,7.06,6.99;12,151.00,148.51,4.36,6.99;12,156.55,140.55,7.95,10.48;12,182.86,133.22,5.85,10.48;12,204.99,133.22,5.16,10.48;12,210.15,137.81,12.83,6.99;12,176.54,147.67,5.16,10.48;12,181.70,152.25,12.83,6.99;12,211.31,147.67,5.85,10.48;12,234.27,140.55,7.95,10.48;12,243.47,138.93,4.93,6.99;12,252.34,137.67,168.55,14.27;12,420.90,145.06,4.23,6.99;12,428.95,140.55,35.91,11.51;12,472.86,140.55,41.11,10.48;12,513.97,138.93,4.93,6.99;12,520.57,137.67,27.95,14.27;12,72.00,159.09,217.95,14.27;12,289.95,166.48,4.23,6.99;12,294.69,161.97,13.62,10.48;12,308.30,166.48,4.23,6.99;12,313.04,161.97,13.62,10.48;12,326.66,166.48,4.23,6.99;12,331.39,161.97,10.29,10.48;12,341.67,166.48,4.23,6.99;12,346.40,159.09,113.87,14.27;12,460.27,166.48,4.84,6.99;12,465.90,161.97,12.69,10.48;12,478.59,166.48,4.84,6.99;12,484.21,159.09,55.79,14.27;12,72.00,173.53,44.82,14.27;12,116.82,180.93,4.23,6.99;12,127.27,175.31,9.30,20.74;12,152.73,175.31,9.30,20.74;12,167.75,176.41,8.37,10.48;12,176.12,180.93,4.23,6.99;12,180.85,176.41,6.08,10.48;12,186.94,174.80,10.82,6.99;12,198.26,173.53,167.05,14.27;12,365.30,180.93,3.95,6.99;12,369.99,173.53,165.28,14.27;12,535.27,174.80,4.23,6.99;12,72.00,187.98,57.64,14.27"><head>Theorem 4 .</head><label>4</label><figDesc xml:id="_usRHGfC">Let d be a suffiently large even integer. Consider the target function f * (x) = ReLU(x T Ax)− c 0 , where A = 1 √ d U 0 I d/2 I d/2 0 U T for some orthogonal matrix U and c 0 = E x∼ν ReLU(x T Ax) . Under Assumption 8, there exist constants C 1 , C 2 , C 3 , c 3 , depending only on (C σ , α σ ), such that for any c 3 ≥ ≥ C 3 d −2 , any two layer neural network N θ (x) of width m and population L 2 error bound</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="19,72.00,345.90,468.00,14.41;19,72.00,360.34,468.00,14.41;19,72.00,374.79,468.00,14.41;19,72.00,389.23,86.82,14.41;19,158.82,390.50,8.47,6.99;19,171.23,389.23,368.76,14.41;19,72.00,403.68,210.75,14.41;19,285.22,405.46,3.32,20.74;19,291.00,403.68,210.09,14.41;19,501.09,404.94,4.93,6.99;19,507.70,403.68,32.30,14.41;19,72.00,418.13,468.00,14.41;19,72.00,432.57,406.68,14.41;19,331.38,249.92,84.75,64.97"><head>Figure 1 :</head><label>1</label><figDesc xml:id="_wjdNwbW">Figure 1: We ran Algorithm 1 on both the single index and quadratic feature settings described in Section 4. Each trial was run with 5 random seeds. The solid lines represent the medians and the shaded areas represent the min and max values. For every trial we recorded both the test loss on a test set of size 2 15 and the linear correlation between the learned feature map φ(x) and the true intermediate feature h (x) where h (x) = x • β for the single index setting and h (x) = x T Ax for the quadratic feature setting. Our results show that the test loss goes to 0 as the linear correlation between the learned feature map φ and the true intermediate feature h approaches 1.</figDesc><graphic coords="19,331.38,249.92,84.75,64.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="21,72.00,128.69,82.66,14.27;21,159.53,130.47,9.30,20.74;21,173.69,128.69,116.58,14.27;21,295.14,130.47,9.30,20.74;21,309.31,128.69,230.69,14.27;21,72.00,144.91,24.38,20.74;21,98.37,146.01,11.83,10.48;21,112.86,144.91,9.30,20.74;21,124.82,143.13,106.96,14.27;21,234.76,144.91,25.92,20.74;21,264.00,146.01,8.84,10.48;21,179.16,175.03,18.44,8.89;21,198.10,169.53,169.50,14.27;21,353.55,184.99,10.47,6.99;21,369.59,171.31,23.36,20.74;21,394.95,172.41,12.85,10.48;21,423.74,172.41,9.10,10.48"><head>Lemma 1 .</head><label>1</label><figDesc xml:id="_b7mGC4K">Let a ∼ Unif({−1, 1}) and let b ∼ N (0, 1). Then there exists v(a, b) supported on {−1, 1} × [0, 2] such that for any |x| ≤ 1, E a,b [v(a, b)σ(ax + b)] = 1 and sup a,b |v(a, b)| 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="21,72.00,355.46,82.66,14.27;21,159.53,357.24,9.30,20.74;21,173.69,355.46,116.58,14.27;21,295.14,357.24,9.30,20.74;21,309.31,355.46,230.69,14.27;21,72.00,371.69,24.38,20.74;21,98.37,372.79,11.83,10.48;21,112.86,371.69,9.30,20.74;21,124.82,369.91,106.96,14.27;21,234.76,371.69,25.92,20.74;21,264.00,372.79,8.84,10.48;21,178.76,401.81,18.44,8.89;21,197.70,396.31,170.30,14.27;21,353.95,411.76,10.47,6.99;21,369.99,398.09,23.36,20.74;21,395.35,399.19,12.85,10.48;21,424.14,399.19,9.10,10.48"><head>Lemma 2 .</head><label>2</label><figDesc xml:id="_tKBC6cw">Let a ∼ Unif({−1, 1}) and let b ∼ N (0, 1). Then there exists v(a, b) supported on {−1, 1} × [0, 2] such that for any |x| ≤ 1, E a,b [v(a, b)σ(ax + b)] = x and sup a,b |v(a, b)| 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="54,219.32,351.34,10.94,6.99;54,217.70,360.78,11.00,6.99;54,234.09,354.73,9.11,10.48;54,259.13,351.64,4.41,6.99;54,262.59,360.94,10.82,6.99;54,275.90,354.73,7.54,10.48;54,285.07,351.34,10.94,6.99;54,299.50,351.85,27.80,14.41;54,328.93,351.34,10.94,6.99;54,327.30,360.78,11.00,6.99;54,343.69,354.73,9.11,10.48;54,368.74,351.64,8.47,6.99;54,372.19,360.94,11.00,6.99;54,385.89,354.73,7.54,10.48;54,395.06,351.34,10.94,6.99;54,406.51,351.85,2.99,14.41;54,72.00,375.88,115.76,14.41;54,188.66,377.15,15.18,6.99;54,204.33,378.77,4.55,10.48;54,208.88,367.48,9.96,20.74;54,218.85,375.88,151.46,14.41;54,373.52,377.67,3.32,20.74;54,380.05,378.77,5.43,10.48;54,385.48,383.28,4.23,6.99;54,390.21,375.88,46.52,14.41;54,441.44,377.67,9.30,20.74;54,455.45,375.88,84.55,14.41;54,72.00,393.21,6.65,10.48;54,82.40,392.11,7.97,20.74;54,93.69,393.21,7.96,10.48;54,101.66,391.59,4.23,6.99;54,106.39,393.21,11.59,10.48;54,117.99,397.73,4.36,6.99;54,122.84,390.33,202.28,14.41;54,140.77,417.99,6.83,10.48;54,147.60,422.50,4.41,6.99;54,152.72,417.99,58.67,11.50;54,212.41,417.99,21.54,10.48;54,236.60,416.89,3.32,20.74;54,242.58,417.99,5.43,10.48;54,248.01,422.50,4.23,6.99;54,252.74,417.99,13.79,10.48;54,266.53,414.60,10.94,6.99;54,266.53,424.04,4.41,6.99;54,277.97,417.99,10.63,10.48;54,288.60,415.77,19.29,6.99;54,308.39,417.99,6.65,10.48;54,317.70,416.89,3.32,20.74;54,323.68,417.99,5.43,10.48;54,329.10,422.50,4.23,6.99;54,333.83,417.99,45.92,10.48;54,379.75,415.77,12.70,6.99;54,392.95,417.99,5.43,10.48;54,398.38,422.50,4.23,6.99;54,403.11,416.89,11.12,20.74;54,416.23,417.99,9.23,10.48;54,425.46,414.60,10.94,6.99;54,425.46,421.80,42.02,9.22;54,467.98,417.99,3.25,10.48;54,72.00,439.45,111.06,14.41;54,186.05,441.23,10.15,20.74;54,196.20,446.85,4.41,6.99;54,201.32,442.33,19.51,10.48;54,224.15,441.23,9.30,20.74;54,242.75,442.33,6.65,10.48;54,255.81,447.64,25.93,8.92;54,282.23,442.33,36.42,10.48;54,318.65,440.72,19.29,6.99;54,341.76,442.33,38.88,10.48;54,387.05,447.64,25.93,8.92;54,413.47,442.33,6.08,10.48;54,419.56,440.72,19.68,6.99;54,439.73,442.33,7.54,10.48"><head></head><label></label><figDesc xml:id="_q8CpcY3">that ν = Unif(S d−1 ( √ d)).Let μd be the density of x • e 1 , where x ∼ ν. For a function σ ∈ L 2 (μ d ), we define its Gegenbauer coefficients asλ k (σ) := E x∼ν [σ(x • e 1 )G (d) k (d −1/2 x • e 1 )] = σ(d 1/2 e 1 •), G (d) k L 2 (µ d ) .By Cauchy, we get that|λ k (σ)| ≤ σ L 2 (μ d ) B(d, k) −1/2 = O( σ L 2 (μ d ) d −k/2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="54,72.00,673.47,61.55,10.75;54,218.47,702.74,9.23,10.48;54,227.70,699.36,10.94,6.99;54,227.70,708.80,8.64,6.99;54,239.14,702.74,54.96,10.48;54,294.10,700.53,4.41,6.99;54,320.54,694.66,16.49,10.48;54,340.10,693.56,9.30,20.74;54,352.05,694.66,16.91,10.48;54,311.71,709.69,15.44,6.99;54,311.71,718.98,14.71,6.99;54,327.65,712.78,35.75,10.48;54,366.73,711.68,9.30,20.74;54,378.68,712.78,10.41,10.48"><head></head><label></label><figDesc xml:id="_hfVabSv">2k (0) = (−1) k (2k − 1)!! k−1 j=0 (d + 2j − 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="55,131.21,169.37,58.09,10.48;55,231.62,161.28,5.85,10.48;55,193.81,178.30,11.82,10.48;55,206.83,176.31,4.23,6.99;55,206.83,185.14,4.23,6.99;55,212.26,178.30,3.25,10.48;55,218.70,176.31,15.18,6.99;55,224.17,185.14,4.23,6.99;55,235.07,178.30,15.19,10.48;55,252.92,177.20,9.30,20.74;55,264.87,178.30,10.41,10.48;55,276.47,169.37,9.23,10.48;55,285.71,165.98,10.94,6.99;55,285.71,175.02,4.23,6.99;55,297.15,169.37,27.52,10.48;55,331.56,161.28,5.85,10.48;55,328.52,177.57,11.94,10.48;55,341.65,169.37,9.23,10.48;55,350.89,165.98,10.94,6.99;55,350.89,175.02,4.23,6.99;55,362.33,169.37,15.76,10.48;55,191.48,203.46,9.11,10.48;55,204.16,220.28,15.44,6.99;55,220.51,203.46,24.26,10.48;55,244.77,201.24,15.44,6.99;55,305.57,195.37,16.49,10.48;55,325.12,194.27,9.30,20.74;55,337.07,195.37,16.91,10.48;55,261.90,213.49,11.82,10.48;55,274.92,211.50,4.23,6.99;55,274.92,220.33,4.23,6.99;55,280.35,213.49,3.25,10.48;55,286.79,211.50,15.18,6.99;55,292.27,220.33,4.23,6.99;55,303.17,213.49,4.55,10.48;55,321.00,210.40,4.41,6.99;55,321.00,219.70,14.71,6.99;55,336.21,213.49,35.75,10.48;55,375.28,212.39,9.30,20.74;55,387.24,213.49,10.41,10.48;55,398.84,203.46,51.50,10.48;55,450.34,200.07,10.94,6.99;55,450.34,209.51,8.64,6.99;55,461.79,203.46,19.01,10.48;55,72.00,237.09,87.86,14.27;55,164.76,271.87,7.54,10.48;55,173.93,268.48,10.94,6.99;55,172.31,277.72,18.31,6.99;55,191.12,271.87,45.66,10.48;55,243.42,264.00,4.23,6.99;55,243.42,281.95,25.93,8.92;55,273.17,271.87,9.11,10.48;55,285.60,288.69,18.70,6.99;55,332.38,263.78,16.49,10.48;55,351.93,262.68,9.30,20.74;55,363.88,263.78,16.91,10.48;55,380.79,262.17,4.23,6.99;55,385.52,263.78,42.27,10.48;55,307.48,281.90,11.82,10.48;55,320.50,279.92,4.23,6.99;55,320.50,288.75,4.23,6.99;55,325.93,281.90,3.25,10.48;55,332.37,279.92,15.18,6.99;55,337.84,288.75,4.23,6.99;55,348.74,281.90,4.55,10.48;55,353.30,278.81,22.43,9.35;55,371.31,288.11,14.71,6.99;55,386.52,281.90,35.75,10.48;55,425.59,280.80,9.30,20.74;55,437.55,281.90,10.41,10.48;55,447.95,281.17,4.23,6.99"><head>2 L 2</head><label>22</label><figDesc xml:id="_cpgqaEJ">(µ d ) = k≥m (2k − 3)!! 2 B(d, 2k) β( 1 2 , d−1 2 ) 2 k j=0 (d + 2j − 1) 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="55,72.00,373.45,80.90,10.75;55,156.22,372.44,7.24,20.74;55,164.36,371.92,15.18,6.99;55,180.04,373.54,14.96,10.48;55,197.44,372.44,18.98,20.74;55,217.31,371.92,15.18,6.99;55,232.99,373.54,14.96,10.48;55,251.27,372.44,11.96,20.74;55,266.54,376.15,8.63,8.54;55,278.11,370.65,261.89,14.41;55,72.00,385.10,77.13,14.27;55,152.45,386.88,11.96,20.74;55,167.73,390.60,11.62,8.54;55,72.00,409.57,86.18,10.75;55,161.59,408.55,7.24,20.74;55,169.72,408.03,15.18,6.99;55,185.40,409.65,14.96,10.48;55,203.05,408.55,19.23,20.74;55,223.18,408.03,15.18,6.99;55,238.86,409.65,14.96,10.48;55,257.23,408.55,11.96,20.74;55,272.59,412.27,8.63,8.54;55,284.27,406.77,255.73,14.41;55,72.00,421.21,61.48,14.27;55,140.03,423.00,18.53,20.74;55,159.46,422.48,15.18,6.99;55,175.14,421.21,89.55,14.27;55,264.69,422.48,4.23,6.99;55,272.74,423.00,11.96,20.74;55,288.02,426.71,11.62,8.54;55,72.00,442.88,191.88,14.41;55,268.07,444.66,7.24,20.74;55,276.21,444.15,15.18,6.99;55,291.88,445.76,14.96,10.48;55,310.38,444.66,20.08,20.74;55,331.36,444.15,15.18,6.99;55,347.04,442.88,133.89,14.41;55,486.47,444.66,9.30,20.74;55,501.97,442.88,38.03,14.41;55,76.65,460.21,18.55,10.48;55,106.66,459.11,9.30,20.74;55,119.97,460.21,7.04,10.48;55,127.01,464.73,4.36,6.99;55,131.87,457.33,244.64,14.41;55,383.76,465.51,25.00,8.92;55,413.28,460.21,26.02,10.48;55,445.28,465.51,25.93,8.92;55,471.71,457.33,68.29,14.41;55,72.00,477.55,91.56,14.41;55,165.19,477.05,10.94,6.99;55,163.56,486.49,4.41,6.99;55,176.63,480.43,5.77,10.48;55,190.32,490.52,25.93,8.92;55,220.06,480.43,26.61,10.48;55,248.30,477.05,10.94,6.99;55,246.68,486.49,4.41,6.99;55,259.75,480.43,6.93,10.48;55,273.32,490.52,25.93,8.92"><head>Definition 9 .L 2</head><label>92</label><figDesc xml:id="_GdFRQ8q">f : S d−1 (1) × S d−1 (1) → R is an inner product function if f (x, x ) = φ( x, x ) for some φ : [−1, 1] → R. Definition 10. g : S d−1 (1) × S d−1 (1) → R is a separable function if g(x, x ) = ψ( v, x , v , x ) for some v, v ∈ S d−1 (1) and ψ : [−1, 1] 2 → R.Let νd be the uniform distribution over S d−1 (1) × S d−1 (1). We note that if (x, x ) ∼ νd , then x, x ∼ µ d . For an inner product function f , we thus have f L 2 (ν d ) = φ L 2 (µ d ) . We overload notation and let P (µ d )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="56,166.29,94.97,9.96,20.74;56,176.25,101.80,25.85,14.41;56,205.09,93.89,9.96,20.74;56,215.06,104.68,6.08,10.48;56,223.80,103.58,9.30,20.74;56,235.75,101.80,129.60,14.41;56,368.34,101.80,93.26,14.41;56,220.77,133.20,20.45,8.54;56,244.21,129.48,9.30,20.74;56,256.16,130.58,59.20,10.48;56,318.68,129.48,9.30,20.74;56,331.30,130.58,59.93,10.48;56,72.00,155.43,53.64,14.41;56,126.47,160.93,95.36,10.80;56,221.83,158.27,7.06,6.99;56,228.89,163.10,47.96,8.62;56,276.85,157.63,7.06,6.99;56,283.91,164.01,18.29,6.99;56,302.70,158.31,10.32,10.48;56,314.30,156.69,4.23,6.99;56,319.03,158.31,15.76,10.48;56,337.44,157.21,9.30,20.74;56,349.40,158.31,9.37,10.48;56,358.77,162.82,3.95,6.99;56,363.46,158.31,20.31,10.48;56,383.76,155.22,4.23,6.99;56,388.50,155.43,90.40,14.41;56,244.37,183.63,4.23,6.99;56,252.42,184.74,9.30,20.74;56,265.04,185.84,5.19,10.48;56,273.33,184.74,3.32,20.74;56,279.31,188.46,17.13,8.54;56,300.09,184.74,7.97,20.74;56,311.38,185.84,60.98,10.48;56,72.00,208.86,48.36,14.41;56,124.12,210.64,9.30,20.74;56,141.47,210.12,4.23,6.99;56,146.20,211.74,10.41,10.48;56,159.26,210.64,9.30,20.74;56,171.22,208.86,197.77,14.41;56,372.64,210.64,7.97,20.74;56,383.93,208.86,99.00,14.41;56,179.38,240.26,46.23,10.80;56,225.60,237.60,7.06,6.99;56,232.66,242.43,47.96,8.62;56,280.62,236.96,7.06,6.99;56,287.68,243.34,18.29,6.99;56,306.47,237.64,10.32,10.48;56,318.07,235.42,4.23,6.99;56,322.80,237.64,15.76,10.48;56,341.21,236.54,9.30,20.74;56,353.17,237.64,9.37,10.48;56,362.54,242.15,3.95,6.99;56,367.23,237.64,20.31,10.48;56,387.54,234.55,4.23,6.99;56,395.59,236.54,9.30,20.74;56,412.93,235.42,4.23,6.99;56,417.67,237.64,14.96,10.48"><head></head><label></label><figDesc xml:id="_TsbT3z9">√ r and √ d − r, respectively. We see that Er = d/2, and thusP(|r − d/2| &gt; d/4) ≤ exp(−Ω(d)) Let δ = inf r∈[d/4,3d/4] E x 1 ∼S d−1 ( √ r),x 2 ∼S d−1 ( √ d−r) (f * (x) − N θ (x)) 2 .We get the bound2 ≥ δ • P(r ∈ [d/4, 3d/4]),and thus δ ≤ 2 (1 − exp(−Ω(d))). Therefore there exists an r ∈ [d/4, 3d/4] such thatE x 1 ∼S d−1 ( √ r),x 2 ∼S d−1 ( √ d−r) (f * (x) − N θ (x)) 2 ≤ 2 /2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="56,72.00,408.24,128.64,14.41;56,213.50,403.80,6.65,10.48;56,220.15,408.32,4.23,6.99;56,213.50,418.25,6.65,10.48;56,220.15,422.76,4.23,6.99;56,233.69,408.24,217.16,14.41;56,450.84,415.64,4.23,6.99;56,455.58,411.13,10.22,10.48;56,465.80,415.64,4.23,6.99;56,470.53,408.24,69.47,14.41;56,72.00,441.28,20.63,10.48;56,104.67,423.98,9.96,20.74;56,114.63,433.69,5.28,10.48;56,122.89,432.59,3.32,20.74;56,128.87,433.69,5.16,10.48;56,167.50,433.69,5.85,10.48;56,116.89,448.67,5.85,10.48;56,144.93,437.88,9.96,20.74;56,154.90,448.67,6.08,10.48;56,163.63,447.57,9.30,20.74;56,175.59,448.67,5.28,10.48;56,183.85,447.57,3.32,20.74;56,189.82,448.67,5.16,10.48;56,208.67,438.40,130.59,14.41;56,339.26,447.21,3.95,6.99;56,343.94,441.28,44.42,10.48;56,388.36,445.80,3.95,6.99;56,393.05,438.40,146.95,14.41;56,72.00,459.29,74.03,14.41;56,146.03,468.10,3.95,6.99;56,153.70,459.29,37.86,14.41;56,211.36,490.69,27.66,9.61;56,245.00,488.07,26.02,10.48;56,271.03,492.58,4.23,6.99;56,275.76,488.07,11.90,10.48;56,287.65,492.58,4.23,6.99;56,297.04,488.07,4.55,10.48;56,304.25,486.97,9.30,20.74;56,316.20,488.07,9.37,10.48;56,325.57,494.00,3.95,6.99;56,330.26,488.07,20.31,10.48;56,350.57,484.98,4.23,6.99;56,363.60,486.97,9.30,20.74;56,380.95,485.85,4.23,6.99;56,385.68,488.07,14.96,10.48;56,72.00,512.29,38.85,14.41;56,110.85,519.68,4.23,6.99;56,115.58,515.17,11.90,10.48;56,127.48,519.68,4.23,6.99;56,135.20,512.29,130.47,14.41;56,266.56,513.55,23.65,6.99;56,290.71,515.17,22.50,10.48"><head>Furthermore, defining x = x 1 x 2 ,</head><label>2</label><figDesc xml:id="_5AxJ6CT">choosing the parameter vector θ = (a, W , b 1 , b 2 ), where W = • I yields a network so that N θ (x) = N θ (x). Therefore we get that the new network N θ satisfiesE x 1 ,x 2 (q( x 1 , x 2 ) − N θ (x)) 2 ≤ 2 /2,where x 1 , x 2 are drawn i.i.d over Unif(S d/2−1 (1)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="57,91.87,220.19,16.49,10.48;57,111.42,219.09,9.30,20.74;57,123.38,220.19,16.91,10.48;57,140.29,218.57,4.23,6.99;57,145.02,220.19,53.98,10.48;57,104.54,235.22,4.41,6.99;57,104.54,244.52,14.71,6.99;57,119.74,238.31,47.45,10.48;57,170.52,237.21,9.30,20.74;57,182.48,238.31,10.41,10.48;57,192.88,237.58,4.23,6.99;57,203.51,228.28,9.11,10.48;57,217.13,220.19,16.49,10.48;57,236.68,219.09,9.30,20.74;57,248.64,220.19,16.91,10.48;57,265.55,218.57,4.23,6.99;57,231.36,236.48,24.70,10.48;57,274.13,227.18,3.32,20.74;57,284.05,220.19,48.70,10.48;57,335.80,219.09,9.30,20.74;57,347.76,220.19,62.36,10.48;57,413.17,219.09,9.30,20.74;57,425.13,220.19,10.41,10.48;57,281.30,238.31,22.34,10.48;57,306.30,237.21,9.30,20.74;57,318.26,238.31,13.66,10.48;57,345.20,235.22,4.41,6.99;57,345.20,244.52,14.71,6.99;57,360.40,238.31,47.45,10.48;57,411.18,237.21,9.30,20.74;57,423.14,238.31,10.41,10.48;57,433.54,237.58,4.23,6.99;57,203.51,269.01,9.11,10.48;57,217.13,260.92,16.49,10.48;57,236.68,259.82,9.30,20.74;57,248.64,260.92,16.91,10.48;57,265.55,259.30,4.23,6.99;57,231.36,277.21,24.70,10.48;57,274.13,267.91,3.32,20.74;57,320.73,260.92,48.70,10.48;57,372.49,259.82,9.30,20.74;57,384.44,260.92,10.41,10.48;57,281.30,277.21,48.70,10.48;57,333.06,276.11,9.30,20.74;57,345.02,277.21,10.41,10.48;57,355.42,276.47,4.23,6.99;57,360.15,277.21,48.70,10.48;57,411.91,276.11,9.30,20.74;57,423.87,277.21,10.41,10.48;57,437.46,256.78,15.44,6.99;57,437.83,285.56,14.71,6.99;57,466.32,260.92,42.90,10.48;57,456.09,277.21,42.90,10.48;57,502.32,276.11,9.30,20.74;57,514.28,277.21,5.85,10.48;57,203.51,306.04,9.30,20.74;57,217.32,299.05,16.49,10.48;57,236.88,297.95,9.30,20.74;57,248.83,299.05,16.91,10.48;57,265.74,297.43,4.23,6.99;57,231.55,315.34,24.70,10.48;57,274.32,306.04,3.32,20.74;57,320.92,299.05,48.70,10.48;57,372.68,297.95,9.30,20.74;57,384.64,299.05,10.41,10.48;57,281.50,315.34,48.70,10.48;57,333.26,314.24,9.30,20.74;57,345.21,315.34,10.41,10.48;57,355.62,314.60,4.23,6.99;57,360.35,315.34,48.70,10.48;57,412.11,314.24,9.30,20.74;57,424.06,315.34,10.41,10.48;57,203.51,338.26,9.30,20.74;57,262.48,331.28,5.85,10.48;57,217.32,347.56,28.84,10.48;57,249.22,346.46,9.30,20.74;57,261.17,347.56,26.90,10.48;57,291.13,346.46,9.30,20.74;57,303.09,347.56,10.41,10.48;57,317.34,338.26,3.32,20.74;57,363.94,331.28,48.70,10.48;57,415.70,330.18,9.30,20.74;57,427.65,331.28,10.41,10.48;57,324.52,347.56,48.70,10.48;57,376.27,346.46,9.30,20.74;57,388.23,347.56,10.41,10.48;57,398.63,346.83,4.23,6.99;57,403.37,347.56,48.70,10.48;57,455.13,346.46,9.30,20.74;57,467.08,347.56,10.41,10.48;57,72.00,363.56,189.82,14.41;57,162.67,396.93,6.61,10.48;57,179.93,388.84,5.85,10.48;57,179.93,405.13,5.85,10.48;57,186.98,396.93,3.25,10.48;57,193.42,388.84,6.08,10.48;57,202.16,387.74,9.30,20.74;57,214.12,388.84,5.85,10.48;57,203.77,405.13,5.85,10.48;57,233.29,396.93,9.11,10.48;57,246.91,378.43,9.96,20.74;57,256.87,388.14,18.94,10.48;57,277.00,386.16,15.18,6.99;57,282.47,394.99,4.23,6.99;57,293.37,388.14,4.55,10.48;57,260.83,405.86,11.87,10.48;57,273.89,403.87,4.36,6.99;57,273.96,412.70,4.23,6.99;57,279.45,405.86,4.55,10.48;57,302.44,395.83,9.30,20.74;57,315.06,386.63,9.96,20.74;57,325.02,396.93,6.64,10.48;57,342.09,388.84,6.08,10.48;57,342.20,405.13,5.85,10.48;57,352.02,395.83,9.30,20.74;57,363.98,396.93,5.85,10.48;57,378.63,385.47,19.29,6.99;57,401.74,395.83,9.30,20.74;57,414.36,396.93,11.94,10.48;57,426.30,394.71,19.29,6.99;57,446.08,396.93,3.25,10.48;57,72.00,422.99,47.13,14.41;57,112.67,450.80,7.54,10.48;57,120.22,455.31,8.64,6.99;57,129.57,450.80,45.66,10.48;57,181.21,447.71,4.23,6.99;57,181.21,456.10,33.40,9.25;57,218.43,449.70,9.30,20.74;57,283.26,442.71,5.85,10.48;57,232.25,459.00,28.84,10.48;57,264.14,457.90,9.30,20.74;57,276.09,459.00,26.90,10.48;57,306.05,457.90,9.30,20.74;57,318.01,459.00,22.11,10.48;57,343.97,449.70,3.32,20.74;57,387.53,442.71,48.70,10.48;57,439.29,441.61,9.30,20.74;57,451.24,442.71,16.49,10.48;57,351.14,459.00,48.70,10.48;57,402.90,457.90,9.30,20.74;57,414.86,459.00,10.41,10.48;57,425.26,458.27,4.23,6.99;57,429.99,459.00,48.70,10.48;57,481.75,457.90,9.30,20.74;57,493.71,459.00,10.41,10.48;57,218.43,481.93,9.30,20.74;57,243.71,474.94,5.85,10.48;57,232.25,491.23,23.65,10.48;57,256.29,490.50,4.23,6.99;57,264.88,481.93,3.32,20.74;57,308.44,474.94,48.70,10.48;57,360.19,473.84,9.30,20.74;57,372.15,474.94,16.49,10.48;57,272.05,491.23,48.70,10.48;57,323.81,490.13,9.30,20.74;57,335.77,491.23,10.41,10.48;57,346.17,490.50,4.23,6.99;57,350.90,491.23,48.70,10.48;57,402.66,490.13,9.30,20.74;57,414.62,491.23,10.41,10.48;57,218.43,512.89,9.30,20.74;57,246.75,505.91,5.85,10.48;57,232.25,522.20,23.65,10.48;57,256.29,521.46,4.23,6.99;57,261.03,522.20,6.08,10.48;57,72.00,536.02,23.02,14.41;57,98.74,537.80,9.30,20.74;57,111.36,536.02,78.13,14.41;57,185.43,572.67,7.54,10.48;57,192.98,577.19,18.31,6.99;57,211.79,572.67,45.66,10.48;57,263.43,569.58,4.23,6.99;57,263.43,578.12,33.40,9.25;57,300.65,571.57,9.30,20.74;57,323.36,564.59,5.85,10.48;57,314.47,580.87,23.64,10.48;57,344.23,559.52,12.83,6.99;57,341.30,589.50,18.70,6.99;57,365.87,564.59,5.85,10.48;57,363.18,580.87,6.09,10.48;57,369.67,580.14,4.23,6.99;57,378.92,571.57,9.30,20.74;57,409.12,564.59,5.85,10.48;57,392.74,580.87,27.80,10.48;57,420.53,580.14,4.23,6.99;57,425.27,580.87,6.08,10.48;57,72.00,608.38,27.17,14.41;57,102.49,610.16,9.30,20.74;57,115.11,608.38,130.78,14.41;57,259.05,608.29,25.64,6.99;57,269.69,618.11,4.36,6.99;57,285.89,611.26,10.52,10.48;57,299.07,610.16,9.30,20.74;57,311.02,611.26,5.04,10.48;57,316.06,615.78,4.23,6.99;57,324.11,611.26,18.28,10.48;57,355.54,608.29,25.64,6.99;57,366.19,618.11,4.36,6.99;57,382.38,611.26,44.98,10.48;57,430.02,610.16,9.30,20.74;57,441.98,611.26,5.04,10.48;57,447.01,615.78,4.23,6.99;57,451.75,608.38,65.98,14.41;57,204.39,642.50,7.54,10.48;57,211.94,647.02,18.31,6.99;57,230.74,642.50,5.19,10.48;57,242.34,647.95,21.59,7.90;57,267.75,641.40,9.30,20.74;57,280.37,642.50,5.85,10.48;57,287.42,634.41,16.24,10.48;57,306.31,633.31,9.30,20.74;57,318.27,634.41,10.15,10.48;57,304.88,650.70,6.08,10.48;57,335.59,642.50,7.54,10.48;57,343.14,647.02,18.31,6.99;57,361.95,642.50,45.66,10.48"><head>(2k − 3 )− 1 ≥ 1 − 1 / 2 ≤</head><label>31112</label><figDesc xml:id="_vt3e8Sa">!! 2 B(d/2, 2k) k j=0 (d/2 + 2j − 1) 2 = (2k − 3)!! 2 (2k)! • (d/2 + 2k − 3)!(d/2 + 4k − 2) (d/2 − 2)! k j=0 (d/2 + 2j − 1) 2 = (2k − 3)!! 2 (2k)! • (d/2 + 4k − 2) (d/2 + 2k − 1) 2 (d/2 + 2k − 3) (2k − 3)!! 2 (2k)! • (d/2 + 4k − 2) (d/2 + 2k − 1) 2 (d/2 + 2k − 3) ≥ 1 2k(2k − 1)(2k − 2) • (d/2 + 4k − 2) (d/2 + 2k − 1) 2 (d/2 + 2k − 3) By Gautschi's inequality, we can bound 4d −1/2 . Therefore P 2k ReLU(x) 2 L 2 (µ d/2 ) ≥ 1 2k(2k − 1)(2k − 2)16 • (d/2 + 4k − 2)d (d/2 + 2k − 1) 2 (d/2 + 2k − 3) ≥ 1 128k 3 • (d/2 + 4k − 2)d (d/2 + 2k − 1) 2 (d/2 + 2k − 3) ≥ 1 128k 3 d for k ≤ d/4. Altogether, P ≥2m ReLU(x) 2 L 2 (µ d/2 ) for m ≤ d/8. Since q(z) = ReLU(2 r(d−r) d z) − c 0 = 2 r(d−r)d ReLU(z) − c 0 , we have thatP ≥2m q L 2 (µ) ≥ 4 r(d − r) d P ≥2m ReLU(x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="58,299.16,201.63,12.70,6.99;58,312.36,203.24,4.55,10.48;58,316.91,201.63,21.26,6.99;58,275.38,220.36,8.89,10.48;58,284.27,224.94,23.82,6.99;58,343.18,210.23,9.30,20.74;58,363.17,203.24,5.85,10.48;58,357.00,219.53,17.79,10.48;58,376.39,211.33,3.25,10.48;58,72.00,236.68,9.96,14.41;58,131.90,264.76,59.74,10.48;58,191.65,262.55,12.70,6.99;58,204.85,264.76,4.55,10.48;58,209.40,262.55,21.26,6.99;58,234.48,263.66,9.30,20.74;58,254.46,256.68,5.85,10.48;58,248.29,272.96,17.79,10.48;58,267.68,264.76,53.98,10.48;58,321.66,262.55,12.70,6.99;58,338.18,263.66,9.30,20.74;58,350.80,264.76,6.08,10.48;58,356.88,262.55,4.41,6.99;58,362.00,264.76,5.85,10.48;58,367.86,262.55,11.00,6.99;58,382.22,263.66,3.32,20.74;58,413.89,256.68,5.85,10.48;58,389.39,274.63,54.85,10.48;58,234.48,299.59,20.80,10.48;58,255.28,304.11,4.23,6.99;58,262.00,299.59,60.35,10.48;58,325.01,298.49,9.30,20.74;58,336.97,299.59,23.20,10.48;58,363.23,298.49,9.30,20.74;58,376.38,291.51,5.85,10.48;58,376.38,307.79,5.85,10.48;58,385.42,299.59,39.82,10.48;58,427.89,298.49,9.30,20.74;58,439.85,299.59,31.45,10.48;58,234.48,323.95,9.30,20.74;58,247.10,325.05,8.37,10.48;58,255.47,329.56,4.23,6.99;58,262.20,325.05,56.11,10.48;58,320.96,323.95,9.30,20.74;58,332.91,325.05,23.20,10.48;58,359.17,323.95,9.30,20.74;58,371.13,325.05,45.05,10.48;58,418.83,323.95,9.30,20.74;58,430.79,325.05,36.00,10.48;58,234.48,349.60,9.30,20.74;58,247.10,350.70,8.37,10.48;58,255.47,355.22,4.23,6.99;58,262.20,350.70,50.12,10.48;58,315.87,342.61,6.08,10.48;58,315.67,358.90,6.09,10.48;58,326.01,349.60,9.30,20.74;58,337.97,350.70,23.20,10.48;58,364.23,349.60,9.30,20.74;58,376.18,350.70,37.30,10.48;58,234.48,382.28,9.30,20.74;58,247.10,383.38,8.37,10.48;58,255.47,387.89,4.23,6.99;58,262.20,383.38,35.06,10.48;58,297.26,387.89,4.23,6.99;58,301.99,383.38,23.44,10.48;58,328.98,375.29,6.08,10.48;58,328.78,391.58,6.09,10.48;58,72.00,415.77,43.09,14.41;58,118.81,417.55,9.30,20.74;58,131.43,418.65,8.37,10.48;58,139.81,423.17,4.23,6.99;58,144.54,415.77,79.18,14.41;58,251.45,416.67,4.23,6.99;58,245.50,425.50,12.70,6.99;58,271.13,415.77,28.56,14.41;58,126.28,446.20,55.87,10.48;58,185.47,445.10,9.30,20.74;58,198.09,446.20,8.37,10.48;58,206.47,450.72,4.23,6.99;58,213.19,446.20,31.74,10.48;58,244.93,450.72,4.23,6.99;58,254.39,443.99,19.29,6.99;58,276.17,446.20,35.03,10.48;58,319.34,445.10,3.32,20.74;58,325.32,446.20,6.08,10.48;58,331.40,443.99,19.29,6.99;58,354.51,445.10,9.30,20.74;58,367.13,446.20,8.37,10.48;58,375.50,450.72,4.23,6.99;58,382.23,446.20,31.74,10.48;58,413.97,450.72,4.23,6.99;58,423.43,443.99,19.29,6.99;58,445.21,446.20,35.03,10.48"><head>≥ C 1 exp k log d k − log k − 2k log 2 ≥ C 1</head><label>21</label><figDesc xml:id="_AXQFGpx">1)(Bd 3/2 ) ασ+1 ≥ 1 64k B(d/2, 2k) 1/2 ≥ d k 2 −k • 1 64k (2k)! = C 1 exp k log d − log k − 1 2 log(2k)! − k log 2 ≥ C 1 exp (k log d − log k − k log(2k) − k log 2) exp C 2 k log d k for any k ≤ C 3 d. Selecting k = 1 512 yields max(m, B) ≥ C 1 exp C 2 −1/2 log(d ) • d −3/2 ≥ C 1 exp C 2 −1/2 log(d )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="58,196.76,612.75,9.41,7.90;58,225.12,607.44,6.08,10.48;58,231.21,605.83,10.82,6.99;58,242.53,604.56,65.95,14.41;58,325.96,605.46,15.18,6.99;58,329.25,614.29,8.59,6.99;58,342.33,604.56,131.98,14.41;58,474.31,605.83,4.93,6.99;58,480.91,607.44,15.43,10.48;58,502.98,613.94,9.41,7.90;58,517.72,607.44,22.28,10.48;58,72.00,627.14,103.08,14.41;58,176.36,628.41,4.23,6.99;58,181.09,630.02,39.46,10.48;58,220.98,628.41,4.23,6.99;58,225.71,630.02,11.20,10.48;58,236.91,628.41,4.93,6.99;58,243.52,627.14,46.77,14.41;58,290.72,628.41,4.23,6.99;58,295.45,630.02,28.68,10.48;58,345.07,628.04,8.59,6.99;58,341.77,636.87,15.18,6.99;58,358.15,630.02,44.98,10.48;58,406.26,628.92,9.30,20.74;58,418.68,630.02,5.04,10.48;58,423.72,634.54,4.23,6.99;58,428.45,627.14,111.54,14.41;58,72.00,644.68,143.54,14.41;58,206.08,688.82,61.33,10.48;58,270.73,668.41,10.63,7.65;58,270.73,679.17,10.63,11.24;58,270.73,704.28,10.63,11.24;58,281.36,671.96,5.85,10.48;58,343.00,671.96,5.44,10.48;58,352.29,670.86,21.92,20.74;58,284.26,687.31,4.23,6.99;58,282.55,696.14,4.23,6.99;58,291.41,689.30,34.90,10.48;58,326.31,687.68,4.23,6.99;58,343.00,688.20,26.64,20.74;58,372.96,689.30,5.85,10.48;58,382.13,688.20,9.30,20.74;58,281.36,706.63,6.65,10.48;58,342.99,706.63,6.65,10.48;58,352.97,705.53,9.30,20.74;58,402.67,688.82,3.25,10.48;61,72.00,72.91,177.26,14.41;61,251.91,74.69,9.30,20.74;61,263.87,75.79,6.65,10.48;61,270.52,74.17,4.23,6.99;61,275.25,75.79,4.55,10.48;61,281.00,72.10,13.25,5.24;61,285.80,78.59,3.65,5.24;61,295.94,72.91,32.74,14.41;61,216.17,109.98,8.89,10.48;61,225.67,106.60,10.94,6.99;61,225.07,115.64,4.23,6.99;61,240.43,109.98,9.11,10.48;61,264.81,99.59,4.23,6.99;61,259.50,123.50,4.23,6.99;61,271.54,109.98,19.78,10.48;61,291.32,114.50,4.36,6.99;61,296.17,109.98,40.28,10.48;61,336.45,106.60,10.94,6.99;61,336.45,115.64,4.23,6.99;61,351.21,109.98,9.11,10.48;61,371.67,101.90,8.01,10.48;61,379.69,106.41,4.36,6.99;61,364.83,118.18,6.08,10.48;61,373.57,117.08,9.30,20.74;61,385.53,118.18,5.85,10.48;61,392.58,109.98,3.25,10.48"><head>L 2 d − 2 . 1 4</head><label>221</label><figDesc xml:id="_96ehax8">Define A = d+2 2d A. This scaling ensures x T Ax L 2 = 1. Then, we can write f * (x) = g * (x T Ax) for g * (z) = 2d d+2 ReLU(z) − c 0 . For &gt; 0, define the smoothed ReLU ReLU (z) as ReLU (z) = (x + ) 2 − ≤ 0 ≤ x x ≥ .where we use the substitution u = (1 − x 2 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="61,72.00,416.39,8.78,10.48;61,80.78,413.00,10.94,6.99;61,80.78,422.44,19.68,6.99;61,104.27,416.39,20.44,10.48;61,124.71,420.91,4.36,6.99;61,132.22,415.29,3.32,20.74;61,175.82,408.30,48.22,10.48;61,139.39,424.73,10.63,10.48;61,152.69,423.63,9.30,20.74;61,164.64,424.73,19.18,10.48;61,183.82,423.33,4.41,6.99;61,183.82,430.51,14.71,6.99;61,199.02,424.73,35.75,10.48;61,238.10,423.63,9.30,20.74;61,250.06,424.73,10.41,10.48;61,261.66,416.39,24.26,10.48;61,285.91,414.18,15.44,6.99;61,304.51,416.39,9.11,10.48;61,317.47,408.30,83.67,10.48;61,339.11,424.59,10.63,10.48;61,352.40,423.49,9.30,20.74;61,364.36,424.59,10.41,10.48;61,374.76,423.86,4.23,6.99;61,404.99,415.29,3.32,20.74;61,417.57,408.30,8.01,10.48;61,425.58,412.82,4.36,6.99;61,412.16,424.59,8.01,10.48;61,420.17,429.11,15.18,6.99;61,437.04,416.39,8.89,10.48;61,446.54,413.00,21.76,6.99;61,445.94,422.44,19.68,6.99;61,104.27,450.38,20.44,10.48;61,124.71,454.90,4.36,6.99;61,132.22,449.28,3.32,20.74;61,175.82,442.29,48.22,10.48;61,139.39,458.72,10.63,10.48;61,152.69,457.62,9.30,20.74;61,164.64,458.72,19.18,10.48;61,183.82,457.32,4.41,6.99;61,183.82,464.50,14.71,6.99;61,199.02,458.72,35.75,10.48;61,238.10,457.62,9.30,20.74;61,250.06,458.72,10.41,10.48;61,261.66,450.38,24.26,10.48;61,285.91,448.17,15.44,6.99;61,304.51,450.38,19.77,10.48;61,324.29,454.90,4.36,6.99;61,331.80,449.28,3.32,20.74;61,338.97,442.29,83.67,10.48;61,360.61,458.58,10.63,10.48;61,373.90,457.48,9.30,20.74;61,385.86,458.58,10.41,10.48;61,396.26,457.85,4.23,6.99;61,426.49,449.28,3.32,20.74;61,452.07,442.29,16.49,10.48;61,471.62,441.19,9.30,20.74;61,483.57,442.29,16.91,10.48;61,433.66,458.72,8.78,10.48;61,442.44,457.32,4.41,6.99;61,442.44,464.50,14.71,6.99;61,457.64,458.72,61.24,10.48;61,520.08,450.38,24.26,10.48;61,544.34,448.17,4.41,6.99;61,104.27,484.65,36.68,10.48;61,140.95,482.44,15.44,6.99;61,156.89,484.65,8.01,10.48;61,164.91,489.17,4.36,6.99;61,172.42,483.55,3.32,20.74;61,198.46,476.56,16.49,10.48;61,218.01,475.46,9.30,20.74;61,229.97,476.56,16.91,10.48;61,179.59,493.82,8.78,10.48;61,188.37,491.59,15.44,6.99;61,188.37,499.61,14.71,6.99;61,204.31,493.82,35.75,10.48;61,243.38,492.72,9.30,20.74;61,255.34,493.82,10.41,10.48;61,274.45,476.56,103.94,10.48;61,313.14,492.85,6.08,10.48;61,321.88,491.75,9.30,20.74;61,333.84,492.85,5.85,10.48;61,382.23,483.55,9.30,20.74;61,395.39,476.56,83.67,10.48;61,423.94,492.85,6.08,10.48;61,432.68,491.75,9.30,20.74;61,444.64,492.85,5.85,10.48;61,104.27,519.75,36.68,10.48;61,140.95,517.54,15.44,6.99;61,156.89,519.75,8.01,10.48;61,164.91,524.27,4.36,6.99;61,172.42,518.65,3.32,20.74;61,198.46,511.67,16.49,10.48;61,218.01,510.57,9.30,20.74;61,229.97,511.67,16.91,10.48;61,179.59,528.92,8.78,10.48;61,188.37,526.69,15.44,6.99;61,188.37,534.71,14.71,6.99;61,204.31,528.92,35.75,10.48;61,243.38,527.82,9.30,20.74;61,255.34,528.92,10.41,10.48;61,274.45,510.57,15.38,20.74;61,292.48,511.67,17.61,10.48;61,279.00,527.95,6.08,10.48;61,287.74,526.85,9.30,20.74;61,299.69,527.95,5.85,10.48;61,104.27,554.57,36.68,10.48;61,140.95,552.36,15.44,6.99;61,156.89,554.57,8.01,10.48;61,164.91,559.09,4.36,6.99;61,172.42,553.47,3.32,20.74;61,198.46,546.49,16.49,10.48;61,218.01,545.39,9.30,20.74;61,229.97,546.49,16.91,10.48;61,179.59,563.74,8.78,10.48;61,188.37,561.52,15.44,6.99;61,188.37,569.53,14.71,6.99;61,204.31,563.74,35.75,10.48;61,243.38,562.64,9.30,20.74;61,255.34,563.74,10.41,10.48"><head>A= 1 = (− 1 ) 1 = (− 1 )</head><label>1111</label><figDesc xml:id="_cCwzN57">Z d • (2k + 1)!! (d − 1)Π k j=0 (d + 2j − 1) (−1) k+1 + Z d • (2k + 2)(2k + d) (d − 1) 2 • (2k − 1)!! Π k j=0 (d + 2j + 1) (−1) k = (−1) k+1 Z d • (2k − 1)!! Π k+1 j=0 (d + 2j − 1) (d + 2k + 1)(2k + 1) d − 1 − (2k + 2)(2k + d) d − k+1 Z d • (2k − 1)!! Π k+1 j=0 (d + 2j − 1) −d + 1 d − k+2 Z d • (2k − 1)!! Π k+1 j=0 (d + 2j − 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,72.00,513.27,468.00,72.19"><head></head><label></label><figDesc xml:id="_VRTePHx">• Kf * . Choosing η so that |φ(x )| ≤ 1, we see that Algorithm 1 learns functions of the form φ • (η • Kf * ). Finally, we translate the above analysis to the finite sample gradient via standard concentration tools. Since the empirical estimate to</figDesc><table coords="8,72.00,549.78,468.00,35.68"><row><cell>Kf  *  concentrates at a 1/ √ n rate, n approximation (Lemma 7).</cell><cell>Kf  *  −2 L 2 samples are needed to obtain a constant factor</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="26,72.00,72.91,468.60,43.68"><head></head><label></label><figDesc xml:id="_bwDYgCp">and m 2 Kf * −2 L 2 ι 2ασ+1 . There exists η = Θ( Kf * −2 L 2 ) such that with high probability, sup x∈D 2 |ηφ(x)| ≤ 1 and sup x∈D 2 |η • Kf</figDesc><table /><note xml:id="_GesKTMg">* (x)| ≤ 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="33,72.00,272.85,367.04,54.64"><head></head><label></label><figDesc xml:id="_BS4EJXH">Kf  </figDesc><table /><note xml:id="_Eu3GfB6">* Kf * L 2 yields the second bound Corollary 1. With high probability, sup x∈D 2 |f * (x)| ≤ C f e • ι and sup x∈D 2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="56,248.67,686.05,139.61,41.09"><head></head><label></label><figDesc xml:id="_7ugmNFz">3/2 /2 + B) ασ ≤ (Bd 3/2 ) ασ+1 , since W ∞ ≤ max( √ r, √ d − r) W U ∞ ≤ Bd/2.Therefore by Lemma 39E x 1 ,x 2 (q( x 1 , x 2 ) − N θ (x)) 2 ≥ P ≥k q L 2 • P ≥k q L 2 − 2(m + 1)(Bd 3/2 ) ασ+1 B d/2,k .</figDesc><table coords="57,72.00,137.09,393.35,74.83"><row><cell>By Lemma 38, we have that</cell><cell></cell><cell></cell></row><row><cell>P ≥2m ReLU(x) 2 L 2 (µ d/2 ) = Simplifying, we have that</cell><cell>k≥m</cell><cell>(2k − 3)!! 2 B(d/2, 2k) 2 , d/2−1 β( 1 2 ) 2 k j=0 (d/2 + 2j − 1) 2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">) denote the normalizing constant.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">≥ E x (f * (x) − N θ (x)) 2 = E r∼µ E x 1 ∼S d−1 ( √ r),x 2 ∼S d−1 ( √ d−r) (f * (x) − N θ (x)) 2 ,</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ERVSnRY">Acknowledgements</head><p xml:id="_cCZFgZv">EN acknowledges support from a National Defense Science &amp; Engineering Graduate Fellowship. AD acknowledges support from a NSF Graduate Research Fellowship. EN, AD, and JDL acknowledge support of the ARO under MURI Award W911NF-11-1-0304, the Sloan Research Fellowship, NSF CCF 2002272, NSF IIS 2107304, NSF CIF 2212262, ONR Young Investigator Award, and NSF CAREER Award 2144994.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_ntYemdK"><p xml:id="_JapTC5n">One sees that ReLU is twice differentiable with ReLU 1,∞ ≤ 1 and ReLU 2,∞ = 1   2   We select the test function q to be q(z) = 2d d+2 ReLU (η −1 Kf * −1 L 2 • z) − c 0 . We see that q(η(Kf * )(x)) = ReLU Kf * −1 L 2 (Kf * )(x) , and thus</p><p xml:id="_fasB8yA">where the first inequality follows from Lipschitzness and the second inequality is Corollary 3, using κ = 1.</p><p xml:id="_kb2m2pA">There exists a constant upper bound for the density of x T Ax, and thus we can upper bound</p><p xml:id="_AaCZ5ns">Therefore by Theorem 6 we can bound the population loss as</p><p xml:id="_TTU732D">Choosing = d −1/4 yields the desired result. As for the sample complexity, we have q 2,∞ = Õ( −1 ) = Õ(d 1/4 ), and so the runtime is poly(d, m 1 , m 2 , n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_YGe2YmS">G.1 Proof of Lemma 38</head><p xml:id="_Xx9je9R">Proof of Lemma 38. For any integer k, we define the quantities</p><p xml:id="_7pZ2NwH">2k+1 as</p><p xml:id="_tWCtdys">We also let</p><p xml:id="_Myt37Sg">2 ) to be the normalization constant. Integration by parts yields</p><p xml:id="_auQAAYF">From Corollary 5 we have</p><p xml:id="_EE3jpFB">The recurrence formula yields</p><p xml:id="_Rz8zAak">I claim that</p><p xml:id="_TbEEHC7">.</p><p xml:id="_qJEzpTy">We proceed by induction on k. For the base cases, we first have</p><p xml:id="_qguKCqy">.</p><p xml:id="_7sUjSfb">Therefore by induction the claim holds for all k, d.</p><p xml:id="_9pYJhzR">The Gegenbauer expansion of ReLU is given by</p><p xml:id="_F9QANAM">Note that ReLU(x) = 1 2 (x + |x|). Since |x| is even, the only nonzero odd Gegenbauer coefficient is for G </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="13,97.78,332.90,442.22,14.41;13,97.77,347.35,442.23,14.41;13,97.77,361.80,442.23,14.41;13,97.77,376.24,86.68,14.41" xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_bEFs96c">The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on twolayer neural networks</title>
		<author>
			<persName coords=""><forename type="first">Emmanuel</forename><surname>Abbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Enric</forename><surname>Boix Adsera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theodor</forename><surname>Misiakiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_AGpMhPn">Proceedings of Thirty Fifth Conference on Learning Theory</title>
				<meeting>Thirty Fifth Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4782" to="4887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,97.78,397.91,442.23,14.41;13,97.77,412.36,394.59,14.41;13,496.96,416.74,43.04,8.42;13,97.77,431.19,189.49,8.42" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Emmanuel</forename><surname>Abbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Enric</forename><surname>Boix-Adserà</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theodor</forename><surname>Misiakiewicz</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2302.11055" />
		<title level="m" xml:id="_U6q4gcu">Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics. arXiv</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,97.78,448.47,442.22,14.41;13,97.77,462.92,296.47,14.41" xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_2TYBQ3t">What can resnet learn efficiently, going beyond kernels?</title>
		<author>
			<persName coords=""><forename type="first">Zeyuan</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-Zhu</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_8aPJt8P">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,97.78,484.58,442.22,14.41;13,97.77,499.03,190.56,14.41;13,291.32,503.42,232.53,8.42" xml:id="b3">
	<monogr>
		<title level="m" type="main" xml:id="_zrGD7Nn">Backward feature correction: How deep learning performs deep learning. arXiv</title>
		<author>
			<persName coords=""><forename type="first">Zeyuan</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-Zhu</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2001.04413" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,97.77,520.70,442.22,14.41;13,97.77,535.15,442.23,14.41;13,97.77,549.59,126.31,14.41" xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_GwvSADF">Learning and generalization in overparameterized neural networks, going beyond two layers</title>
		<author>
			<persName coords=""><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_hJXxKmr">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,97.77,571.26,442.22,14.41;13,97.77,585.71,442.23,14.41;13,97.77,600.15,178.43,14.41" xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_5krVmQ7">On exact computation with an infinitely wide neural net</title>
		<author>
			<persName coords=""><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruosong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wnUV4sJ">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,97.77,621.82,442.22,14.41;13,97.77,636.27,442.23,14.41;13,97.77,650.71,299.56,14.41" xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_35p4Hcq">Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks</title>
		<author>
			<persName coords=""><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruosong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_PsjNc58">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,97.77,672.38,442.22,14.41;13,97.77,686.83,442.23,14.41;13,97.77,701.27,408.35,14.41" xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_aCDUu8G">High-dimensional asymptotics of feature learning: How one gradient step improves the representation</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Murat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Taiji</forename><surname>Erdogdu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhichao</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Denny</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_NccKh5f">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,97.78,72.91,442.22,14.41;14,97.77,87.35,442.23,14.41;14,97.77,101.80,26.90,14.41" xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_VXWXpFq">Beyond linearization: On quadratic and higher-order approximation of wide neural networks</title>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_zU7bxFv">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,97.78,123.47,442.22,14.41;14,97.77,137.91,442.23,14.41;14,97.77,152.36,348.60,14.41" xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_WuVKZH5">Hidden progress in deep learning: Sgd learns parities near the computational limit</title>
		<author>
			<persName coords=""><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><forename type="middle">L</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Surbhi</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cyril</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_JppCTnC">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,97.78,174.03,442.22,14.41;14,97.77,188.48,442.23,14.41;14,97.77,202.92,68.74,14.41" xml:id="b10">
	<monogr>
		<title level="m" type="main" xml:id="_EQKQC8G">Sobolev inequalities, the poisson semigroup, and analysis on the sphere sn</title>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Beckner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="4816" to="4819" />
		</imprint>
		<respStmt>
			<orgName>Proceedings of the National Academy of Sciences of the United States of America</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="14,97.78,224.59,442.22,14.41;14,97.77,239.04,442.23,14.41;14,97.77,253.48,121.91,14.41" xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_VbCZzv9">Online stochastic gradient descent on non-convex losses from high-dimensional inference</title>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>Ben Arous</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reza</forename><surname>Gheissari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aukosh</forename><surname>Jagannath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hFS74ec">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,97.78,275.15,442.22,14.41;14,97.77,289.60,442.23,14.41;14,97.77,304.04,82.01,14.41" xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_JUJfMnZ">Learning single-index models with shallow neural networks</title>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Bietti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clayton</forename><surname>Sanford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min</forename><forename type="middle">Jae</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_MBNQM3p">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,97.78,325.71,442.22,14.41;14,97.77,340.16,426.15,14.41" xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_dhd9Ggx">Generalization bounds of stochastic gradient descent for wide and deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_e7h2Gzc">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,97.78,361.83,442.22,14.41;14,97.77,376.27,442.23,14.41;14,97.77,390.72,283.53,14.41" xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_GnXNaj2">Towards understanding hierarchical learning: Benefits of neural representations</title>
		<author>
			<persName coords=""><forename type="first">Minshuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_DRfccyf">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,97.78,412.39,442.23,14.41;14,97.77,426.83,405.04,14.41" xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_BdVQcXt">On lazy training in differentiable programming</title>
		<author>
			<persName coords=""><forename type="first">Lénaïc</forename><surname>Chizat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wvwKXBQ">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,97.78,448.50,442.22,14.41;14,97.77,462.95,442.23,14.41;14,97.77,477.39,156.73,14.41" xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_MstU6dH">Neural networks can learn representations with gradient descent</title>
		<author>
			<persName coords=""><forename type="first">Alexandru</forename><surname>Damian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mahdi</forename><surname>Soltanolkotabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_gVRycfC">Proceedings of Thirty Fifth Conference on Learning Theory</title>
				<meeting>Thirty Fifth Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5413" to="5452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,97.78,499.06,442.22,14.41;14,97.77,513.51,442.23,14.41;14,97.77,527.95,394.64,14.41;14,496.96,532.34,43.04,8.42;14,97.77,546.79,311.43,8.42" xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_NcTyghF">Depth separation for neural networks</title>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Daniely</surname></persName>
		</author>
		<idno>PMLR, 07-10</idno>
		<ptr target="https://proceedings.mlr.press/v65/daniely17a.html" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_FZbkKZZ">Proceedings of the 2017 Conference on Learning Theory</title>
				<meeting>the 2017 Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul 2017</date>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="690" to="696" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="14,97.78,564.07,442.22,14.41;14,97.77,578.51,274.56,14.41" xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_JV7PXtf">Learning parities with neural networks</title>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Daniely</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ww2GPVF">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,97.78,600.18,442.22,14.41;14,97.77,614.63,442.23,14.41;14,97.77,629.07,442.23,14.27;14,97.77,643.52,442.23,14.41;14,97.77,657.96,442.23,14.41;14,97.77,672.41,143.62,14.41;14,244.37,676.80,240.20,8.42" xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_EJsS7uT">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://aclanthology.org/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_krhNnk8">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s" xml:id="_7Q6Pyb3">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,97.78,694.08,442.22,14.41;15,97.77,72.91,442.23,14.41;15,97.77,87.35,381.84,14.41" xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_9ruCvFb">On the power of over-parametrization in neural networks with quadratic activation</title>
		<author>
			<persName coords=""><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_u9RuACy">Proceedings of the 35th International Conference on Machine Learning, Proceedings of Machine Learning Research</title>
				<meeting>the 35th International Conference on Machine Learning, Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1329" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,97.78,109.02,442.22,14.41;15,97.77,123.47,442.24,14.41;15,97.77,137.91,146.33,14.41" xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_uSGR7nn">Gradient descent provably optimizes over-parameterized neural networks</title>
		<author>
			<persName coords=""><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiyu</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_R29yvCk">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,97.78,159.58,442.22,14.41;15,97.77,174.03,442.23,14.41;15,97.77,188.48,442.23,14.41;15,97.77,202.92,395.81,14.41;15,496.96,207.31,43.04,8.42;15,97.77,221.75,289.91,8.42" xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_2PydbG5">The power of depth for feedforward neural networks</title>
		<author>
			<persName coords=""><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v49/eldan16.html" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_3sC94rf">of Proceedings of Machine Learning Research</title>
				<editor>
			<persName><forename type="first">Vitaly</forename><surname>Feldman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexander</forename><surname>Rakhlin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">Jun 2016</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="23" to="26" />
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
	<note>29th Annual Conference on Learning Theory</note>
</biblStruct>

<biblStruct coords="15,97.78,239.04,442.22,14.41;15,97.77,253.48,442.23,14.41;15,97.77,267.93,70.73,14.41" xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_BaUqfz5">Limitations of lazy training of two-layers neural network</title>
		<author>
			<persName coords=""><forename type="first">Behrooz</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Song</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theodor</forename><surname>Misiakiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_eZT78Tu">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,97.78,289.60,442.22,14.41;15,97.77,304.04,442.23,14.41;15,97.77,318.49,70.73,14.41" xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_6FqMxUQ">When do neural networks outperform kernel methods?</title>
		<author>
			<persName coords=""><forename type="first">Behrooz</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Song</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theodor</forename><surname>Misiakiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_h4NnJWS">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,97.78,340.16,442.22,14.41;15,97.77,354.60,442.23,14.41" xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_rR9Xgjb">Linearized two-layers neural networks in high dimension</title>
		<author>
			<persName coords=""><forename type="first">Behrooz</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Song</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theodor</forename><surname>Misiakiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jm77Y4a">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1029" to="1054" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,97.78,376.27,442.22,14.41;15,97.77,390.72,245.14,14.41;15,346.33,395.10,193.67,8.42;15,97.77,409.55,38.85,8.42" xml:id="b26">
	<monogr>
		<title level="m" type="main" xml:id="_Y55KTdk">Exact convergence rates of the neural tangent kernel in the large depth limit</title>
		<author>
			<persName coords=""><forename type="first">Soufiane</forename><surname>Hayou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Judith</forename><surname>Rousseau</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1905.13654" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,97.78,426.83,442.22,14.41;15,97.77,441.28,442.23,14.41;15,97.77,455.72,104.93,14.41" xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_JdQQ2BU">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Kaiming He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_XHyBTMY">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2015">2016. 2015</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,97.78,477.39,442.22,14.41;15,97.77,491.84,442.23,14.41;15,97.77,506.28,442.23,14.41;15,97.77,520.73,442.23,14.41;15,97.77,535.18,137.75,14.41;15,238.73,539.56,301.27,8.42;15,97.77,554.01,340.62,8.42" xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_YqKwfBc">Why do deep residual networks generalize better than deep feedforward networks? -a neural tangent kernel perspective</title>
		<author>
			<persName coords=""><forename type="first">Kaixuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Molei</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/1c336b8080f82bcc2cd2499b4c57261d-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_GacXfnM">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2698" to="2709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,97.78,571.29,442.22,14.41;15,97.77,585.74,442.23,14.41;15,97.77,600.18,177.98,14.41" xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_rnnbDQT">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<author>
			<persName coords=""><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Franck</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clément</forename><surname>Hongler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_uhyaaqV">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8571" to="8580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,97.78,621.85,442.22,14.41;15,97.77,636.30,442.23,14.41;15,97.77,650.74,296.47,14.41" xml:id="b30">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<title level="m" xml:id="_ENbpp8E">Finite versus infinite neural networks: an empirical study</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct coords="15,97.78,672.41,442.22,14.41;15,97.77,686.86,442.23,14.41;15,97.77,701.30,332.02,14.41" xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_PqgRxwG">Learning over-parametrized two-layer neural networks beyond ntk</title>
		<author>
			<persName coords=""><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongyang</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Z2Kwj3A">Proceedings of Thirty Third Conference on Learning Theory, Proceedings of Machine Learning Research</title>
				<meeting>Thirty Third Conference on Learning Theory, Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2613" to="2682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,97.78,72.91,442.22,14.41;16,97.77,87.35,442.23,14.41;16,97.77,101.80,56.36,14.41;16,171.80,101.80,23.91,14.41;16,202.86,106.19,337.14,8.42;16,97.77,120.63,304.76,8.42" xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_UeAVsSf">Is deeper better only when shallow is good?</title>
		<author>
			<persName coords=""><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/606555cf42a6719782a952aa33cfa2cb-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_DD29NNg">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,97.78,137.91,442.22,14.41;16,97.77,152.36,442.23,14.41;16,97.77,166.81,442.23,14.41;16,97.77,181.25,56.79,14.41" xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_w3sd8V8">Quantifying the benefit of using differentiable learning over tangent kernels</title>
		<author>
			<persName coords=""><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pritish</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emmanuel</forename><surname>Abbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_y5uzucU">Proceedings of the 38th International Conference on Machine Learning, Proceedings of Machine Learning Research</title>
				<meeting>the 38th International Conference on Machine Learning, Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7379" to="7389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,97.78,202.92,442.22,14.41;16,97.77,217.37,442.23,14.41;16,97.77,231.81,442.23,14.41;16,97.77,246.26,442.23,14.41;16,97.77,260.70,78.69,14.41;16,179.45,265.09,354.47,8.42" xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_kKMxwXu">The connection between approximation, depth separation and learnability in neural networks</title>
		<author>
			<persName coords=""><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gilad</forename><surname>Yehudai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shai</forename><surname>Shalev-Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v134/malach21a.html" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_czFfwKm">Proceedings of Thirty Fourth Conference on Learning Theory</title>
				<editor>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Samory</forename><surname>Kpotufe</surname></persName>
		</editor>
		<meeting>Thirty Fourth Conference on Learning Theory</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-08-19">15-19 Aug 2021</date>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="3265" to="3295" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="16,97.78,282.37,442.22,14.41;16,97.77,296.82,256.38,14.41" xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_DK5dcPs">The landscape of empirical risk for nonconvex losses</title>
		<author>
			<persName coords=""><forename type="first">Song</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qmw2bhu">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="2747" to="2774" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,97.78,318.49,442.22,14.41;16,97.77,332.93,442.23,14.41;16,97.77,347.38,442.23,14.27;16,97.77,361.83,121.88,14.41" xml:id="b36">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Song</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theodor</forename><surname>Misiakiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
		<title level="m" xml:id="_CEs34t4">Generalization error of random features and kernel methods: hypercontractivity and kernel matrix concentration. Applied and Computational Harmonic Analysis, Special Issue on Harmonic Analysis and Machine Learning</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="3" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,97.78,383.49,442.22,14.41;16,97.77,397.94,310.45,14.41;16,410.88,402.33,129.12,8.42;16,97.77,416.77,103.41,8.42" xml:id="b37">
	<monogr>
		<title level="m" type="main" xml:id="_JeesD8C">The interpolation phase transition in neural networks: Memorization and generalization under lazy training</title>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiqiao</forename><surname>Zhong</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2007.12826" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,97.78,434.05,442.22,14.41;16,97.77,448.50,442.23,14.41;16,97.77,462.95,442.23,14.27;16,97.77,477.39,26.90,14.41" xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_hvFk9xt">Increasing depth leads to u-shaped test risk in over-parameterized convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">Eshaan</forename><surname>Nichani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adityanarayanan</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_7vKhMhc">International Conference of Machine Learning Workshop on Over-parameterization: Pitfalls and Opportunities</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,97.78,499.06,442.22,14.41;16,97.77,513.51,442.23,14.41;16,97.77,527.95,238.54,14.41" xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_Syskb3F">Identifying good directions to escape the ntk regime and efficiently learn low-degree plus sparse polynomials</title>
		<author>
			<persName coords=""><forename type="first">Eshaan</forename><surname>Nichani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_7Yp7wab">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,97.78,549.62,442.22,14.41;16,97.77,564.07,21.59,14.41;16,141.75,564.07,398.25,14.41" xml:id="b40">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Subhaneil</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<title level="m" xml:id="_gMVdJaV">Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="16,97.77,578.51,442.23,14.41;16,97.77,592.96,442.22,14.41;16,97.77,607.40,56.36,14.41;16,171.80,607.40,23.91,14.41;16,202.86,611.79,337.14,8.42;16,97.77,626.24,304.76,8.42" xml:id="b41">
	<monogr>
		<author>
			<persName coords=""><forename type="first">In</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2016/file/148510031349642de5ca0c544f31b2ef-Paper.pdf" />
		<title level="m" xml:id="_UTdCGXb">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,97.78,643.52,442.22,14.41;16,97.77,657.96,344.80,14.41" xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_nYNeV5b">Depth separation with multilayer mean-field networks</title>
		<author>
			<persName coords=""><forename type="first">Yunwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_XJb6fdj">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,97.78,679.63,442.23,14.41;16,97.77,694.08,279.31,14.41" xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_ExdUqMV">Optimization-based separations for neural networks</title>
		<author>
			<persName coords=""><forename type="first">Itay</forename><surname>Safran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_V5ZW77Z">Proceedings of Thirty Fifth Conference on Learning Theory</title>
				<meeting>Thirty Fifth Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,97.78,72.91,442.22,14.41;17,97.77,87.35,442.23,14.41;17,97.77,101.80,442.22,14.41;17,97.77,116.25,393.73,14.41;17,496.96,120.63,43.04,8.42;17,97.77,135.08,304.26,8.42" xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_5mwt8rM">Depth-width tradeoffs in approximating natural functions with neural networks</title>
		<author>
			<persName coords=""><forename type="first">Itay</forename><surname>Safran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v70/safran17a.html" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_EM5tdye">Proceedings of the 34th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08">Aug 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="17,97.78,152.36,442.22,14.41;17,97.77,166.81,138.85,14.41;17,253.85,166.81,286.16,14.41;17,97.77,181.25,442.23,14.41;17,97.77,195.70,394.69,14.41;17,496.96,200.09,43.04,8.42;17,97.77,214.53,304.26,8.42" xml:id="b45">
	<analytic>
		<title level="a" type="main" xml:id="_2gvrqhV">Depth separations in neural networks: What is actually being separated?</title>
		<author>
			<persName coords=""><forename type="first">Itay</forename><surname>Safran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v99/safran19a.html" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_DrSPsV6">Proceedings of the Thirty-Second Conference on Learning Theory</title>
				<editor>
			<persName><forename type="first">Alina</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</editor>
		<meeting>the Thirty-Second Conference on Learning Theory</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-28">25-28 Jun 2019</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="2664" to="2666" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="17,97.78,231.81,442.22,14.41;17,97.77,246.26,442.23,14.41;17,97.77,265.09,297.09,8.42" xml:id="b46">
	<analytic>
		<title level="a" type="main" xml:id="_mAYCZc3">Deep information propagation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Surya</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jascha</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sohl-Dickstein</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1W1UN9gg" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_eqtGt2p">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,97.78,282.37,442.22,14.41;17,97.77,296.82,200.02,14.41" xml:id="b47">
	<analytic>
		<title level="a" type="main" xml:id="_uTjqgNU">Learning relus via gradient descent</title>
		<author>
			<persName coords=""><forename type="first">Mahdi</forename><surname>Soltanolkotabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_y5unxcP">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,97.78,318.49,442.22,14.41;17,97.77,332.93,442.23,14.41;17,97.77,347.38,189.96,14.41" xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_GXZUWeD">Theoretical insights into the optimization landscape of over-parameterized shallow neural networks</title>
		<author>
			<persName coords=""><forename type="first">Mahdi</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adel</forename><surname>Javanmard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9pWvWFU">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="742" to="769" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,97.78,369.05,442.22,14.41;17,97.77,383.49,442.23,14.41;17,97.77,397.94,442.23,14.41;17,97.77,412.39,266.46,14.41;17,367.85,416.77,172.15,8.42;17,97.77,431.22,189.49,8.42" xml:id="b49">
	<analytic>
		<title level="a" type="main" xml:id="_ysXgvAd">benefits of depth in neural networks</title>
		<author>
			<persName coords=""><forename type="first">Matus</forename><surname>Telgarsky</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v49/telgarsky16.html" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_NAy9f7M">of Proceedings of Machine Learning Research</title>
				<editor>
			<persName><forename type="first">Vitaly</forename><surname>Feldman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexander</forename><surname>Rakhlin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">Jun 2016</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="23" to="26" />
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
	<note>29th Annual Conference on Learning Theory</note>
</biblStruct>

<biblStruct coords="17,97.78,448.50,322.89,14.41;17,425.23,452.89,114.77,8.42;17,97.77,467.33,218.18,10.71" xml:id="b50">
	<monogr>
		<title level="m" type="main" xml:id="_B8APXAj">Probability in high dimension</title>
		<author>
			<persName coords=""><forename type="first">Ramon</forename><surname>Van Handel</surname></persName>
		</author>
		<ptr target="http://web.math.princeton.edu/˜rvan/APC550.pdf" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,97.78,484.61,442.22,14.41;17,97.77,499.06,337.95,14.41;17,439.58,503.45,100.42,8.42;17,97.77,517.89,132.10,8.42" xml:id="b51">
	<monogr>
		<title level="m" type="main" xml:id="_xd4FrnB">Regularization matters: Generalization and optimization of neural nets v.s. their induced kernel</title>
		<author>
			<persName coords=""><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1810.05369" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,97.78,535.18,442.22,14.41;17,97.77,549.62,442.23,14.41;17,97.77,564.07,442.23,14.41;17,97.77,578.51,116.89,14.41" xml:id="b52">
	<analytic>
		<title level="a" type="main" xml:id="_6QrurUJ">Kernel and rich regimes in overparametrized models</title>
		<author>
			<persName coords=""><forename type="first">Blake</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Moroshko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pedro</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Itay</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_xJjSW5d">Conference on Learning Theory, Proceedings of Machine Learning Research</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3635" to="3673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,97.78,600.18,442.22,14.41;17,97.77,614.63,442.23,14.41;17,97.77,629.07,442.23,14.41;17,97.77,643.52,442.23,14.41;17,97.77,657.96,442.23,14.41;17,97.77,676.80,332.95,8.42" xml:id="b53">
	<analytic>
		<title level="a" type="main" xml:id="_nanctHk">Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v80/xiao18a.html" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_CJgVyuT">Proceedings of the 35th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-15">10-15 Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5393" to="5402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,97.78,694.08,442.22,14.41;17,97.77,708.53,442.23,14.41" xml:id="b54">
	<analytic>
		<title level="a" type="main" xml:id="_SsXmrYx">Disentangling trainability and generalization in deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>Schoenholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_CMDPv5C">Pro</title>
				<editor>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iii</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
