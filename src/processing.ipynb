{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-10T13:40:23.395352527Z",
     "start_time": "2023-05-10T13:40:21.728489742Z"
    }
   },
   "outputs": [],
   "source": [
    "from grobid.client import GrobidClient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import xml.etree.ElementTree as ET\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'/hola'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper = \"/hola/quetal.pdf\"\n",
    "\"/\".join(paper.split(\"/\")[:-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-10T13:41:15.998406007Z",
     "start_time": "2023-05-10T13:41:15.993414382Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Modelo para tokenizar</h1>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"jamescalam/minilm-arxiv-encoder\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T08:04:37.908946906Z",
     "start_time": "2023-05-08T08:04:37.448408890Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Funciones útiles para sacar el abstract y los reconocimientos de los árboles xml</h1>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "def get_schema(tree):\n",
    "    '''\n",
    "    This function gets the schema of a given XML tree.\n",
    "    :param tree: an ElementTree object\n",
    "    :return: the schema string\n",
    "    '''\n",
    "\n",
    "    res = tree.getroot().tag.split(\"}\")\n",
    "    return res[0] + \"}\" if len(res) > 0 else \"\"\n",
    "\n",
    "def get_authors(papers, elem, schema):\n",
    "    '''\n",
    "    This function gets the authors of a given paper.\n",
    "    :param papers: a dictionary of papers\n",
    "    :param elem: the name of the XML file containing the paper\n",
    "    :param schema: the schema string\n",
    "    :return: a list of dictionaries, where each dictionary represents an author\n",
    "    '''\n",
    "    authors = []\n",
    "    for author in papers[elem].findall(f\"{schema}teiHeader/{schema}fileDesc/{schema}titleStmt/{schema}author\"):\n",
    "        author_dict = {}\n",
    "        if author.find(f\"{schema}persName\") is not None:\n",
    "            author_dict[\"name\"] = author.find(f\"{schema}persName\").text\n",
    "        if author.find(f\"{schema}email\") is not None:\n",
    "            author_dict[\"email\"] = author.find(f\"{schema}email\").text\n",
    "        if author.find(f\"{schema}affiliation\") is not None:\n",
    "            author_dict[\"affiliation\"] = author.find(f\"{schema}affiliation\").text\n",
    "        authors.append(author_dict)\n",
    "    return authors\n",
    "\n",
    "def get_abstract(papers, elem, schema):\n",
    "    '''\n",
    "    This function gets the abstract of a given paper.\n",
    "    :param papers: a dictionary of papers\n",
    "    :param elem: the name of the XML file containing the paper\n",
    "    :param schema: the schema string\n",
    "    :return: the abstract string\n",
    "    '''\n",
    "    if papers[elem].find(f\"{schema}teiHeader\") is not None:\n",
    "        if papers[elem].find(f\"{schema}teiHeader\").find(f\"{schema}profileDesc\") is not None:\n",
    "            if papers[elem].find(f\"{schema}teiHeader\").find(f\"{schema}profileDesc\").find(\n",
    "                    f\"{schema}abstract\") is not None:\n",
    "                return ET.tostring(\n",
    "                    papers[elem].find(f\"{schema}teiHeader\").find(f\"{schema}profileDesc\").find(f\"{schema}abstract\"),\n",
    "                    encoding='utf-8', method='text').strip().decode(\"utf-8\")\n",
    "    return \"\"\n",
    "\n",
    "def get_acknowledgements(papers, elem, schema):\n",
    "    try:\n",
    "        return list(map(lambda x: [y.text for y in x], map(lambda x: [y for y in x.iter()], filter(lambda elem: \"acknowledgement\" in list(elem.attrib.values()), [elem for elem in papers[elem].find(f\"{schema}text\").find(f\"{schema}back\").findall(rf\"{schema}div\")]))))[-1][-1]\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def get_references(papers, elem, schema):\n",
    "    '''\n",
    "    This function gets the bibliographic references of a given paper.\n",
    "    :param papers: a dictionary of papers\n",
    "    :param elem: the name of the XML file containing the paper\n",
    "    :param schema: the schema string\n",
    "    :return: a list of dictionaries, where each dictionary represents a reference\n",
    "    '''\n",
    "    def get_name_parts(authors, schema):\n",
    "        ret_authors = []\n",
    "        for author in authors:\n",
    "            ret_author = []\n",
    "            for name_part in author.iter():\n",
    "                if name_part.text is not None:\n",
    "                    ret_author.append(name_part.text)\n",
    "            ret_authors.append(\" \".join(ret_author[1:]).strip().replace(\"  \", \" \"))\n",
    "        return ret_authors\n",
    "\n",
    "    refs = []\n",
    "    for ref in papers[elem].findall(f\"{schema}text/{schema}back/{schema}div/{schema}listBibl/{schema}biblStruct\"):\n",
    "        ref_dict = {}\n",
    "        if ref.find(f\"{schema}analytic\") is not None:\n",
    "            if ref.find(f\"{schema}analytic/{schema}title\") is not None:\n",
    "                ref_dict[\"title\"] = ref.find(f\"{schema}analytic/{schema}title\").text\n",
    "            if ref.find(f\"{schema}analytic/{schema}author\") is not None:\n",
    "                aux_authors = [author for author in ref.findall(f\"{schema}analytic/{schema}author\")]\n",
    "                authors = get_name_parts(aux_authors, schema)\n",
    "                ref_dict[\"authors\"] = authors\n",
    "        if ref.find(f\"{schema}monogr\") is not None:\n",
    "            if ref.find(f\"{schema}monogr/{schema}title\") is not None:\n",
    "                ref_dict[\"journal\"] = ref.find(f\"{schema}monogr/{schema}title\").text\n",
    "            if ref.find(f\"{schema}monogr/{schema}imprint\") is not None:\n",
    "                if ref.find(f\"{schema}monogr/{schema}imprint/{schema}date\") is not None:\n",
    "                    ref_dict[\"date\"] = ref.find(f\"{schema}monogr/{schema}imprint/{schema}date\").text\n",
    "        refs.append(ref_dict)\n",
    "    return refs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T15:54:33.523488354Z",
     "start_time": "2023-05-09T15:54:33.479967872Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n",
    "input_path = \"../res/datasets/raw\"\n",
    "output_path = \"../res/datasets/grobid\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T14:35:43.406215180Z",
     "start_time": "2023-05-09T14:35:43.399270266Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extraer el xml de cada paper con grobid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 30 of 30..."
     ]
    }
   ],
   "source": [
    "# Extract with Grobid\n",
    "client = GrobidClient(host=\"localhost\", port=8070)\n",
    "for i, file in enumerate(os.listdir(input_path)):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        print(f\"\\rProcessing file {i + 1} of {len(os.listdir(input_path))}...\", end=\"\")\n",
    "        resp = client.serve(\"processFulltextDocument\", input_path + \"/\" + file, consolidate_header=True, consolidate_citations=True)\n",
    "        if resp[1] != 200:\n",
    "            print(f\"Error processing file {file}!\")\n",
    "            continue\n",
    "        with open(output_path + \"/\" + file.replace(\".pdf\", \".xml\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(resp[0].text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T08:13:56.468447572Z",
     "start_time": "2023-05-08T08:12:37.077877343Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "leemos los xml y sacamos el arbol"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "papers = {}\n",
    "for file in os.listdir(output_path):\n",
    "    if file.endswith(\".xml\"):\n",
    "        papers[file] = ET.parse(output_path + \"/\" + file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T14:35:45.009347567Z",
     "start_time": "2023-05-09T14:35:44.872537067Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sacamos los abstracts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "abstracts = dict(zip(papers.keys(), [get_abstract(papers, elem, get_schema(papers[elem])) for elem in papers.keys()]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T08:23:24.963249822Z",
     "start_time": "2023-05-08T08:23:24.962270772Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "acknowledgements = dict(zip(papers.keys(), [get_acknowledgements(papers, elem, get_schema(papers[elem])) for elem in papers.keys()]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T15:19:43.333078635Z",
     "start_time": "2023-05-09T15:19:43.331176620Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "references = dict(zip(papers.keys(), [get_references(papers, elem, get_schema(papers[elem])) for elem in papers.keys()]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T15:52:46.928222317Z",
     "start_time": "2023-05-09T15:52:46.858826405Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "authors = dict(zip(papers.keys(), [get_authors(papers, elem, get_schema(papers[elem])) for elem in papers.keys()]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T15:54:38.577181880Z",
     "start_time": "2023-05-09T15:54:38.575914184Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "{'1511.01844.xml': [],\n 'acs.jcim.9b01120.xml': [],\n 'A_Novel_Approach_for_Classification_of_Speech_Emotions_Based_on_Deep_and_Acoustic_Features.xml': [],\n '1704.05742.xml': [],\n 'NIPS-2017-adagan-boosting-generative-models-Paper.xml': [],\n '1810.09136.xml': [],\n '5423.xml': [],\n '11_43.xml': [],\n '2001.10238.xml': [],\n '1-s2.0-S2090447914001567-main.xml': [],\n 'NeurIPS-2020-gradient-surgery-for-multi-task-learning-Paper.xml': [],\n 'maaloe16.xml': [],\n 'brotli-2015-09-22.xml': [],\n 'wordCompression.xml': [],\n 'grumbach.xml': [],\n 'A_Comparative_Study_Of_Text_Compression_Algorithms.xml': [],\n '1803.03324.xml': [],\n '179693.xml': [],\n 'annurev-statistics-010814-020120.xml': [],\n '1206.3255.xml': [],\n '1706.02901.xml': [],\n 'satt17_interspeech.xml': [],\n 'symmetry-12-00021 (1).xml': [],\n 's10052-015-3703-3.xml': [],\n '1812.05920v1.xml': [],\n 'Speech_emotion_recognition_using_deep_1D (2).xml': [],\n 'document.xml': [],\n '090902_1.xml': [],\n '2005.00341.xml': [],\n 'badshah2017.xml': []}"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T15:54:42.041038802Z",
     "start_time": "2023-05-09T15:54:42.038815185Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
